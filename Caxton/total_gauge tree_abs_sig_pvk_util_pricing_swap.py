gauge tree.py



Portfolio tree

"Portfolio, "

"©À©¤©¤ Basket_usa_rates1c, weight = 0.25"

"©À©¤©¤ Basket_can_rates1b, weight = 0.25"

"©À©¤©¤ Basket_gbr_rates1b, weight = 0.25"

"©¸©¤©¤ Basket_aus_rates1b, weight = 0.25"

 

USA

Gauge tree

"Condition minus pricing, "

"©À©¤©¤ Pricing gauge, weight = 0.3"

"©¦   ©À©¤©¤ Pricing gauge, weight = 0"

"©¦   ©¸©¤©¤ Pricing gauge, steepness between 2y1d and 1w ois, weight = 1.0"

"©¸©¤©¤ Condition gauge, weight = 0.7"

"    ©À©¤©¤ Level gauge, weight = 3.0"

"    ©¦   ©À©¤©¤ GDP slack, weight = 1.0"

"    ©¦   ©À©¤©¤ Growth vs potential, weight = 7.0"

"    ©¦   ©À©¤©¤ Unemployment vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Capacity vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Wage vs trend, weight = 2.0"

"    ©¦   ©À©¤©¤ Wage(atlanta) vs trend, weight = 0"

"    ©¦   ©À©¤©¤ Cpi(GS tracker) vs target, weight = 1.0"

"    ©¦   ©¸©¤©¤ BEI5 vs target, weight = 1.0"

"    ©À©¤©¤ Change gauge, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in growth, weight = 0"

"    ©¦   ©À©¤©¤ Change in citi econ chg index, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in wage, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in cpi pce, weight = 1.0"

"    ©¦   ©À©¤©¤ Change in surprise in cpi, weight = 1.0"

"    ©¦   ©À©¤©¤ Change in cpi GStracker, weight = 0"

"    ©¦   ©¸©¤©¤ Change in bei5, weight = 1.0"

"    ©À©¤©¤ Forward growth, weight = 4.0"

"    ©¦   ©À©¤©¤ Housing_price_1st, weight = 1.0"

"    ©¦   ©À©¤©¤ Housing_price_2nd, weight = 0"

"    ©¦   ©¸©¤©¤ In-house growth estimate, weight = 6.0"

"    ©À©¤©¤ Forward cpi, weight = 1.0"

"    ©¦   ©À©¤©¤ Oil_impulse, weight = 7.0"

"    ©¦   ©¸©¤©¤ FX_impulse, weight = 3.0"

"    ©¸©¤©¤ Global gauge, weight = 1.0"

"        ©À©¤©¤ Glob growth pot, weight = 1.0"

"        ©À©¤©¤ Glob FCI impulse, weight = 1.0"

"        ©¸©¤©¤ Glob change in growth, weight = 2.0"

 

 

UK

Gauge tree

"Condition minus pricing, "

"©À©¤©¤ Pricing gauge, weight = 0.3"

"©¦   ©À©¤©¤ Pricing gauge, weight = 0"

"©¦   ©¸©¤©¤ Pricing gauge, steepness between 2y1d and 1w ois, weight = 1.0"

"©¸©¤©¤ Condition gauge, weight = 0.7"

"    ©À©¤©¤ Level gauge, weight = 1.0"

"    ©¦   ©À©¤©¤ GDP slack, weight = 1.0"

"    ©¦   ©À©¤©¤ Growth vs potential, weight = 1.0"

"    ©¦   ©À©¤©¤ Unemployment vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Capacity vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Wage (regular pay) vs trend, weight = 1.0"

"    ©¦   ©¸©¤©¤ Core cpi vs target, weight = 2.0"

"    ©À©¤©¤ Change gauge, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in growth, weight = 5.0"

"    ©¦   ©À©¤©¤ Change in wage(regular), weight = 2.0"

"    ©¦   ©À©¤©¤ Change in cpi (core), weight = 1.0"

"    ©¦   ©¸©¤©¤ Change in headline cpi, weight = 2.0"

"    ©À©¤©¤ Forward growth, weight = 4.0"

"    ©¦   ©À©¤©¤ Housing_price_1st, weight = 2.0"

"    ©¦   ©À©¤©¤ Mortgage approval pct gdp, weight = 2.0"

"    ©¦   ©¸©¤©¤ In-house growth estimate, weight = 10.0"

"    ©À©¤©¤ Forward cpi, weight = 1.0"

"    ©¦   ©À©¤©¤ Oil_impulse, weight = 2.0"

"    ©¦   ©À©¤©¤ FX_impulse, weight = 2.0"

"    ©¦   ©¸©¤©¤ Headline minus core cpi, weight = 1.0"

"    ©¸©¤©¤ Global gauge, weight = 1.0"

"        ©À©¤©¤ Glob growth pot, weight = 2.0"

"        ©À©¤©¤ Glob FCI impulse, weight = 1.0"

"        ©¸©¤©¤ Glob change in growth, weight = 2.0"

 

CAN

Gauge tree

"Condition minus pricing, "

"©À©¤©¤ Pricing gauge, weight = 0.3"

"©¦   ©À©¤©¤ Pricing gauge, weight = 0"

"©¦   ©¸©¤©¤ Pricing gauge, steepness between 2y1d and 1w ois, weight = 1.0"

"©¸©¤©¤ Condition gauge, weight = 0.7"

"    ©À©¤©¤ Level gauge, weight = 1.0"

"    ©¦   ©À©¤©¤ GDP slack, weight = 1.0"

"    ©¦   ©À©¤©¤ Growth vs potential, weight = 1.0"

"    ©¦   ©À©¤©¤ Unemployment vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Capacity vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Wage vs trend, weight = 1.0"

"    ©¦   ©¸©¤©¤ Cpi(trimmed) vs target, weight = 1.0"

"    ©À©¤©¤ Change gauge, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in growth, weight = 2.0"

"    ©¦   ©À©¤©¤ Change in wage, weight = 1.0"

"    ©¦   ©¸©¤©¤ Change in cpi (trimmed mean), weight = 1.0"

"    ©À©¤©¤ Forward growth, weight = 4.0"

"    ©¦   ©À©¤©¤ Housing_price_1st, weight = 2.0"

"    ©¦   ©À©¤©¤ Credit_ngdp_1st, weight = 2.0"

"    ©¦   ©¸©¤©¤ In-house growth estimate (6m), weight = 10.0"

"    ©À©¤©¤ Forward cpi, weight = 1.0"

"    ©¦   ©À©¤©¤ Oil_impulse, weight = 1.0"

"    ©¦   ©¸©¤©¤ FX_impulse, weight = 1.0"

"    ©¸©¤©¤ Global gauge, weight = 1.0"

"        ©À©¤©¤ Glob growth pot, weight = 2.0"

"        ©À©¤©¤ Glob FCI impulse, weight = 1.0"

"        ©¸©¤©¤ Glob change in growth, weight = 2.0"

 

 

AUS

 

Gauge tree

"Condition minus pricing, "

"©À©¤©¤ Pricing gauge, weight = 0.3"

"©¦   ©À©¤©¤ Pricing gauge, weight = 0"

"©¦   ©¸©¤©¤ Pricing gauge, steepness between 2y1d and 1w ois, weight = 1.0"

"©¸©¤©¤ Condition gauge, weight = 0.7"

"    ©À©¤©¤ Level gauge, weight = 1.0"

"    ©¦   ©À©¤©¤ GDP slack, weight = 1.0"

"    ©¦   ©À©¤©¤ Growth vs potential, weight = 1.0"

"    ©¦   ©À©¤©¤ Unemployment vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Capacity vs trend, weight = 1.0"

"    ©¦   ©À©¤©¤ Wage vs trend, weight = 1.0"

"    ©¦   ©¸©¤©¤ Cpi(trimmed mean) vs target, weight = 2.0"

"    ©À©¤©¤ Change gauge, weight = 4.0"

"    ©¦   ©À©¤©¤ Change in growth, weight = 2.0"

"    ©¦   ©À©¤©¤ Change in wage, weight = 1.0"

"    ©¦   ©¸©¤©¤ Change in cpi (trimmed mean), weight = 2.0"

"    ©À©¤©¤ Forward growth, weight = 4.0"

"    ©¦   ©À©¤©¤ Housing_price_1st, weight = 2.0"

"    ©¦   ©À©¤©¤ Credit_ngdp_1st, weight = 2.0"

"    ©¦   ©¸©¤©¤ In-house growth estimate, weight = 10.0"

"    ©À©¤©¤ Forward cpi, weight = 1.0"

"    ©¦   ©À©¤©¤ Oil_impulse, weight = 1.0"

"    ©¦   ©¸©¤©¤ FX_impulse, weight = 1.0"

"    ©¸©¤©¤ Global gauge, weight = 1.0"

"        ©À©¤©¤ Glob growth pot, weight = 2.0"

"        ©À©¤©¤ Glob FCI impulse, weight = 1.0"

"        ©¸©¤©¤ Glob change in growth, weight = 2.0"

 

 

USA

 

GDP nowcast,

©À©¤©¤ Personal consumption expenditure, weight = 0.72

©¦   ©À©¤©¤ S&P 500 stock price index, weight = 8.0

©¦   ©À©¤©¤ S&P 500 stock price index (1st, detrend), weight = 0.0

©¦   ©À©¤©¤ Total return index of 10 year T-bonds, weight = 2.0

©¦   ©À©¤©¤ S&P CoreLogic Case-Shiller home price index, weight = 2.0

©¦   ©À©¤©¤ MBA : volume index : mortgage loan app for purchase, weight = 1.0

©¦   ©À©¤©¤ Salary, weight = 2.0

©¦   ©À©¤©¤ WTI intermediate, Cushing, weight = 1.0

©¦   ©¸©¤©¤ Conference board consumer confidence, weight = 1.0

©¦       ©À©¤©¤ CRB consumer confidence, weight = 1.0

©¦       ©¸©¤©¤ UMich consumer confidence, weight = 1.0

©À©¤©¤ Private fixed investment (residential), weight = 0.03

©¦   ©À©¤©¤ Housing units authorized by builidng permit, weight = 1.0

©¦   ©À©¤©¤ Months supply at current sales rate for new 1-family houses for sale, weight = 1.0

©¦   ©À©¤©¤ MBA : volume index : mortgage loan app for purchase, weight = 1.0

©¦   ©À©¤©¤ 30-year fixed mortgage rate, weight = 1.0

©¦   ©¸©¤©¤ House builders bloomberg forward EPS, weight = 2.0

©À©¤©¤ Private fixed investment (non residential),ex. energy capex, weight = 0.14

©¦   ©À©¤©¤ S&P global fixed income research: industrials BBB bond yields, weight = 1.0

©¦   ©À©¤©¤ S&P 500 stock price index, weight = 1.0

©¦   ©À©¤©¤ Manufacturing PPI, weight = 1.0

©¦   ©À©¤©¤ Trade weighted average of partners CAI, weight = 1.0

©¦   ©À©¤©¤ The composite capex planning survey, weight = 1.0

©¦   ©¦   ©À©¤©¤ Fed Kansas outlook survey : capex, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed New York outlook survey : capex, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed Philly outlook survey : capex, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed Richmond outlook survey : capex, 6m ahead, weight = 0.5

©¦   ©¦   ©¸©¤©¤ Fed Texas outlook survey : capex, 6m ahead, weight = 0.5

©¦   ©À©¤©¤ The composite expected new order survey, weight = 1.0

©¦   ©¦   ©À©¤©¤ Fed Texas Mfg outlook survey : new order, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed Kansas Mfg outlook survey : new order, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed Richmond Mfg outlook survey : new_order, 6m ahead, weight = 0.5

©¦   ©¦   ©À©¤©¤ Fed Philly Mfg outlook survey : new order, 6m ahead, weight = 0.5

©¦   ©¦   ©¸©¤©¤ Empire Mfg outlook survey : new order,  6m ahead, weight = 0.5

©¦   ©À©¤©¤ Composite consumer confidence (Umich &CRB), weight = 0.5

©¦   ©¦   ©À©¤©¤ CRB consumer confidence, weight = 1.0

©¦   ©¦   ©¸©¤©¤ UMich consumer confidence, weight = 1.0

©¦   ©À©¤©¤ Retail sales & food services ex autos and gas, weight = 0.5

©¦   ©¸©¤©¤ JPM Capex tracker, weight = 1.0

©À©¤©¤ Private fixed investment in energy capex, weight = 0.01

©¦   ©À©¤©¤ Crude oil price : WTI intermediate, Cushing, weight = 2.0

©¦   ©À©¤©¤ FRB : capacity utilisation : mining, weight = 1.0

©¦   ©¸©¤©¤ Total rig count, weight = 1.0

©¸©¤©¤ Export, weight = 0.14

    ©À©¤©¤ Nominal effective exchange rates, weight = 1.0

©¸©¤©¤ Trade weighted average of partners CAI, weight = 1.0

 

 

AUS

GS current activity index,

©À©¤©¤ Personal consumption expenditure, weight = 0.57

©¦   ©À©¤©¤ AUS : stock price index, weight = 2.0

©¦   ©À©¤©¤ Total return index of 10 year govt bond, weight = 0.5

©¦   ©À©¤©¤ Housing prices, existing homes, weight = 4.0

©¦   ©À©¤©¤ Salary, weight = 3.0

©¦   ©¦   ©À©¤©¤ Average weekly hours worked, weight = 1.0

©¦   ©¦   ©À©¤©¤ Average hourly earnings, regular pay, weight = 2.0

©¦   ©¦   ©¸©¤©¤ Employment, weight = 1.0

©¦   ©À©¤©¤ Composite consumer confidence, weight = 2.0

©¦   ©À©¤©¤ Mortgage loans approved, ex remortgaging, as %GDP, weight = 3.0

©¦   ©À©¤©¤ CRB Spot commodity price index: raw industrials, weight = 2.0

©¦   ©À©¤©¤ RBA commodity prices, weight = 1.0

©¦   ©À©¤©¤ Trade weighted average of partners CAI, weight = 1.0

©¦   ©¸©¤©¤ Total debt servicing multiply mortgage rate, weight = 1.0

©À©¤©¤ Private fixed investment (residential), weight = 0.05

©¦   ©À©¤©¤ Mortgages rates , weight = 1.0

©¦   ©À©¤©¤ Mortgage loans approved, ex remortgaging, as %GDP, weight = 1.0

©¦   ©À©¤©¤ New home sales : detached houses, weight = 1.0

©¦   ©¸©¤©¤ Work started for dwellings, weight = 1.0

©À©¤©¤ Private fixed investment (non residential),ex. energy capex, weight = 0.10

©¦   ©À©¤©¤ Trade weighted average of partners CAI, weight = 5.0

©¦   ©¸©¤©¤ Composite business confidence, weight = 1.0

©À©¤©¤ Capex in mining industry, weight = 0.02

©¦   ©À©¤©¤ CRB Spot commodity price index: raw industrials, weight = 1.0

©¦   ©À©¤©¤ Mineral exploration expenditure, weight = 1.0

©¦   ©À©¤©¤ Mineral exploration : meters drilled, weight = 1.0

©¦   ©¸©¤©¤ RBA commodity prices, weight = 1.0

©¸©¤©¤ Export, weight = 0.22

    ©À©¤©¤ Nominal effective exchange rates, weight = 1.0

    ©À©¤©¤ Trade weighted average of partners CAI, weight = 1.0

    ©À©¤©¤ CRB Spot commodity price index: raw industrials, weight = 1.0

    ©¸©¤©¤ RBA commodity prices, weight = 1.0

 

Canada

GS current activity index,

©À©¤©¤ Personal consumption expenditure, weight = 0.56

©¦   ©À©¤©¤ CAN : stock price index, weight = 2.5

©¦   ©À©¤©¤ Total return index of 10 year govt bond, weight = 0.5

©¦   ©À©¤©¤ Housing prices, new houses, weight = 1.5

©¦   ©À©¤©¤ Consumer confidence, nano, weight = 2.0

©¦   ©À©¤©¤ Crude oil prices: west Canadian select, weight = 2.0

©¦   ©À©¤©¤ Total wages: labor force survey, weight = 0.5

©¦   ©À©¤©¤ Residential mortgage credit as percentage of GDP, weight = 1.0

©¦   ©¸©¤©¤ Trading partner gdp weighted CAI, weight = 2.0

©À©¤©¤ Private fixed investment (residential), weight = 0.07

©¦   ©À©¤©¤ Residential mortgage credit as percentage of GDP, weight = 1.0

©¦   ©À©¤©¤ Residential building permits, units created, weight = 1.0

©¦   ©¸©¤©¤ Conventional mortgage lending rate: 5y, weight = 1.0

©À©¤©¤ Private fixed investment (non residential),ex. energy capex, weight = 0.09

©¦   ©À©¤©¤ CAN : stock price index, weight = 1.0

©¦   ©À©¤©¤ Trading partner gdp weighted CAI, weight = 1.0

©¦   ©À©¤©¤ Retail sales value ex mortor vehicles & parts dealers, weight = 0.5

©¦   ©À©¤©¤ Jpm capex tracker, weight = 1.0

©¦   ©À©¤©¤ 5y IRS, weight = 1.0

©¦   ©À©¤©¤ New orders: all manufacturing induxtries, weight = 1.0

©¦   ©À©¤©¤ Consumer confidence, nano, weight = 0.5

©¦   ©À©¤©¤ CFIB business barometer, weight = 1.0

©¦   ©¸©¤©¤ Capacity utilisation, industry, weight = 1.0

©À©¤©¤ Capex in oil&gas extraction industry, weight = 0.01

©¦   ©À©¤©¤ Crude oil: west canada select, weight = 1.0

©¦   ©¸©¤©¤ Rig count:hughes baker, weight = 1.0

©¸©¤©¤ Export, weight = 0.32

    ©À©¤©¤ Trading partner gdp weighted CAI, weight = 2.0

    ©À©¤©¤ Nominal effective exchange rates, weight = 1.0

    ©¸©¤©¤ Crude oil prices: west Canadian select, weight = 2.0

 

GBR

GS current activity index,

©À©¤©¤ Personal consumption expenditure, weight = 0.64

©¦   ©À©¤©¤ FTSE 100 index, weight = 4.0

©¦   ©À©¤©¤ Total return index of 10 year govt bond, weight = 1.0

©¦   ©À©¤©¤ National housing price index, weight = 2.5

©¦   ©À©¤©¤ Salary, weight = 2.0

©¦   ©¦   ©À©¤©¤ Total weekly hours worked, weight = 1.0

©¦   ©¦   ©¸©¤©¤ Average hourly earnings, regular pay, weight = 1.0

©¦   ©À©¤©¤ Brent oil, weight = 1.0

©¦   ©À©¤©¤ Composite consumer confidence, weight = 2.0

©¦   ©¸©¤©¤ Nominal effective exchange rates, weight = 2.0

©À©¤©¤ Private fixed investment (residential), weight = 0.04

©¦   ©À©¤©¤ Mortgages: fixed rates: MFIs ex central bank, 5-year 75% ltv ratio, weight = 1.0

©¦   ©À©¤©¤ Mortgage loans approved, ex remortgaging, as %GDP, weight = 1.0

©¦   ©À©¤©¤ RICS survey: sales to stocks ratio, weight = 1.0

©¦   ©¸©¤©¤ Work started for dwellings, weight = 1.0

©À©¤©¤ Private fixed investment (non residential),ex. energy capex, weight = 0.09

©¦   ©À©¤©¤ FTSE 100 index, weight = 1.0

©¦   ©À©¤©¤ 5 year interest rate, weight = 1.0

©¦   ©À©¤©¤ Trade weighted average of partners CAI, weight = 1.0

©¦   ©À©¤©¤ Retail sales, ex fuel, autos, weight = 1.0

©¦   ©À©¤©¤ Composite consumer confidence, weight = 1.0

©¦   ©À©¤©¤ Composite business confidence, weight = 1.0

©¦   ©¦   ©À©¤©¤ European commission economic sentiment indicator, weight = 1.0

©¦   ©¦   ©À©¤©¤ Retail sector expected busines situation, weight = 1.0

©¦   ©¦   ©¸©¤©¤ Services sector expected demand over next 3 months, weight = 1.0

©¦   ©¸©¤©¤ JPM Capex tracker, weight = 1.0

©¸©¤©¤ Export, weight = 0.28

    ©À©¤©¤ Nominal effective exchange rates, weight = 1.0

    ©¸©¤©¤ Trade weighted average of partners CAI, weight = 1.0











abstract_sig_genr.py





import os
import sys
import pandas as pd
import numpy as np
import collections
import panormus.data.bo.econ as econ

import Analytics.series_utils as s_util
from Analytics.download_util import Downloader as Downloader
import os
import pickle
from datetime import datetime,timedelta
import Analytics.wfcreate as wf
from panormus.data import haver
import backtesting_utils.post_signal_genr as post_signal_genr
import backtesting_utils.chart as TCT

# part of the utilities
dateparse = lambda x: pd.datetime.strptime(x, '%d/%m/%Y')
dateparse2 = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')
conn = econ.get_instance_using_credentials()
WKDIR = os.path.dirname(os.path.realpath(__file__))
PROJ_DIR = os.path.join(WKDIR,"..")
TEMP_DIR = os.environ['TEMP']
TEMP_RAW_PICKLE = os.path.join(TEMP_DIR,'TEMP_JY_local_db.pickle')

class abstract_sig_genr:
    def __init__(self):
        self.local_db = self.read_pickle()
        self.SU = s_util.date_utils_collection()

    def conn_get_clean(self,ticker):
        df_raw = conn.get(ticker)
        df_list = []
        if len(df_raw.columns)<0.01:
            df_list.append(df_raw)
        for col in df_raw.columns:
            df = df_raw[[col]]
            if len(df.index) > 0.01:
                test_index = [d.date() for d in df.loc[:'2019-8-19',:].index[-1000:]]
                if len(test_index) != len(set(test_index)):
                    new_index = [d.replace(hour=0, minute=0, second=0) for d in df.index]
                    df.index = new_index
                    df.dropna(inplace=True)
                    df = df[~df.index.duplicated(keep='last')]
            df_list.append(df)

        return pd.concat(df_list, axis=1)

    def read_pickle(self):
        '''
        :return: read from local database (in pickle) the series.
        '''
        if os.path.exists(TEMP_RAW_PICKLE):
            if os.path.getmtime(TEMP_RAW_PICKLE) > datetime.timestamp(datetime.now() - timedelta(hours=5)):
                with open(TEMP_RAW_PICKLE, 'rb') as handle:
                    self.local_db = pickle.load(handle)
                return self.local_db
            else:
                return dict()
        else:
            return dict()

    def check_in_use(self):
        #TODO: this file is to check if the pdf charts file is in use
        '''
        :return: check if the pdf file is in use
        '''
        pass

    #TODO: try to use the suffix-name in the file , as the key to the data set, must be much easier to

    def download_data_2(self,MASTER_INPUT_DIR,Short_Name):
        '''
        downloading method allows for more sources (EconDB or csv), return a dictionary
        :return: an ordered dictionary in the format like: {iso1:{'data':[df1,df2],'des':[des1,des2]}}
        '''
        result_dict = collections.OrderedDict()
        import_df = pd.read_excel(MASTER_INPUT_DIR, sheet_name=Short_Name, index_col=False, header=0)
        import_df.replace(np.nan, 'invalid_name', inplace=True)
        iso_list = import_df['ISO'].values.tolist()

        for iso in set(iso_list):
            result_dict[iso] = {}
            result_dict[iso]['data'] = []
            result_dict[iso]['des'] = []
        # start download the data
        number_of_series_col = sum('series' in i for i in import_df.columns.tolist())
        for i in range(number_of_series_col):
            i = str(int(i)+int(1))
            series_col = ('').join(['series',i])
            load_fn_col = ('').join(['load_fn',i])
            location_col = ('').join(['location',i])

            ser_list = import_df[series_col].values.tolist()
            load_fn_list = import_df[load_fn_col].values.tolist()
            location_list = import_df[location_col].values.tolist()

            for k,iso in enumerate(iso_list):
                if ser_list[k] == 'invalid_name':
                    continue
                if load_fn_list[k] == 'EconDB':

                    des = len(result_dict[iso]['des'])
                    result_dict[iso]['des'].append(des)
                    data = self.conn_get_clean([ser_list[k]])
                    print (iso, ser_list[k],' is done!')
                    result_dict[iso]['data'].append(data)
                if load_fn_list[k] == 'csv':
                    print ('Sorry, currently does not support csv import, to be added...')
        return result_dict

    def download_data_3(self,MASTER_INPUT_DIR,Short_Name):
        '''
        downloading method allows for more sources (EconDB, csv or haver), return a dictionary
        the returned data structure is different, as {'AUS_SER1':df_AUS_SER1, 'AUS_SER2':df_AUS_SER2 ...}, so that you can use wildcard to iterate through
        :return: an ordered dictionary in the format like: {iso1:{'data':[df1,df2],'des':[des1,des2]}}
        :param MASTER_INPUT_DIR: excel spread sheet of data info
        :param Short_Name: excel tac name
        :return: dict of data
        '''

        print('start downloading data...')
        result_dict = collections.OrderedDict()
        import_df = pd.read_excel(MASTER_INPUT_DIR, sheet_name=Short_Name, index_col=False, header=0)
        import_df.replace(np.nan, 'invalid_name', inplace=True)
        iso_list = import_df['ISO'].values.tolist()

        # start download the data
        number_of_series_col = sum('series' in i for i in import_df.columns.tolist())
        for i in range(number_of_series_col):
            i = str(int(i)+int(1))
            series_col = ('').join(['series',i])
            load_fn_col = ('').join(['load_fn',i])
            location_col = ('').join(['location',i])
            suffix_col = ('').join(['in_file_suf_fix',i])

            ser_list = import_df[series_col].values.tolist()
            load_fn_list = import_df[load_fn_col].values.tolist()
            location_list = import_df[location_col].values.tolist()
            suffix_list = import_df[suffix_col].values.tolist()

            for k,iso in enumerate(iso_list):
                if ser_list[k] == 'invalid_name':
                    result_dict[suffix_list[k]] = pd.DataFrame()
                    continue
                if 'panel' in ser_list[k] :
                    assert location_list[k] != 'invalid_name', 'sorry, the loc is invalid'
                    try:
                        result_dict[suffix_list[k]] = pd.read_csv(location_list[k],index_col=0,header=0)
                    except:
                        loc = os.path.join(PROJ_DIR,location_list[k])
                        result_dict[suffix_list[k]] = pd.read_csv(loc, index_col=0, header=0)
                    continue
                if load_fn_list[k] == 'EconDB':
                    if ser_list[k] not in self.local_db.keys():
                        df = self.conn_get_clean([ser_list[k]])
                        result_dict[suffix_list[k]] = df
                        self.local_db[ser_list[k]] = df
                    else:
                        result_dict[suffix_list[k]] = self.local_db[ser_list[k]]
                    continue
                if load_fn_list[k] == 'csv':
                    assert location_list[k]!='invalid_name','sorry, the loc is invalid'
                    if not os.path.isfile(location_list[k]):
                        try:
                            loc = os.path.join(PROJ_DIR, location_list[k])
                            df = pd.read_csv(loc, index_col=0, header=0)
                        except:
                            result_dict[suffix_list[k]] = pd.DataFrame()
                            continue
                    else:
                        df = pd.read_csv(location_list[k],index_col=0,header=0)
                        df.index = pd.to_datetime(df.index)
                    if len(df.index)<0.001 or (ser_list[k] not in df.columns.tolist()):
                        result_dict[suffix_list[k]] = pd.DataFrame()
                        continue
                    df.index = pd.to_datetime(df.index)
                    result_dict[suffix_list[k]] = df.loc[:,[ser_list[k]]]
                    continue
                if load_fn_list[k] == 'haver':
                    if ser_list[k] not in self.local_db.keys():
                        #print (ser_list[k])
                        try:
                            df = haver.get_data([ser_list[k]])
                        except:
                            print (ser_list[k])
                        #print (df)
                        df.index = df.index.to_timestamp()
                        result_dict[suffix_list[k]] = df
                        self.local_db[ser_list[k]] = df
                    else:
                        result_dict[suffix_list[k]] = self.local_db[ser_list[k]]
                    continue

        # dump the updated file to the pickle
        with open(TEMP_RAW_PICKLE, 'wb') as handle:
            pickle.dump(self.local_db, handle, protocol=pickle.HIGHEST_PROTOCOL)
        print ('done downloading data...')
        return result_dict

    def download_data_4(self,CONTROL_FILE_DIR,sheet_name = 'Control'):
        # data downloader for control file @FT format
        result_dict = collections.OrderedDict()
        import_df = pd.read_excel(CONTROL_FILE_DIR,sheet_name=sheet_name,index_col=False,header=0)
        iso_list = import_df['Country'].values.tolist()

       #download the data
        ser_list = import_df['Code'].values.tolist()
        for k,iso in enumerate(iso_list):
            result_dict[ser_list[k]] = self.conn_get_clean([ser_list[k]])
            print (iso, ser_list[k],' is downloaded')
        return result_dict

    def import_csv_panel(self,MASTER_INPUT_DIR,Short_Name,csv_path,series):
        '''
        :return: import the csv files given be FT, which
        contains the very long historical real GDP data. Then sort them into a dictionary, with (iso,df) pairs
        '''
        SU = s_util.date_utils_collection()
        iso_with_series = self.import_names_to_list(MASTER_INPUT_DIR,Short_Name,series)
        raw_data = pd.read_csv(csv_path,index_col=0,header=0,na_values='NA')
        #print (raw_data)
        result_dict = {}
        for iso,series in iso_with_series:
            #assert series in raw_data.columns.tolist() , "sorry, series"+series+' is not in the raw data columns!'
            if series not in raw_data.columns.tolist():
                continue
            df = raw_data[[series]]
            df  = SU.truncate_NAs(df)
            result_dict[iso] = df
        #print (self.raw_data)
        return result_dict

    def import_names_to_list(self, master_input_dir,Short_Name, col_name):
        import_df = pd.read_excel(master_input_dir, sheet_name=Short_Name, index_col=False, header=0)
        import_df.replace(np.nan, 'invalid_name', inplace=True)
        self.all_ISO = import_df.iloc[:, 0].values.tolist()
        series1 = import_df.loc[:, col_name].values.tolist()
        return list(zip(self.all_ISO, series1))

    def download_data(self, MASTER_INPUT_DIR, Short_Name, series_name):
        '''
        :return: connecting to the sql-server (in this case via a Downloader class), and converting into a dictionary of (ISO,dataframe) pair
        '''
        s_with_id = self.import_names_to_list(MASTER_INPUT_DIR, Short_Name, series_name)
        download = Downloader(s_with_id)
        #try:
        series_result = download.submit_with_mp()
        #except:
        #series_result = download.submit()
        result_dict = download.convert_result_to_dict(series_result, s_with_id)
        # print (result_dict)
        return result_dict

    def sanity_check(self):
        pass

    def apply_smoothing(self):
        pass

    def post_conversion(self):
        pass

    def convert_indicator_tolist(self,indicator,indicator_exp_dir):
        indicator_list = []
        for iso, content in indicator.items():
            if len(content['des'])==0:
                continue
            else:
                zipped = zip(content['des'],content['data'])
                this_list = [(iso+'_'+str(v[0]),v[1]) for v in zipped]
                indicator_list = indicator_list + this_list
        self.indicator_list = indicator_list
        self.export_indicator(indicator_list,indicator_exp_dir)
        return

    def convert_indicator_tolist_new(self,indicator,indicator_exp_dir):
        self.create_indicator_group_folder(indicator_exp_dir)
        indicator_list = []
        for k, v in indicator.items():
            indicator_list = indicator_list + [(k,v)]
        self.indicator_list = indicator_list
        self.export_indicator(indicator_list,indicator_exp_dir)
        return

    def export_indicator(self,indicator_list,export_path):
        for ticker_df in indicator_list:
            #print (id_df)
            name_of_file = str(ticker_df[0])
            ticker_df[1].to_csv(os.path.join(export_path,name_of_file+'.csv'))
        return

    def create_indicator_group_folder(self,indicator_dir):
        """
        Tearsheet is saved to the reporting folder
        """
        destination = indicator_dir

        self.create_folder(destination)
        assert os.path.isdir(destination)
        assert os.path.exists(destination)
        return destination

    #TODO:might put a group of charting utilities, including pdf/tearsheet etc into a
    # tearsheet utility, so that can be used in the backtesting reporting in the future
    def create_tearsheet_folder(self,RPTDIR):
        """
        Tearsheet is saved to the reporting folder
        """
        destination = RPTDIR

        self.create_folder(destination)
        assert os.path.isdir(destination)
        assert os.path.exists(destination)
        return destination

    def create_folder(self, path):
        "Creates all folders necessary to create the given path."
        try:
            if not os.path.exists(path):
                os.makedirs(path)
        except IOError as io:
            print("Cannot create dir %s" % path)
            raise io

    def create_report_page(self, all_dict):
        pass

    def pickle_result(self,temp_pickle,strategy_work_file):
        '''
        save down the strategy container (work_frame) for future use
        :param temp_pickle: pickle path
        :param strategy_work_file: work_frame object
        '''
        # pickle the result to a temp folder
        with open(temp_pickle, 'wb') as handle:
            pickle.dump(strategy_work_file, handle, protocol=pickle.HIGHEST_PROTOCOL)

    @staticmethod
    def mask_list(l,m):
        return [z[0] for z in zip(l, m) if z[1]]

class abstract_sig_genr_for_rates_tree(abstract_sig_genr):
    def __init__(self):
        super(abstract_sig_genr_for_rates_tree, self).__init__()

    def add_dir_info(self):
        '''
        adding directory information of the strategy
        '''
        self.WKDIR = os.path.dirname(os.path.realpath(__file__))
        self.PROJ_DIR = os.path.join(self.WKDIR,"..")
        self.MASTER_INPUT_DIR = os.path.join(self.PROJ_DIR,"input/master_input.xlsx")
        self.OUTPUT_DIR = os.path.join(self.PROJ_DIR,"output")
        self.SCRATCH_DIR = os.path.join(self.PROJ_DIR,"zzz_NO_commit_folder",self.Short_Name)
        self.INDICATOR_EXP_DIR = os.path.join(self.PROJ_DIR,"zzz_NO_commit_folder",self.Short_Name,'indicator_group')
        self.LOCAL_MACRO_DATA_DIR = os.path.join(self.PROJ_DIR,'macro_data')
        self.time_stamp =str(datetime.timestamp(datetime.now().replace(microsecond=0)))
        self.BT_ID1 = self.Short_Name+self.time_stamp+'.pdf'
        self.BT_ID2 = self.Short_Name+self.time_stamp+'_pnl.pdf'
        self.BT_ID2010_1 = self.Short_Name+self.time_stamp+'2010.pdf'
        self.BT_ID2010_2 = self.Short_Name+self.time_stamp+'_pnl2010.pdf'
        self.BT_BACKUP_ROOT_DIR = os.path.join(self.SCRATCH_DIR,'BT_BACKUP')
        # backup component since 2000
        self.BT_BACKUP_DIR1 = os.path.join(self.SCRATCH_DIR,'BT_BACKUP',self.BT_ID1)
        # backup pnl since 2000
        self.BT_BACKUP_DIR2 = os.path.join(self.SCRATCH_DIR,'BT_BACKUP',self.BT_ID2)
        # backup component since 2000
        self.BT_BACKUP_DIR2010_1 = os.path.join(self.SCRATCH_DIR,'BT_BACKUP',self.BT_ID2010_1)
        # backup pnl since 2010
        self.BT_BACKUP_DIR2010_2 = os.path.join(self.SCRATCH_DIR,'BT_BACKUP',self.BT_ID2010_2)

        self.PARS_DIR = os.path.join(self.SCRATCH_DIR,'BT_BACKUP','PARS'+self.time_stamp+'.csv')
        self.DATA_DIR = os.path.join(self.SCRATCH_DIR,'BT_BACKUP','DATA'+self.time_stamp+'.csv')
        self.ALPHA_DIR = os.path.join(self.SCRATCH_DIR,'BT_BACKUP','ALPHA'+self.time_stamp+'.csv')

        #Firstly export all the relevant result into the csv format. Secondly plot into the charts
        self.SHARE_DIR = r'Y:\MacroQuant\JYang\JY_Project'
        self.RPTDIR = os.path.join(self.SHARE_DIR,'reporting',self.Short_Name) if os.access(self.SHARE_DIR, os.W_OK) else os.path.join(self.PROJ_DIR,'reporting',self.Short_Name)

        # reporting file
        # reporting component since 2000
        self.RPT_COMPONENT_DIR = os.path.join(self.RPTDIR, self.Short_Name + datetime.now().strftime('%Y%m%d') + '.pdf')
        # reporting pnl 2010
        self.RPT_PNL2010_DIR = os.path.join(self.RPTDIR, self.Short_Name + datetime.now().strftime('%Y%m%d') + '_pnl2010.pdf')
        # reporting pnl since 2000
        self.RPT_PNL2000_DIR = os.path.join(self.RPTDIR,
                                            self.Short_Name + datetime.now().strftime('%Y%m%d') + '_pnl2000.pdf')

        #######
        self.IMPORT_DATA_DIR = os.path.join(self.WKDIR,'   ')
        self.TEMP_LOCAL_PICKLE = os.path.join(os.environ['TEMP'],'TEMP_JY_'+self.Short_Name+'_local_db.pickle')
        #######
        self.EXPORT_TREE_STRUCT_FOLDER = os.path.join(self.RPTDIR,'Gauge_tree')
        self.create_folder(self.EXPORT_TREE_STRUCT_FOLDER)
        self.EXPORT_TREE_STRUCT_DIR = os.path.join(self.EXPORT_TREE_STRUCT_FOLDER,'Gauge_tree'+self.Short_Name + datetime.now().strftime('%Y%m%d') +'.csv')
        self.OUT_DATA_DIR = os.path.join(self.PROJ_DIR, 'output', self.Short_Name)

    def import_parse_param(self,master_input_dir,short_name):
        '''
        import and parse the parameters of the strategy
        :param master_input_dir: the excel contains the parameter and weights
        :param short_name: the short name of the strategy
        :return: dictionary that contains the parameters and the weights
        '''
        param_df = pd.read_excel(master_input_dir, sheet_name=short_name, index_col=False, header=0,
                                 na_values=['NA', ''])
        self.all_ISO = param_df.iloc[:,0].values.tolist()
        self.trans_param_df = param_df.loc[:,'Param Table':'type'].set_index('Param Table').dropna().T.to_dict('list')

        # convert to int
        for key,value in self.trans_param_df.items():
            self.trans_param_df[key][0]=int(self.trans_param_df[key][0]) if self.trans_param_df[key][1]=='int' else self.trans_param_df[key][0]

    def run_indicator_and_post_sig_genr(self,*args,**kwargs):
        '''
        generate the trading signal and positions, apply transaction cost reduction algo
        :param args: parameters to pass : curve names, transaction-cost reduction algo etc
        :param kwargs:
        :return:
        '''
        # check if condition minus pricing exists:
        assert 'condition_m_pricing' in self.new_wf.df.keys(), 'sorry, condition minus pricing z-score is not in the new_wf!!!'
        z_diff = self.new_wf.df['condition_m_pricing'].dropna()
        new_name = 'condition - pricing'
        z_diff.columns = [new_name]
        z_diff = z_diff.dropna().rolling(window=self.trans_param_df['con_smooth_window'][0]).apply(np.mean)

        self.new_wf.update_df(new_name, z_diff[[new_name]])
        self.new_wf.add_df('indicator_group', -z_diff[[new_name]].shift(self.trans_param_df['indicator_lag'][0]), repeat=True)
        self.post_indicator_genr(*args,**kwargs)

    def post_indicator_genr(self,*args,**kwargs):
        '''
        generate trading signals and performance matrix
        :param args:
        :param kwargs:
        :return: trading signals and performance matrix
        '''
        self.sci = self.sci_panel(**kwargs)
        signal_dict = post_signal_genr.LS_filtered(self.new_wf.df['indicator_group'], self.sci, method='s_curve',
                                                   tc_reduction_method='inertia', inertia_torlerance=[50])
        self.new_wf.add_df('signal_group', signal_dict['signal_group'])
        self.new_wf.add_df('signal_group_flip', signal_dict['signal_group_flip'])
        self.new_wf.add_df('actual_trade_group', signal_dict['actual_trade_group'])

        self.profit_sample = ['2000-01-01', datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')]
        # profit_sample = ['2000-01-01', '2016-12-31']
        self.pnl = post_signal_genr.profit(self.new_wf.df['actual_trade_group'], self.sci, 1000, self.profit_sample)
        self.new_wf.add_df('equity_curve', self.pnl)
        self.new_wf.add_df('cumprof', self.pnl[['cumprof']])

        retstats_dict = post_signal_genr.returnstats_dict(self.new_wf.df['equity_curve'], 1000, self.profit_sample, benchmark=self.new_wf.df[kwargs['tri_name']])
        self.new_wf.alpha['ann_mean'] = retstats_dict['ann_mean']
        self.new_wf.alpha['ann_std'] = retstats_dict['ann_std']
        self.new_wf.alpha['ann_sharpe'] = retstats_dict['ann_sharpe']
        self.new_wf.alpha['calmar'] = retstats_dict['calmar']
        self.new_wf.add_df('drawdown', retstats_dict['drawdown'])
        self.new_wf.add_df('rolling_corr_1y_bm', retstats_dict['1y_corr'])
        self.new_wf.add_df('avg_corr_bm', retstats_dict['avg_corr'])

        self.profit_sample_2010 = ['2010-01-01', datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')]
        # profit_sample_2010 = ['2010-01-01', '2016-12-31']
        retstats_dict = post_signal_genr.returnstats_dict(self.new_wf.df['equity_curve'], 1000,self.profit_sample_2010, benchmark=self.new_wf.df[kwargs['tri_name']])

        self.new_wf.alpha['ann_mean_2010'] = retstats_dict['ann_mean']
        self.new_wf.alpha['ann_std_2010'] = retstats_dict['ann_std']
        self.new_wf.alpha['ann_sharpe_2010'] = retstats_dict['ann_sharpe']
        self.new_wf.alpha['calmar_2010'] = retstats_dict['calmar']
        self.new_wf.add_df('drawdown_2010', retstats_dict['drawdown'])
        self.new_wf.add_df('rolling_corr_1y_bm_2010', retstats_dict['1y_corr'])
        self.new_wf.add_df('avg_corr_bm_2010', retstats_dict['avg_corr'])
        return

   def run_step3(self,*args,**kwargs):
        '''
        run the reporting system and save down the bactesting result
        '''
        # dump to csv
        post_signal_genr.write_pars_to_csv(self.trans_param_df, self.PARS_DIR)
        post_signal_genr.write_pars_to_csv(self.new_wf.alpha, self.ALPHA_DIR)
        post_signal_genr.dump_wf_obj_to_csv(self.new_wf, self.DATA_DIR)

        if kwargs['run_charting']:
            root = self.tree.nodes['condition_m_pricing']
            chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_component_chart(root)
            # component chart 2000
            TCT.rates_tree_component_plot(plot_dict=chart_pack_dict,chart_start_dt='2000-01-01',chart_end_dt='2030-01-01',rate_rise_fall=self.rates_rise_fall_path,bt_backup_dir=self.RPT_COMPONENT_DIR,pdfpath=self.BT_BACKUP_DIR1)

            #calculate the pnl
            root = self.tree.nodes['condition_m_pricing']
            self.tree.get_rid_of_candidate_node(root)
            self.tree.print_structure(root)
            # plot pnl since 2000
            self.series_name_dict = {'yield_series':'IRS_5Y',
                                     'signal_flip':'signal_group_flip',
                                     'indicator_group':'indicator_group',
                                     'cumprof':'cumprof',
                                     'TRI':'USA_5y_TRI',
                                     'drawdown':'drawdown',
                                     'corr':'rolling_corr_1y_bm_2010'
                                     }
            self.alpha_name_dict = {'ann_mean':'ann_mean',
                                    'ann_std':'ann_std',
                                    'ann_sharpe':'ann_sharpe',
                                    'calmar':'calmar',
                                    'avg_corr':'avg_corr'
                                     }
            pnl_chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_pnl_chart(root,self.new_wf.df,self.series_name_dict,self.new_wf.alpha,self.alpha_name_dict)
            TCT.rates_tree_component_plot(plot_dict=pnl_chart_pack_dict, chart_start_dt='2000-01-01', chart_end_dt='2030-01-01',rate_rise_fall=self.rates_rise_fall_path, bt_backup_dir=self.RPT_PNL2000_DIR ,pdfpath=self.BT_BACKUP_DIR2)

            # plot pnl since 2010
            self.series_name_dict.update({'drawdown':'drawdown_2010'})
            self.alpha_name_dict = {'ann_mean': 'ann_mean_2010',
                                    'ann_std': 'ann_std_2010',
                                    'ann_sharpe': 'ann_sharpe_2010',
                                    'calmar': 'calmar_2010',
                                    'avg_corr': 'avg_corr_2010'
                                    }
            pnl_chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_pnl_chart(root, self.new_wf.df,
                                                                                          self.series_name_dict,
                                                                                          self.new_wf.alpha,
                                                                                          self.alpha_name_dict)
            TCT.rates_tree_component_plot(plot_dict=pnl_chart_pack_dict, chart_start_dt='2010-01-01',
                                          chart_end_dt='2030-01-01', rate_rise_fall=self.rates_rise_fall_path,
                                          bt_backup_dir=self.RPT_PNL2010_DIR, pdfpath=self.BT_BACKUP_DIR2010_2)

class abstract_sig_genr_for_fwdGrow_tree(abstract_sig_genr):
    def __init__(self):
        super(abstract_sig_genr_for_fwdGrow_tree, self).__init__()

    def add_dir_info(self):
        self.WKDIR = os.path.dirname(os.path.realpath(__file__))
        self.PROJ_DIR = os.path.join(self.WKDIR, "..")
        self.MASTER_INPUT_DIR = os.path.join(self.PROJ_DIR, "input/master_input.xlsx")
        self.OUTPUT_DIR = os.path.join(self.PROJ_DIR, "output")
        self.SCRATCH_DIR = os.path.join(self.PROJ_DIR, "zzz_NO_commit_folder", self.Short_Name)
        self.INDICATOR_EXP_DIR = os.path.join(self.PROJ_DIR, "zzz_NO_commit_folder", self.Short_Name, 'indicator_group')
        self.TEMP_LOCAL_PICKLE = os.path.join(os.environ['TEMP'], 'TEMP_' + self.Short_Name + '_local_db.pickle')
        # Firstly export all the relevant result into the csv format. Secondly plot into the charts
        self.SHARE_DIR = r'Y:\MacroQuant\JYang\JY_Project'
        self.RPTDIR = os.path.join(self.SHARE_DIR, 'reporting', self.Short_Name) if os.access(self.SHARE_DIR, os.W_OK) else os.path.join(
            self.PROJ_DIR, 'reporting', self.Short_Name)
        #######
        self.IMPORT_DATA_DIR = os.path.join(self.WKDIR, '   ')





 








series_util.py





#@zy 20190313: this file contains all sorts of frequently used time series utility methods
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import panormus.data.bo.econ as econ
import statsmodels.api as sta_m
from pandas.tseries.offsets import BMonthEnd
import panormus.data.haver as haver
conn = econ.get_instance_using_credentials()
import os
WKDIR = os.path.dirname(os.path.realpath(__file__))

class date_utils_collection(object):
    def __init__(self):
        self.bday_index = pd.date_range('19450101',periods=25200,freq='B')

    def a_day_in_previous_month(self,dt):
        return dt.replace(day=1) - timedelta(days=1)

    def crosscorr(self,datax, datay, lag=0):
        """ Lag-N cross correlation.
        Parameters
        ----------
        lag : int, default 0
        datax, datay : pandas.Series objects of equal length

        Returns
        ----------
        crosscorr : float
        """
        datax,datay = datax.iloc[:,0],datay.iloc[:,0]
        return datay.corr(datax.shift(lag))

    def drop_duplicate(self,df):
        df.sort_index(inplace=True)
        return df.loc[~df.index.duplicated(keep='last')]

    def EOD_x_monthago(self,reference_dt, x):
        '''
        :param reference_dt: the reference date to deal with, input should be a string
        :param x: the month you want to move forward
        :return: will return the end of month of x before prior to the reference_dt
        '''
        reference_dt = datetime.strptime(reference_dt, '%Y-%m-%d')
        for i in range(x):
            reference_dt = self.a_day_in_previous_month(reference_dt)
        return reference_dt

    def FOD_x_monthago(self,reference_dt, x):
        '''
        :param reference_dt: the reference date to deal with, input should be a string
        :param x: the month you want to move forward
        :return: will return the end of month of x before prior to the reference_dt
        '''
        reference_dt = datetime.strptime(reference_dt, '%Y-%m-%d')
        for i in range(x):
            reference_dt = self.a_day_in_previous_month(reference_dt)
        return reference_dt.replace(day=1)

    def get_1st_index_date_as_str(self,df):
        df.sort_index(inplace=True)
        return df.index.strftime('%Y-%m-%d')[0]

    def first_day_or_last_day_of_a_month(self,dt):
        '''
        :param dt: identify whether this date is the first day of the month or last day of the month
        :return: True if LAST day, False if FIRST DAY
        '''
        return True if datetime.strptime(dt,'%Y-%m-%d').day>=15 else False

    def freq_conversion(self,freq):
        if freq.upper() == 'M':
            return 12
        if freq.upper() == 'Y':
            return 1
        if freq.upper() == 'Q':
            return 3
        if freq.upper() == "D":
            return 252

    def truncate_NAs(self,df,col=-1):
        '''
        :param df: this is to truncate the NA's at the beginning or at the end of the time series, ignore NAs in between though
        :param col: the number of col that has NAs, default set to the last column
        :return: the df tha has been truncated the NAs at the beginning of the dataframe and at the end of the dataframe
        '''
        first_idx = df.iloc[:,col].first_valid_index()
        last_idx = df.iloc[:,col].last_valid_index()
        return df.loc[first_idx:last_idx,:]

    def yoy_to_idx(self,df,freq='M', col=0,idx_col_name = 'new_Index'):
        '''
        :param df: convert from yoy change rate to index. NOTE: by default the index should be the date !!!
        :param freq: freq of the series, default as monthly
        :param col: the col number that contains the yoy change rate, default as the column 0
        :return: the original column PLUS the new column contains the index
        '''
        print ('please do not use method since it is inconsistent with Felix_s method')
        raise IOError
        first_date = self.get_1st_index_date_as_str(df) # now first_date is a string
        # identify if the date is the start of the month or end of the month
        if self.first_day_or_last_day_of_a_month(first_date) == True:
            insert_date = self.EOD_x_monthago(first_date,1)
        else:
            insert_date = self.FOD_x_monthago(first_date,1)

        #genr the inserted row a a list
        total_col = df.shape[1]
        insert_row = [0] * total_col
        insert_row = insert_row+[1]
        #print (insert_row)
        inserted_df = pd.DataFrame([insert_row],columns=[i for i in range(len(insert_row))],index=[insert_date])
        inserted_df.replace(0,np.nan,inplace=True)
        #print (inserted_df)

        df[idx_col_name] = np.nan
        inserted_df.columns = df.columns
        df = pd.concat([df,inserted_df],axis=0)
        # convert the str type to timestamp
        df.index = pd.to_datetime(df.index)
        df.sort_index(inplace=True)

        # convert the yoy to index using the fomula: index_t+1 = index_t*((1+yoy_t+1%/12))
        df.iloc[1:,-1] = (1+df.iloc[1:,col]/100/self.freq_conversion(freq))
        df.iloc[:,-1] = df.iloc[:,-1].cumprod()
        return df

    def yoy_from_idx(self,df,freq='M',col = 0,yoy_col_name = 'new_yoy'):
        '''
        :param df: the df that perform the yoy change calculation
        :param freq: freq of the calculation, default as monthly. This will be used in the annulisation calc
        :param col: the col number that
        :param yoy_col_name: new column name for the yoy changes
        :return:appending at the end of the column a yoy change
        '''
        mask = np.isnan(df.iloc[:,col])
        col = df.columns.tolist()[col]
        df.loc[~mask,yoy_col_name] = df.loc[~mask,col].pct_change(periods=12)*100
        return df

    def empty_M_df(self):
        x = [0]*2000
        df = pd.DataFrame({'1':x},index = pd.date_range('19200101',periods=2000,freq='M'))
        df['Date'] = df.index
        df.reset_index(drop=True,inplace=True)
        df['Date'] = [x.date() for x in df['Date']]
        df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]
        #print (type(df['Date'].values[0]))
        return df[['Date']]

    def empty_Q_df(self):
        x = [0]*460
        df = pd.DataFrame({'1':x},index = pd.date_range('19201201',periods=460,freq='Q'))
        df['Date'] = df.index
        df.reset_index(drop=True,inplace=True)
        df['Date'] = [x.date() for x in df['Date']]
        df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]
        #print (type(df['Date'].values[0]))
        return df[['Date']]

    def empty_BDay_df(self):
        x = [0]*252*100
        df = pd.DataFrame({'1':x},index = pd.date_range('19450101',periods=25200,freq='B'))
        df['Date'] = df.index
        df.reset_index(drop=True, inplace=True)
        df['Date'] = [x.date() for x in df['Date']]
        df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        return df[['Date']]

    def empty_Day_df(self):
        x = [0]*365*100
        df = pd.DataFrame({'1':x},index = pd.date_range('19450101',periods=36500,freq='D'))
        df['Date'] = df.index
        df.reset_index(drop=True, inplace=True)
        df['Date'] = [x.date() for x in df['Date']]
        df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        return df[['Date']]

    def empty_Week_df(self):
        x = [0]*52*100
        df = pd.DataFrame({'1':x},index = pd.date_range('19450101',periods=5200,freq='W'))
        df['Date'] = df.index
        df.reset_index(drop=True, inplace=True)
        df['Date'] = [x.date() for x in df['Date']]
        df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        return df[['Date']]

    def extend_df1_with_most_recent_df2(self,df1,df2,method='geo'):
        '''
        :param df1: the dataframe that you want to use whenever available, but release slowly. Example: UK HPI
        :param df2: the dataframe to extend df1, release faster, Example: UK rightmove HP index
        :param method: geo: scale factor is df1/df2
                        aris: scale factor is df1-df2
        :return: full series with df1's column name, or 0 if df1 is Series
        '''
        df1,df2 = df1.dropna(),df2.dropna()
        df1.index, df2.index = pd.to_datetime(df1.index), pd.to_datetime(df2.index)
        # check if df2 is longer than df1
        if df1.index[-1]<df2.index[-1]:
            # convert to dataframe
            df1 = df1.to_frame() if len(df1.shape) < 1.01 else df1
            df2 = df2.to_frame() if len(df2.shape) < 1.01 else df2
            common_date = df1.index.intersection(df2.index)[-1]
            if method=='geo':
                scale = df2.loc[common_date, :].iloc[0] / df1.loc[common_date, :].iloc[0]
                df2 = df2 / scale
            else:
                scale = df2.loc[common_date, :].iloc[0] - df1.loc[common_date, :].iloc[0]
                df2 = df2 - scale
            assert df2.dropna().shape[0] > 2, (df2.columns[0], 'splice method does not work, please check!!!')
            df2 = df2.loc[common_date:, :]
            df2.columns = df1.columns
            df1 = pd.concat([df1, df2], axis=0)
            return self.drop_duplicate(df1)
        else:
            return df1

    def extend_backward_df1_by_df2(self,df1,df2,method='geo'):
        '''
        :param df1: the dataframe that you want to use whenever available, but has short history. Example: 1w OIS curve
        :param df2: the dataframe that has longer history, but you don;t want to use now. Example: policy rate
        :param method: geo: scale factor is df1/df2
                        aris: scale factor df1-df2
        :return: full long series with df1's column name, or 0 if df1 is Series
        '''
        df1,df2 = df1.dropna(),df2.dropna()
        df1.index,df2.index = pd.to_datetime(df1.index),pd.to_datetime(df2.index)
        #convert to dataframe
        df1 = df1.to_frame() if len(df1.shape)<1.01 else df1
        df2 = df2.to_frame() if len(df2.shape)<1.01 else df2

        common_date = df1.index.intersection(df2.index)[0]
        if method=='geo':
            scale = df2.loc[common_date,:].iloc[0]/df1.loc[common_date,:].iloc[0]
            df2 = df2/scale
        else:
            scale = df2.loc[common_date,:].iloc[0]-df1.loc[common_date,:].iloc[0]
            df2 = df2 - scale
        assert df2.dropna().shape[0] > 2, (df2.columns[0], 'splice method does not work, please check!!!')
        df2 = df2.loc[:common_date,:]
        df2.columns = df1.columns
        df1 = pd.concat([df1,df2],axis=0)
        return self.drop_duplicate(df1)

    def extend_date_index(self,df,n):
        #https://stackoverflow.com/questions/34159342/extrapolate-pandas-dataframe/35960833
        pass

    def repeat_value(self,df, first_idx, last_idx, col_number=-1):
        col_name = df.columns.tolist()[col_number]
        df[col_name].fillna(method="ffill", inplace=True)
        df.loc[(df.index<first_idx)|(df.index>last_idx),col_name] = np.nan
        return df

    # @v.i.
    def rolling_ignore_nan(self,_df, _window, _func):
        _df = _df.rolling(window=_window, min_periods=1).apply(lambda x: _func(x[~np.isnan(x)]))
        idx_mask = (_df.index < _df.iloc[:, -1].dropna().index[_window - 1])
        _df.iloc[idx_mask] = np.nan
        return _df

    def conversion_to_FOM(self,df):
        '''
        simply convert to the first day of the month, no repeating is needed; no frequency change, no nothing!!
        :param df:
        :return:
        '''
        df = df.copy()
        if len(df.index) == 0:
            return df

        try:
            df['Date'] = pd.to_datetime(df.index)
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
        except:
            df['Date'] = df.index
            df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        df['Date'] = [datetime.strptime(dt, '%Y-%m-%d').replace(day=1) for dt in df['Date']]
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        return df

    def conversion_to_FOW(self,df):
        '''
        simply convert to the first day of the week, no repeating is needed; no frequency change, no nothing!!
        :param df:
        :return:
        '''
        df = df.copy()
        if len(df.index) == 0:
            return df

        try:
            df['Date'] = pd.to_datetime(df.index)
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
        except:
            df['Date'] = df.index
            df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        df['Date'] = [datetime.strptime(dt, '%Y-%m-%d')+timedelta(days=( - datetime.strptime(dt, '%Y-%m-%d').weekday())) for dt in df['Date']]
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        return df

    def conversion_to_m(self,df,method='repeat',col_to_repeat = -1):
        #convert the data series to from quarterly or longer to monthly frequency and repeat the values
        # OR set the monthly df date to the FIRST of the month
        '''
        convert the date of the data to the first day of the month, merge the data with a empty monthly time series
        :param df: dataframe, with date as its index
        :param col_to_repeat: the number of column of concerned
        :param method: method for and NAs in between, can choose either repeat or 'None'
        :return: return a series that is monthly freq, all repeated
        '''
        df = df.copy()
        if len(df.index) == 0:
            return df

        try:
            df['Date'] = pd.to_datetime(df.index)
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
        except:
            df['Date'] = df.index
            df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        df['Date'] = [datetime.strptime(dt, '%Y-%m-%d').replace(day=1) for dt in df['Date']]
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)

        # Second convert the Qtr freq into monthly freq:
        empty_m_ts = self.empty_M_df()
        empty_m_ts['Date'] = [datetime.strptime(dt, '%Y-%m-%d').replace(day=1) for dt in empty_m_ts['Date']]
        empty_m_ts['Date'] = pd.to_datetime(empty_m_ts['Date'])
        empty_m_ts.set_index('Date', inplace=True)
        # merge the quarterlt freq into monthly freq
        left = empty_m_ts
        right = df
        df_monthly = pd.merge(left, right, how='left', left_index=True, right_index=True)

        df_monthly.sort_index(inplace=True)

        # repeat all the col_to_repeat or col_to interpolate or fill values
        if not isinstance(col_to_repeat,list):
            col_to_repeat = [col_to_repeat]

        for col_num in col_to_repeat:
            first_idx = df_monthly.iloc[:,col_num].first_valid_index()
            last_idx = df_monthly.iloc[:,col_num].last_valid_index()

            # repeat and truncate the NA value at the beginning and end of the series
            if method=='repeat':
                df_monthly = self.repeat_value(df_monthly, first_idx, last_idx, col_num)
            elif method == 'interpolate':
                col_name = df.columns.tolist()[col_num]
                df_monthly.loc[:, col_name].interpolate(inplace=True,method='linear')
                df_monthly.loc[(df_monthly.index < first_idx) | (df_monthly.index > last_idx), col_name] = np.nan
            else:
                pass
        first_idx = df_monthly.first_valid_index()
        last_idx = df_monthly.last_valid_index()
        return df_monthly.loc[first_idx:last_idx, :]

    def conversion_to_q(self,df,method='repeat',col_to_repeat = -1,keep_end_dates_longer=False):
        #convert the data series to from quarterly or longer to monthly frequency and repeat the values
        # OR set the monthly df date to the FIRST of the month
        '''
        convert the date of the data to the first day of the month, merge the data with a empty monthly time series
        :param df: dataframe, with date as its index
        :param col_to_repeat: the number of column of concerned
        :param method: method for and NAs in between, can choose either repeat or 'None'
        :return: return a series that is monthly freq, all repeated
        '''
        df = df.copy()
        if len(df.index) <= 0.1:
            return df

        try:
            df['Date'] = pd.to_datetime(df.index)
            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')
        except:
            df['Date'] = df.index
            df['Date'] = [x.strftime('%Y-%m-%d') for x in df['Date']]

        df['Date'] = [datetime.strptime(dt, '%Y-%m-%d').replace(day=1) for dt in df['Date']]
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)

        # force all the date to the last month of the quarter. e.g.: 2019-10-01 => 2019-12-01
        new_idx = []
        for dt in df.index:
            if dt.month in [1,4,7,10]:
                new_idx.append(dt.replace(month=dt.month+2))
            elif dt.month in [2,5,8,11]:
                new_idx.append(dt.replace(month=dt.month + 1))
            else:
                new_idx.append(dt)
        df.index = new_idx
        df = df[~df.index.duplicated(keep='last')]

        # Second convert the whatever freq into Qtr freq:
        empty_q_ts = self.empty_Q_df()
        empty_q_ts['Date'] = [datetime.strptime(dt, '%Y-%m-%d').replace(day=1) for dt in empty_q_ts['Date']]
        empty_q_ts['Date'] = pd.to_datetime(empty_q_ts['Date'])
        empty_q_ts.set_index('Date', inplace=True)
        # merge the quarterlt freq into monthly freq
        left = empty_q_ts
        right = df
        df_q = pd.merge(left, right, how='left', left_index=True, right_index=True)

        df_q.sort_index(inplace=True)

        # repeat all the col_to_repeat or col_to interpolate or fill values
        if not isinstance(col_to_repeat,list):
            col_to_repeat = [col_to_repeat]

        for col_num in col_to_repeat:
            first_idx = df_q.iloc[:,col_num].first_valid_index()
            last_idx = df_q.iloc[:,col_num].last_valid_index()

            # repeat and truncate the NA value at the beginning and end of the series
            if method=='repeat':
                df_q = self.repeat_value(df_q, first_idx, last_idx, col_num)
            elif method == 'interpolate':
                col_name = df.columns.tolist()[col_num]
                df_q.loc[:, col_name].interpolate(inplace=True,method='linear')
                df_q.loc[(df_q.index < first_idx) | (df_q.index > last_idx), col_name] = np.nan
            else:
                pass
        first_idx = df_q.first_valid_index()
        last_idx = df_q.last_valid_index()
        if keep_end_dates_longer:
            return df_q.loc[first_idx:, :]
        else:
            return df_q.loc[first_idx:last_idx, :]

    def conversion_q_to_m(self,df,method='repeat',col_to_repeat = -1):
        # firstly, convert all date to the first day of the quarter: e.g. 2019-3-31 => 2019-01-01
        # second, at the end add an extra quarter-end month. should be like this:
        '''
            2000-01-01
            2000-04-01
            ...
            2019-01-01
            + 2019-03-01
        '''
        # merge with monthly and repeat the value. This should work with both haver tickers and EconDB tickers
        _df = df.copy()
        new_index = []
        if self.get_freq(_df) != 'Q':
            print ('Sorry, df passed is not a quarterly freq!!')
            raise ValueError
        for dt in _df.index:
            dt = dt.replace(day=1)
            if dt.month in [1,4,7,10]:
                new_index.append(dt)
            elif dt.month in [2,5,8,11]:
                dt = dt.replace(month=dt.month-1)
                new_index.append(dt)
            else:
                dt = dt.replace(month=dt.month - 2)
                new_index.append(dt)
        _df.index = new_index

        _df = _df.dropna()
        dt_last = _df.index[-1].replace(month=_df.index[-1].month+2)
        value_last = _df.values[-1]

        df_to_append = pd.DataFrame(index=[dt_last],data=value_last,columns = _df.columns)
        _df = _df.append(df_to_append)
        _df = self.conversion_to_m(_df,method=method,col_to_repeat=col_to_repeat)
        return _df

    def divide_df1_df2(self, df1, df2):
        df1, df2 = df1.copy(), df2.copy()
        assert len(df1.columns) == len(df2.columns), 'Sorry, the columns of the 2 dfs in devision are not the same'

        df1.index = pd.to_datetime(df1.index)
        df2.index = pd.to_datetime(df2.index)

        common_dates = df1.index.intersection(df2.index)

        df11 = df1.loc[common_dates, :]
        df22 = df2.loc[common_dates, :]

        df11 = df11.reset_index(drop=True)
        df22 = df22.reset_index(drop=True)

        df11.columns = range(len(df11.columns))
        df22.columns = range(len(df22.columns))

        df_final = df11 / df22
        df_final.index = common_dates
        df_final.columns = [i + '/(' + j + ')' for i, j in zip(df1.columns.tolist(), df2.columns.tolist())]
        return df_final

    def sum_df1_df2(self, df1, df2, na='drop'):
        df1, df2 = df1.copy(), df2.copy()
        assert len(df1.columns) == len(df2.columns), 'Sorry, the columns of the 2 dfs in devision are not the same'

        df1.index = pd.to_datetime(df1.index)
        df2.index = pd.to_datetime(df2.index)

        common_dates = df1.index.intersection(df2.index)

        df11 = df1.loc[common_dates, :]
        df22 = df2.loc[common_dates, :]

        df11 = df11.reset_index(drop=True)
        df22 = df22.reset_index(drop=True)

        df11.columns = range(len(df11.columns))
        df22.columns = range(len(df22.columns))

        if na == 'zero':
            df_final = df11.fillna(0) + df22.fillna(0)
        else:
            df_final = df11 + df22

        df_final.index = common_dates
        df_final.columns = [i + '+(' + j + ')' for i, j in zip(df1.columns.tolist(), df2.columns.tolist())]
        return df_final

    def minus_df1_df2(self, df1, df2,na='drop'):
        df1,df2 = df1.copy(),df2.copy()
        assert len(df1.columns) == len(df2.columns), 'Sorry, the columns of the 2 dfs in devision are not the same'

        df1.index = pd.to_datetime(df1.index)
        df2.index = pd.to_datetime(df2.index)

        common_dates = df1.index.intersection(df2.index)

        df11 = df1.loc[common_dates, :]
        df22 = df2.loc[common_dates, :]

        df11 = df11.reset_index(drop=True)
        df22 = df22.reset_index(drop=True)

        df11.columns = range(len(df11.columns))
        df22.columns = range(len(df22.columns))

        if na=='zero':
            df_final = df11.fillna(0) - df22.fillna(0)
        else:
            df_final = df11-df22

        df_final.index = common_dates
        df_final.columns = [i + '-(' + j + ')' for i, j in zip(df1.columns.tolist(), df2.columns.tolist())]
        return df_final
    # v.i.
    def conversion_down_to_m(self,df,method='repeat',col_to_repeat=-1, agg_method = 'last'):
        if len(df.index) == 0:
            return df

        _df = df.copy()
        _df.index = pd.to_datetime(_df.index)
        _df['yearrr'] = _df.index.year
        _df.index = pd.to_datetime(_df.index)#???
        _df['monthhh'] = _df.index.month
        if agg_method == 'last':
            _df = _df.groupby(['yearrr', 'monthhh']).last()
        if agg_method == 'sum':
            _df = _df.groupby(['yearrr', 'monthhh']).sum()
        new_index = []
        for i, yr in enumerate(_df.index.get_level_values(0).tolist()):
            mon = _df.index.get_level_values(1).tolist()[i]
            new_index.append(datetime(year=yr, month=mon, day=1))
        _df.index = new_index
        _df.index = pd.to_datetime(_df.index)

        # in case that there are missing month in between, merge with a monthly df
        if self.get_freq(_df) == 'Q':
            _df=self.conversion_q_to_m(_df)
        else:
            _df = self.conversion_to_m(_df,method=method,col_to_repeat=col_to_repeat)
        return _df
    #TODO: the conversion is not properly converting monthly to daily.
    # What should be done is to convert the monthly to the business day of that month!
    # and repeat to the last business of the current month
    def conversion_to_bDay(self,df, method='repeat', col_to_repeat=-1,keep_end_dates_longer=False):
        '''
        convert daily or slower frequency into business day frequency, for NAs using chosen method to fill in the NAs
        :param df:
        :param method: filling NA method can be repeat the previous value or fill in a specific number
        :param col_to_repeat:
        :return:
        '''
        df = df.dropna()
        if len(df.index) == 0:
            return df

        if self.get_freq(df) == 'M':
            index = [self.get_first_bday_of_month(i) for i in df.index]
            df.index = pd.to_datetime(index)
            df = df[~df.index.duplicated(keep='last')]
            df_bday = df.reindex(self.bday_index)
            df_bday.sort_index(inplace=True)
            # repeat all the col_to_repeat or col_to interpolate or fill values
            if not isinstance(col_to_repeat, list):
                col_to_repeat = [col_to_repeat]

            for col_num in col_to_repeat:
                first_idx = df_bday.iloc[:, col_num].first_valid_index()
                last_idx = self.get_last_bday_of_month(df_bday.dropna().index[-1])

                # repeat and truncate the NA value at the beginning and end of the series
                if method == 'repeat':
                    df_bday = self.repeat_value(df_bday, first_idx, last_idx, col_num)
                elif method == 'interpolate':
                    col_name = df.columns.tolist()[col_num]
                    df_bday.loc[:, col_name].interpolate(inplace=True, method='linear')
                    df_bday.loc[(df_bday.index < first_idx) | (df_bday.index > last_idx), col_name] = np.nan
                else:
                    pass

        elif self.get_freq(df) == 'Q':
            # firstly conversion to monthly data, then from monthly to bday, so that the repeat method would work
            df = self.conversion_down_to_m(df)
            index = [self.get_first_bday_of_month(i) for i in df.index]
            df.index = pd.to_datetime(index)
            df = df[~df.index.duplicated(keep='last')]
            df_bday = df.reindex(self.bday_index)
            df_bday.sort_index(inplace=True)
            # repeat all the col_to_repeat or col_to interpolate or fill values
            if not isinstance(col_to_repeat, list):
                col_to_repeat = [col_to_repeat]

            for col_num in col_to_repeat:
                first_idx = df_bday.iloc[:, col_num].first_valid_index()
                last_idx = df_bday.dropna().index[-1]
                last_idx = self.get_last_bday_of_month(last_idx)

                # repeat and truncate the NA value at the beginning and end of the series
                if method == 'repeat':
                    df_bday = self.repeat_value(df_bday, first_idx, last_idx, col_num)
                elif method == 'interpolate':
                    col_name = df.columns.tolist()[col_num]
                    df_bday.loc[:, col_name].interpolate(inplace=True, method='linear')
                    df_bday.loc[(df_bday.index < first_idx) | (df_bday.index > last_idx), col_name] = np.nan
                else:
                    pass

        elif self.get_freq(df)=='W':
            # convert data release on Sun, Sat to previous Fri.
            # For example: 3-31 is a Sat, then assume the data release is on 3-30, remove the Friday release then
            index = [self.get_previous_Friday(i) for i in df.index]
            df.index = pd.to_datetime(index)
            df = df[~df.index.duplicated(keep='last')]

            df_bday = df.reindex(self.bday_index)
            df_bday.sort_index(inplace=True)
            # repeat all the col_to_repeat or col_to interpolate or fill values
            if not isinstance(col_to_repeat, list):
                col_to_repeat = [col_to_repeat]
            for col_num in col_to_repeat:
                first_idx = df_bday.iloc[:, col_num].first_valid_index()
                last_idx = df_bday.iloc[:, col_num].last_valid_index()
                last_idx_plus_1_week = df_bday.index.get_loc(last_idx)+4
                last_idx = df_bday.index[last_idx_plus_1_week]
                # repeat and truncate the NA value at the beginning and end of the series
                if method == 'repeat':
                    df_bday = self.repeat_value(df_bday, first_idx, last_idx, col_num)
                elif method == 'interpolate':
                    col_name = df.columns.tolist()[col_num]
                    df_bday.loc[:, col_name].interpolate(inplace=True, method='linear')
                    df_bday.loc[(df_bday.index < first_idx) | (df_bday.index > last_idx), col_name] = np.nan
                else:
                    pass
        else:
            # convert data release on Sun, Sat to previous Fri.
            # For example: 3-31 is a Sat, then assume the data release is on 3-30, remove the Friday release then
            index = [self.get_previous_Friday(i) for i in df.index]
            df.index = pd.to_datetime(index)
            df = df[~df.index.duplicated(keep='last')]

            df_bday = df.reindex(self.bday_index)
            df_bday.sort_index(inplace=True)
            # repeat all the col_to_repeat or col_to interpolate or fill values
            if not isinstance(col_to_repeat, list):
                col_to_repeat = [col_to_repeat]
            for col_num in col_to_repeat:
                first_idx = df_bday.iloc[:, col_num].first_valid_index()
                last_idx = df_bday.iloc[:, col_num].last_valid_index()

                # repeat and truncate the NA value at the beginning and end of the series
                if method == 'repeat':
                    df_bday = self.repeat_value(df_bday, first_idx, last_idx, col_num)
                elif method == 'interpolate':
                    col_name = df.columns.tolist()[col_num]
                    df_bday.loc[:, col_name].interpolate(inplace=True, method='linear')
                    df_bday.loc[(df_bday.index < first_idx) | (df_bday.index > last_idx), col_name] = np.nan
                else:
                    pass
        first_idx = df_bday.first_valid_index()
        last_idx = df_bday.last_valid_index()
        #adding longer dates for the purpose of shifting
        if not keep_end_dates_longer:
            return df_bday.loc[first_idx:last_idx, :]
        else:
            df_bday = df_bday.reindex(self.bday_index)
            df_bday.sort_index(inplace=True)
            return df_bday.loc[first_idx:, :]

    def conversion_to_Day(self,df,method='repeat',col_to_repeat=-1):
        '''
        convert daily or slower frequency into business day frequency, for NAs using chosen method to fill in the NAs
        :param df:
        :param method: filling NA method can be repeat the previous value or fill in a specific number
        :param col_to_repeat:
        :return:
        '''

        if len(df.index) == 0:
            return df

        #first perform conversoin_to_bday to make sure the quarterly is taken care of
        #df = self.conversion_to_bDay(df,method=method,col_to_repeat=col_to_repeat)

        empty_day = self.empty_Day_df()
        empty_day['Date'] = pd.to_datetime(empty_day['Date'])
        empty_day.set_index('Date', inplace=True)

        left = empty_day
        right = df
        df_day = pd.merge(left, right, how='left', left_index=True, right_index=True)

        df_day.sort_index(inplace=True)
        # repeat all the col_to_repeat or col_to interpolate or fill values
        if not isinstance(col_to_repeat, list):
            col_to_repeat = [col_to_repeat]

        for col_num in col_to_repeat:
            first_idx = df_day.iloc[:, col_num].first_valid_index()
            last_idx = df_day.iloc[:, col_num].last_valid_index()

            # repeat and truncate the NA value at the beginning and end of the series
            if method == 'repeat':
                df_day = self.repeat_value(df_day, first_idx, last_idx, col_num)
            elif method == 'interpolate':
                col_name = df.columns.tolist()[col_num]
                df_day.loc[:, col_name].interpolate(inplace=True, method='linear')
                df_day.loc[(df_day.index < first_idx) | (df_day.index > last_idx), col_name] = np.nan
            else:
                pass
        first_idx = df_day.first_valid_index()
        last_idx = df_day.last_valid_index()
        return df_day.loc[first_idx:last_idx,:]

    # TODO: create a merge function that merge the 2 dataframe for different frequency
    def df1_to_df2_freq_merge(self,df1,df2,how='outer',reverse_order = False):
        # convert df1's freq to df2's freq and merge
        df1,df2 = df1.copy(),df2.copy()
        freq1 = self.get_freq(df1)
        freq2 = self.get_freq(df2)
        if freq1 == freq2:
            if reverse_order:
                df1,df2 = df2,df1
            return pd.merge(df1,df2,left_index=True,right_index=True,how=how)
        elif freq2 == 'D':
            df1 = self.conversion_to_bDay(df1)
            if reverse_order:
                df1, df2 = df2, df1
            return pd.merge(df1, df2, left_index=True, right_index=True, how=how)
        elif freq2 == 'W':
            df1 = self.conversion_to_bDay(df1)
            common_dates = df1.index.intersection(pd.to_datetime(df2.index))
            assert len(common_dates)>26,'Sorry, the length of common dates is too few, check if correct'
            df1 = df1.reindex(common_dates)
            if reverse_order:
                df1, df2 = df2, df1
            return pd.merge(df1, df2, left_index=True, right_index=True, how=how)
        elif freq2 == 'M':
            df1 = self.conversion_down_to_m(df1)
            if reverse_order:
                df1, df2 = df2, df1
            return pd.merge(df1, df2, left_index=True, right_index=True, how=how)
        elif freq2 == 'Q':
            df1 = self.conversion_to_q(df1)
            if reverse_order:
                df1, df2 = df2, df1
            return pd.merge(df1, df2, left_index=True, right_index=True, how=how)
        else:
            print ('the freq is : ',freq2,'We dont currently support this!!!!!')
            raise ValueError

    def df1_to_df2_freq_and_multiply(self,df1,df2,how='outer'):
        # convert df1's freq to df2 and multiply
        df1,df2 = df1.copy(),df2.copy()
        assert len(df1.columns)==1
        assert len(df2.columns)==1
        df_comb = self.df1_to_df2_freq_merge(df1,df2,how=how)
        col_name = df1.columns[0]+' multiply '+df2.columns[0]
        df_comb[col_name] = df_comb.iloc[:,0]*df_comb.iloc[:,1]
        return df_comb[[col_name]]

    def get_freq(self,df):
        # check the frequency of the df, return either 'D' or 'M', or 'Y', FOR THE LATEST PERIOD OF TIME!!!!
        # check the frequency of the df, return either 'D' or 'M', or 'Y', FOR THE LATEST PERIOD OF TIME!!!!
        if len(df.index) < 0.001:
            return 'invalid_name'
        df.index = pd.to_datetime(df.index)
        l1 = df.dropna().tail(20).index.tolist()
        l2 = df.dropna().tail(21).index.tolist()[:20]

        # print (len(l1),len(l2))

        diff = np.array([(z1 - z2).days for z1, z2 in zip(l1, l2)])
       # print (diff)

        avg_diff = diff.mean()

        if avg_diff <= 2:
            return 'D'
        elif avg_diff <= 10:
            return 'W'
        elif avg_diff <= 40:
            return 'M'
        elif avg_diff <= 105:
            return 'Q'
        else:
            return 'Y'

    def delete_zero_beginning(self,df):
        return df.loc[df.replace(0,np.nan,inplace=False).first_valid_index():,:]

    def delete_zero_tail(self,df):
        mask = (df.index<df.replace(0,np.nan,inplace=False).last_valid_index())
        return df.loc[mask,:]

    def fill_backward_with_zero(self,df):
        df = df.copy()
        for col in df.columns:
            mask = (df.index<df.loc[:,[col]].first_valid_index())
            df.loc[mask,col]=0
        return df

    def get_previous_Friday(self,dt):
        if not isinstance(dt, datetime):
            try:
                dt = datetime.strptime(dt, '%Y-%m-%d')
            except:
                dt = dt.to_pydatetime('%Y-%m-%d')

        if dt.weekday() not in [5, 6]:
            return dt
        else:
            return dt - timedelta(dt.weekday() - 4)

    def get_first_bday_of_month(self, dt):
        if not isinstance(dt, datetime):
            try:
                dt = datetime.strptime(dt, '%Y-%m-%d')
            except:
                dt = dt.to_pydatetime('%Y-%m-%d')
        dt = dt.replace(day=1)
        if dt.weekday() not in [5,6]:
            return dt
        else:
            return dt + timedelta(days= 7-dt.weekday())

    def get_last_bday_of_month(self, dt):
        if not isinstance(dt, datetime):
            try:
                dt = datetime.strptime(dt, '%Y-%m-%d')
            except:
                dt = dt.to_pydatetime('%Y-%m-%d')

        offset = BMonthEnd()
        # Last day of current month
        return offset.rollforward(dt)

    def get_last_bday_of_quarter(self, dt):
        if not isinstance(dt, datetime):
            try:
                dt = datetime.strptime(dt, '%Y-%m-%d')
            except:
                dt = dt.to_pydatetime('%Y-%m-%d')

        dt = dt+timedelta(days=80)
        offset = BMonthEnd()
        # Last day of current month
        return offset.rollforward(dt)

    def smooth_change(self,df,periods, ann = False,ann_type = 'geo',col = -1):
        '''
        return: calculate the yoy etc with back-end of the data smoothed
        :param periods: if yoy, then 12
        :param type: the way to annualise it
        '''
        if periods<1.01:
            if self.get_freq(df.dropna())=='D':
                ann_factor=252
            elif self.get_freq(df.dropna())=='M':
                ann_factor=12
            elif self.get_freq(df.dropna())=='Q':
                ann_factor=4
            else:
                ann_factor=1
            return df.dropna().pct_change(1)*ann_factor if ann_type=='geo' else df.dropna().diff(1)*ann_factor
        df1 = df.iloc[:, [col]].dropna()
        periods2 = int(periods*1.5)
        if self.get_freq(df1) == 'M':
            ann_factor = 12/(periods2/2-0.5)
        elif self.get_freq(df1) == 'Q':
            print ('sorry the series is quarterly, use simple percentage change!')
            raise ValueError
        elif self.get_freq(df1)=='D':
            ann_factor = 252/(periods/2)

        if ann_type=='geo':
            if ann:
                return (df1/self.rolling_ignore_nan(df1,periods2,np.mean))**ann_factor-1
            else:
                return (df1 / self.rolling_ignore_nan(df1, periods2, np.mean)) - 1
        else:
            if ann:
                return (df1-self.rolling_ignore_nan(df1,periods2,np.mean))*ann_factor
            else:
                return (df1-self.rolling_ignore_nan(df1,periods2,np.mean))

    def sea_adj(self,df):
        df1 = df.copy()
        df.iloc[:,0] = sta_m.tsa.x13_arima_analysis(df1.dropna(),x12path=WKDIR).seasadj
        return df

    def common_index(self,df1,df2):
        return sorted(list(set(df1.index.tolist()).intersection(set(df2.index.tolist()))))

    def hysteresis(self,df,method='bucket',bin=[-np.inf,-4,-3,-2,-1,0,1,2,3,4, np.inf],value = [-4,-3,-2,-1,0,1,2,3,4,4.01],new_name = 'discret_value',col = -1):
        df1 = df.copy()
        if method=='bucket':
            df1[new_name] = pd.cut(df1.iloc[:,col], bin, labels=value)
        return df1

    def splice_geometric_a(self,s_1, s_2, new_name='longer_series'):
        """splice 2 series together, s_1 has precedence over s_2"""
        # test if first date of series one exists in series 2
        if len(s_1.dropna().index.intersection(s_2.dropna().index)) == 0:
            raise ValueError("Series dates do not overlap, cannot compute scale factor for series 2.")
        s_1.dropna(inplace=True)
        s_2.dropna(inplace=True)

        first_common = s_1.index.intersection(s_2.index)[0]

        scale_s_2 = s_1.loc[first_common, :].values / s_2.loc[first_common, :].values

        # scale second series to match the first
        s_2_truncated_scaled = s_2.loc[:s_1.index[0]] * scale_s_2
        s_2_truncated_scaled.drop(s_1.index[0], errors='ignore', inplace=True)
        # s_2_truncated = s_2.loc[:s_1.index[0]].iloc[:-1] * s_1[0] / s_2[s_1.index[0]]

        # change the column name
        s_2_truncated_scaled.columns=[0]
        s_1.columns = [0]

        # concatenate truncated time-series
        output_series = pd.concat([s_2_truncated_scaled, s_1], axis=0).sort_index()
        output_series.columns = [new_name]

        return output_series

    def check_lag_enough(self,_df=[]):
        for i,df in enumerate(_df):
            if df.dropna().index[-1]<pd.to_datetime(datetime.today().date()):
                print ('Sorry, the last date in df : ',df.columns[0],'is: ',df.dropna().index[-1].strftime('%Y-%m-%d'))
                print (df.dropna().index[-1])
                print (pd.to_datetime(datetime.today().date()))
                #raise ValueError

    def remove_outlier(self,df,frac=0.01,n=3):
        import Analytics.loess_filter as lf
        assert len(df.columns)<1.01,'only 1 columns can be taken!!'
        df = df.copy().dropna()
        y_ticker = df.columns.tolist()[0]
        Lo_Filter = lf.loess_filter(df, None, y_ticker, None, False, frac)
        df_trend = Lo_Filter.estimate()[['y_fitted']]
        df_trend.columns = df.columns
        dif = abs(df-df_trend)
        dif.columns = df.columns
        thres = dif.iloc[:,0].mean()*n
        mask = dif.iloc[:,0]>thres
        df.loc[mask,:] = np.nan
        if self.get_freq(df) == 'D':
            return self.conversion_to_bDay(df)
        elif self.get_freq(df)=='M':
            return self.conversion_down_to_m(df)
        elif self.get_freq(df)=='Q':
            return self.conversion_to_q(df)
        else:
            return df.fillna(method='ffill')

 












chart.py





import math
from matplotlib.backends.backend_pdf import PdfPages
from datetime import datetime,timedelta
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from textwrap import wrap
import matplotlib.dates as mdates
import shutil
import collections
import os
WKDIR = os.path.dirname(os.path.realpath(__file__))


WKDIR = os.path.dirname(os.path.realpath(__file__))
PROJ_DIR = os.path.join(WKDIR,"..")

def parse_chart_param_for_bt(master_input,short_name,ser_container,pack_name = 'Component',customed_titles = {}):
    """
    :param master_input: the master input in xlsx
    :param short_name: the short name of the strategy
    :param ser_container: the object that contains all the series, should all be new_wf
    :return: return a list of object that has parameters all read through the xls
    """
    #init
    df_param = pd.read_excel(master_input,sheet_name=short_name,header=0,index_col=False)
    df_param = df_param.loc[df_param['Chart_pack'] == pack_name,:]
    #deal with NA
    df_param.fillna('invalid_name',inplace=True)
    _chart = collections.OrderedDict()
    _chart_n = collections.OrderedDict()  # number of lines
    _chart_title = collections.OrderedDict()
    _chart_color = collections.OrderedDict()
    _chart_style = collections.OrderedDict()
    _chart_twox = collections.OrderedDict()
    _chart_ratecycle = collections.OrderedDict()
    _chart_rate_rise_fall_file = collections.OrderedDict()

    #initialise the pack
    name_list = df_param['factor_name'].values.tolist()
    for name in name_list:
        _chart[name] = []
        _chart_n[name] = []
        _chart_title[name] = []
        _chart_color[name] = []
        _chart_style[name] = []
        _chart_twox[name] = []
        _chart_ratecycle[name] = []
        _chart_rate_rise_fall_file[name] = []
    #position holder
    name = 'zzz_blank'
    _chart[name] = []
    _chart_n[name] = []
    _chart_title[name] = []
    _chart_color[name] = []
    _chart_style[name] = []
    _chart_twox[name] = []
    _chart_ratecycle[name] = []
    _chart_rate_rise_fall_file[name] = []
    _chart['zzz_blank'].append(0)
    _chart_n['zzz_blank'].append(0)
    _chart_title['zzz_blank'].append(0)
    _chart_color['zzz_blank'].append(0)
    _chart_style['zzz_blank'].append(0)
    _chart_twox['zzz_blank'].append(0)
    _chart_ratecycle['zzz_blank'].append(0)
    _chart_rate_rise_fall_file['zzz_blank'].append(0)

    # parse the parameters line by line
    for i in range(df_param.shape[0]):
        this_df_param = df_param.iloc[[i],:]
        this_name = this_df_param['factor_name'].tolist()[0]
        if this_name.split('_')[0] != 'zzz':
            #parse series tuple
            ser_list = this_df_param.loc[:,['series1','series2','series3','series4']].values.tolist()[0]
            try:
                ser_list = [i for i in ser_list if i!='invalid_name']
                ser_tuple = tuple([ser_container.df[s] for s in ser_list])
                _chart[this_name].append(ser_tuple)

                #parse title
                title = this_df_param.loc[:,'title'].tolist()[0]
                title = '' if title == 'invalid_name' else title
                # check if title is customed
                if 'custom' in title.split('_',1)[0]:
                    title = customed_titles[title.split('_',1)[1]]
                else:
                    title = title
                _chart_title[this_name].append(title)

                # parse number
                _chart_n[this_name].append(len(ser_list))

                # parse color
                color_list = this_df_param.loc[:,['color1','color2','color3','color4']].values.tolist()[0]
                color_list = [i for i in color_list if i!='invalid_name']
                _chart_color[this_name].append(tuple(color_list))

                # parse style
                style_list = this_df_param.loc[:,['style1','style2','style3','style4']].values.tolist()[0]
                style_list = [i for i in style_list if i!='invalid_name']
                _chart_style[this_name].append(tuple(style_list))

                # parse twox
                twox = this_df_param.loc[:,'twox'].tolist()[0]
                _chart_twox[this_name].append(twox)

                #parse ratecycle
                ratecycle = this_df_param.loc[:,'ratecycle'].tolist()[0]
                _chart_ratecycle[this_name].append(ratecycle)

                #parse rate_raise_fall_file
                rise_fall = this_df_param.loc[:,'rate_rise_fall_file'].tolist()[0]
                _chart_rate_rise_fall_file[this_name] = _chart_rate_rise_fall_file[this_name] + [os.path.join(PROJ_DIR,rise_fall)]
            except:
                pass

    name_list_set = []
    for i in name_list:
        if i not in name_list_set or (i.split('_')[0] == 'zzz'):
            name_list_set.append(i)
    return name_list_set,_chart, _chart_title,_chart_n, _chart_color, _chart_style,_chart_twox,_chart_ratecycle,_chart_rate_rise_fall_file

def temp_charts(_component_name_list,_comp_chart,_comp_chart_title,_comp_chart_n,_comp_chart_color,_comp_chart_style,_comp_chart_twox,_comp_chart_ratecycle,_comp_rate_rise_fall_file=None,pdfpath = '',bt_backup_dir = None,start_dt='2000-01-01',end_dt=datetime.now().strftime('%Y-%m-%d'),page_name='',RPTDIR='',rise_dates_path=''):
    # firstly extract the charts parameter into a plain list, and plot one by one
    iso_page = []
    dfs = []
    titles = []
    number_of_lines = []
    colors = []
    styles = []
    two_x = []
    rate_cycle = []
    rate_rise_fall = []

    # create iso - id pairs, extract the title, n, color and style

    for factr in _component_name_list:
        #print (factr)
        n_of_charts = len(_comp_chart[factr])
        iso_page = iso_page + [factr + '_' + str(i) for i in range(n_of_charts)]
        #print ([factr + '_' + str(i) for i in range(n_of_charts)])
        dfs = dfs + _comp_chart[factr]
        titles = titles + _comp_chart_title[factr]
        number_of_lines = number_of_lines + _comp_chart_n[factr]
        #print (_comp_chart_n[factr])
        colors = colors + _comp_chart_color[factr]
        styles = styles + _comp_chart_style[factr]
        two_x = two_x + _comp_chart_twox[factr]
        rate_cycle = rate_cycle+_comp_chart_ratecycle[factr]
        if _comp_rate_rise_fall_file is None:
            rate_rise_fall = rate_rise_fall + [rise_dates_path]*n_of_charts
        else:
            if _comp_rate_rise_fall_file[factr] not in ['invalid_name']:
                rate_rise_fall = rate_rise_fall + _comp_rate_rise_fall_file[factr]
            else:
                rate_rise_fall = rate_rise_fall + [rise_dates_path]

    chart_each_page = 12
    chart_rows = 4
    chart_cols = 3

    pages_number = math.ceil(len(iso_page) / chart_each_page)
    chart_in_page = [chart_each_page] * (pages_number - 1) + [
        len(iso_page) - chart_each_page * (pages_number - 1)]

    report = PdfPages(pdfpath)

    print('chart_in_each_page=', chart_in_page)
    print("CREATING RETURNS PAGE")

    # split iso codes into each page!
    for i, n in enumerate(chart_in_page):
        chart_start_dt = start_dt
        chart_end_dt = end_dt

        chart_rows, chart_cols = chart_rows, chart_cols
        fig, axarr = plt.subplots(chart_rows, chart_cols, figsize=(18.27, 12.69), dpi=100)
        # add the main title
        if page_name !='':
            fig.suptitle(page_name, fontsize=16)
        print('the current page is: ', i)
        start_idx = sum(chart_in_page[:i])
        end_idx = start_idx + n
        df_in_this_page = iso_page[start_idx:end_idx]

        dfs_in_this_page, title_itp, n_itp, color_itp, stype_itp ,twox_itp,rate_cycle_itp,rate_rise_fall_path_itp = dfs[start_idx:end_idx], titles[
                                                                                           start_idx:end_idx], number_of_lines[
                                                                                                               start_idx:end_idx], colors[
                                                                                                                                   start_idx:end_idx], styles[
                                                                                                                                                       start_idx:end_idx],two_x[start_idx:end_idx],rate_cycle[start_idx:end_idx],rate_rise_fall[start_idx:end_idx]

        # print (df_in_this_page)
        for i in range(chart_rows):
            for j in range(chart_cols):
                if i * chart_cols + j < len(df_in_this_page):
                    ax = axarr[i, j]
                    id = df_in_this_page[i * chart_cols + j]
                    if id.split('_')[0]=='zzz':
                        print ('this is zzz')
                        set_ax_invisible(ax)
                        continue

                    current_dfs, current_title, current_n, current_color, current_style,current_twox,current_rate_cycle,current_rate_rise_path = dfs_in_this_page[
                                                                                              i * chart_cols + j], \
                                                                                          title_itp[i * chart_cols + j], \
                                                                                          n_itp[i * chart_cols + j], \
                                                                                          color_itp[i * chart_cols + j], \
                                                                                          stype_itp[i * chart_cols + j], twox_itp[i * chart_cols + j],rate_cycle_itp[i * chart_cols + j],rate_rise_fall_path_itp[i * chart_cols + j]
                    print(current_n)
                    print(i, j, id)
                    # extract the iso code and des for this country
                    current_iso = '_'.join(id.split('_')[:-1])
                    number = int(id.split('_')[-1])

                    if (current_n > 1.1) & (current_n < 2.2) :
                        double_line = True
                        tri_line = False
                        df1 = current_dfs[0]
                        df2 = current_dfs[1]
                    elif (current_n > 2.9):
                        tri_line = True
                        double_line  =False
                        df1 = current_dfs[0]
                        df2 = current_dfs[1]
                        df3 = current_dfs[2]
                    else:
                        try:
                            df1 = current_dfs[0]
                        except:
                            df1 = current_dfs
                        double_line = False
                        tri_line = False

                    #print (df1)

                    mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                    df1 = df1.loc[mask, :]
                    df1 = df1.dropna()

                    if double_line|tri_line:
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]
                        df2 = df2.dropna()
                    if tri_line:
                        mask = (df3.index >= chart_start_dt) & (df3.index <= chart_end_dt)
                        df3 = df3.loc[mask, :]
                        df3 = df3.dropna()

                    # print (df)
                    x1 = pd.to_datetime(df1.index).date
                    if double_line|tri_line:
                        x2 = pd.to_datetime(df2.index).date
                    if tri_line:
                        x3 = pd.to_datetime(df3.index).date

                    y1 = df1.ix[:, 0]
                    if double_line|tri_line:
                        y2 = df2.ix[:, 0]
                    if tri_line:
                        y3 = df3.ix[:, 0]

                    # enable to draw shaded line
                    if current_style[0] == 'shaded_with_line':
                        line1 = ax.plot(x1, y1, color=current_color[0], ls='solid', lw=0.4,
                                        label=df1.columns.tolist()[0])
                        line_shade1 = ax.fill_between(x1,0,y1,facecolors='aqua',alpha=0.3,label='_nolabel_')
                    else:
                        line1 = ax.plot(x1, y1, color=current_color[0], ls=current_style[0], lw=0.9,
                                        label=df1.columns.tolist()[0])
                    if double_line:
                        # allow the possibility of shaded_with_line
                        if current_twox == 'enable':
                            two_ax = True
                            ax2 = ax.twinx()
                            if current_style[1] == 'shaded_with_line':
                                line2 = ax2.plot(x2, y2, color=current_color[1], ls='solid', lw=0.4,
                                                 label=df2.columns.tolist()[0])
                                line_shade2 = ax2.fill_between(x2, 0, y2, facecolors='aqua', alpha=0.3,
                                                              label='_nolabel_')
                            else:
                                line2 = ax2.plot(x2, y2, color=current_color[1], ls=current_style[1], lw=0.9,
                                                 label=df2.columns.tolist()[0])
                        elif (current_twox == 'automatic') and (np.mean(np.abs(y1-y2))>10): #or np.max(y1/y2)>10 or np.max(y1/y2) < 0.1 or np.min(y1/y2) > 10 or np.min(y1/y2) < 0.1:
                            two_ax = True
                            ax2 = ax.twinx()
                            if current_style[1] == 'shaded_with_line':
                                line2 = ax2.plot(x2, y2, color=current_color[1], ls='solid', lw=0.4,
                                                 label=df2.columns.tolist()[0])
                                line_shade2 = ax2.fill_between(x2, 0, y2, facecolors='aqua', alpha=0.3,
                                                               label='_nolabel_')
                            else:
                                line2 = ax2.plot(x2, y2, color=current_color[1], ls=current_style[1], lw=0.9,
                                                 label=df2.columns.tolist()[0])
                        else:
                            two_ax = False
                            if current_style[1] == 'shaded_with_line':
                                line2 = ax.plot(x2, y2, color=current_color[1], ls='solid', lw=0.4,
                                                 label=df2.columns.tolist()[0])
                                line_shade2 = ax.fill_between(x2, 0, y2, facecolors='aqua', alpha=0.3,
                                                               label='_nolabel_')
                            else:
                                line2 = ax.plot(x2, y2, color=current_color[1], ls=current_style[1], lw=0.9,
                                                 label=df2.columns.tolist()[0])

                    if tri_line:
                        if current_twox == 'disable-enable':
                            ax2 = ax.twinx()
                            line2 = ax.plot(x2, y2, color=current_color[1], ls=current_style[1], lw=0.9,
                                             label=df2.columns.tolist()[0])
                            line3 = ax2.plot(x3, y3, color=current_color[2], ls=current_style[2], lw=0.9,
                                            label=df3.columns.tolist()[0])

                    if not (double_line or tri_line):
                        lns = line1
                    elif double_line:
                        lns=line1+line2
                    else:
                        lns = line1+line2+line3

                    labs = [l.get_label() for l in lns]
                    ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                    # set the range for the line1
                    if not (double_line or tri_line):
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max-l1_min
                        ymax = l1_max+l1_diff*.2
                        ymin = l1_min-l1_diff*.2
                        ax.set_ylim(ymin,ymax)

                    if double_line:
                        if two_ax:
                            l1_max = df1.loc[chart_start_dt:].max().values
                            l1_min = df1.loc[chart_start_dt:].min().values
                            l1_diff = l1_max - l1_min
                            ymax = l1_max + l1_diff * .2
                            ymin = l1_min - l1_diff * .2
                            ax.set_ylim(ymin, ymax)

                            l2_max = df2.loc[chart_start_dt:].max().values
                            l2_min = df2.loc[chart_start_dt:].min().values
                            l2_diff = l2_max - l2_min
                            ymax = l2_max + l2_diff * .2
                            ymin = l2_min - l2_diff * .2
                            ax2.set_ylim(ymin, ymax)
                        if not two_ax:
                            l1_max = df1.loc[chart_start_dt:].max().values
                            l1_min = df1.loc[chart_start_dt:].min().values
                            l1_diff = l1_max - l1_min

                            l2_max = df2.loc[chart_start_dt:].max().values
                            l2_min = df2.loc[chart_start_dt:].min().values
                            l2_diff = l2_max - l2_min

                            ymax = (l1_max + l1_diff * .2) if l1_max>l2_max else (l2_max + l2_diff * .2)
                            ymin = (l1_min - l1_diff * .2) if l1_min<l2_min else (l2_min - l2_diff * .2)
                            ax.set_ylim(ymin, ymax)

                    if tri_line:
                        if current_twox == 'disable-enable':
                            l1_max = df1.loc[chart_start_dt:].max().values
                            l1_min = df1.loc[chart_start_dt:].min().values
                            l1_diff = l1_max - l1_min
                            ymax = l1_max + l1_diff * .2
                            ymin = l1_min - l1_diff * .2
                            ax.set_ylim(ymin, ymax)

                            l3_max = df3.loc[chart_start_dt:].max().values
                            l3_min = df3.loc[chart_start_dt:].min().values
                            l3_diff = l3_max - l3_min
                            ymax = l3_max + l3_diff * .2
                            ymin = l3_min - l3_diff * .2
                            ax2.set_ylim(ymin, ymax)

                    ax.set_xlabel('')
                    ax.set_ylabel('')
                    if double_line:
                        if two_ax:
                            ax2.set_xlabel('')
                            ax2.set_ylabel('')
                    if tri_line:
                        if current_twox == 'disable-enable':
                            ax2.set_xlabel('')
                            ax2.set_ylabel('')

                    # wrap up the title since it can be too long
                    title = "\n".join(wrap(current_iso + ' ' + current_title, 60))
                    ax.set_title(title, y=1, fontsize=11, fontweight=600)

                    ax.tick_params(labelsize=12, width=0.1)
                    if double_line:
                        if two_ax:
                            ax2.tick_params(labelsize=12, width=0.1)
                        # change the y tick label to blue
                    if tri_line:
                        if current_twox == 'disable-enable':
                            ax2.tick_params(labelsize=12, width=0.1)
                    ax.tick_params(axis='y', labelcolor=current_color[0])
                    if double_line:
                        if two_ax:
                            ax2.tick_params(axis='y', labelcolor=current_color[1])
                    if tri_line:
                        if current_twox == 'disable-enable':
                            ax2.tick_params(axis='y', labelcolor=current_color[2])
                    # add a zero line
                    if double_line:
                        if two_ax:
                            if if_contains_zero(ax2.get_ylim()):
                                ax2.axhline(linewidth=0.5, color='k')
                            else:
                                ax.axhline(linewidth=0.5, color='k')
                            if if_contains_zero(ax.get_ylim()):
                                ax.axhline(linewidth=0.5, color='k')
                        else:
                            ax.axhline(linewidth=0.5, color='k')
                    else:
                        ax.axhline(linewidth=0.5, color='k')

                    if double_line:
                        if two_ax:
                            ax.set_zorder(10)
                            ax.patch.set_visible(False)

                    if tri_line:
                        if current_twox == 'disable-enable':
                            ax.set_zorder(10)
                            ax.patch.set_visible(False)

                    # set border color and width
                    for spine in ax.spines.values():
                        spine.set_edgecolor('grey')
                        spine.set_linewidth(0.5)

                    # add year tickers as minor tick
                    years = mdates.YearLocator()
                    yearsFmt = mdates.DateFormatter('%Y')
                    ax.xaxis.set_major_formatter(yearsFmt)
                    ax.xaxis.set_minor_locator(years)
                    # set the width of minor tick
                    ax.tick_params(which='minor', width=0.1)
                    # ax2.tick_params(axis='y', labelcolor='deepskyblue')
                    # set y-label to the right hand side
                    ax.yaxis.tick_right()
                    if double_line:
                        if two_ax:
                            ax2.yaxis.tick_left()
                    if tri_line:
                        if current_twox == 'disable-enable':
                           ax2.yaxis.tick_left()

                    # set date max
                    x_longer = x1
                    if double_line:
                        if two_ax:
                            x_longer = x1 if x1[-1] > x2[-1] else x2
                    if tri_line:
                        if current_twox == 'disable-enable':
                            x_longer = x1 if x1[-1] > x3[-1] else x3
                    datemax = np.datetime64(x_longer[-1], 'Y')
                    datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                    x_tick_overrive = [datemin, datemax]
                    date_cursor = datemin
                    while date_cursor + np.timedelta64(5, 'Y') < datemax:
                        date_cursor = date_cursor + np.timedelta64(5, 'Y')
                        x_tick_overrive.append(date_cursor)

                    ax.xaxis.set_ticks(x_tick_overrive)
                    ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))

                    # add rates rising falling vertical lines
                    if current_rate_cycle=='enable':
                        add_rates_rise_fall(ax,rise_dates_path=current_rate_rise_path)
                    elif current_rate_cycle=='type2':
                        add_rates_rise_fall(ax,'type2',rise_dates_path=current_rate_rise_path)
                else:
                    set_ax_invisible(axarr[i, j])

        plt.tight_layout()
        if page_name !='':
            fig.tight_layout()
            fig.subplots_adjust(top=0.88)
        report.savefig(fig, bbox_inches='tight')  # the current page is saved
    report.close()

    # make a copy to bt_backup_dir
    if bt_backup_dir:
        shutil.copy(pdfpath,bt_backup_dir)

def set_ax_invisible(ax):
    ax.axis('off')

def add_rates_rise_fall(ax,type = 'type1',rise_dates_path = os.path.join(PROJ_DIR,r"basket\USA_Rates1\usaratesfalling_rising_period.xlsx")):

    try:
        df = pd.read_excel(rise_dates_path, index_col=False, header=0, parse_dates=[0, 1])
    except:
        print (rise_dates_path)
    fall_dates = df['falling'].dropna().values
    rising_dates = df['rising'].dropna().values
    falling2_dates = df['falling2'].dropna().values
    if type == 'type1':
        for x1,x2 in zip(fall_dates,rising_dates):
            ax.axvspan(x1, x2, alpha=0.1, color='red')
        for x1,x2 in zip(rising_dates,falling2_dates):
            ax.axvspan(x1, x2, alpha=0.1, color='green')
    elif type == 'type2':
        for x1 in fall_dates:
            ax.axvline(x=x1, color='grey', linestyle='--',dashes=(5,5),linewidth=0.4)
        for x2 in rising_dates:
            ax.axvline(x=x2, color='grey', linestyle='--',dashes=(5,5),linewidth=0.4)

def add_vertical_line_on_date(ax,sample_end = datetime.today()+timedelta(days=2)):
    end_point = sample_end
    ax.axvline(x=end_point, color='green', linestyle='--',dashes=(5,10),linewidth=0.2)

def if_contains_zero(rng): return True if (rng[0]<0) and (rng[1]>0) else False

def firm_presentation_equity_curve(cumprof,dd,title,dir,sample_start,sample_end):
    fig,axarr = plt.subplots(1,1,figsize=(8, 6),dpi=100)
    ax = axarr
    cumprof,dd = cumprof.loc[sample_start:sample_end,:],dd.loc[sample_start:sample_end,:]
    x1,x2 = cumprof.dropna().index,dd.dropna().index
    y1,y2 = cumprof.dropna().iloc[:,0],dd.dropna().iloc[:,0]
    line1 = ax.plot(x1,y1,color='b',linewidth=2)
    charttitle = title
    ax.set_title("\n".join(wrap(charttitle)), fontsize=12)
    vals = ax.get_yticks()
    #ax.set_yticklabels(['{:3.1f}%'.format(x * 100) for x in vals])
    ax.grid(True, linestyle='dotted', color='k')
    ax.margins(x=0)

    ax1 = ax.twinx()
    line2 = ax1.plot(x2,y2,color='r', linewidth=2, alpha=0.6)
    lims = plt.ylim()
    ax1.set_ylim([lims[0], 0])
   vals1 = ax1.get_yticks()
    print (vals1)
    ax1.set_yticklabels(['{:3.1f}%'.format(x * 100) for x in vals1])
    ax1.margins(x=0)
    plt.savefig(dir)

def rates_tree_component_plot(plot_dict,chart_start_dt = '2010-01-01',chart_end_dt='2035-01-01',rate_rise_fall = '',pdfpath='',page_name = '',bt_backup_dir = None):
    '''
    :param plot_dict: dictionary which defines df pairs, titles,
    :param chart_start_dt:
    :return:
    '''
    chart_each_page = 12
    chart_rows = 4
    chart_cols = 3

    df_pair,title_list,chart_style = plot_dict['df_res'],plot_dict['title_res'],plot_dict['chart_type']
    rate_rise_fall = [rate_rise_fall] * len(df_pair)
    pages_number = math.ceil(len(plot_dict['title_res'])/chart_each_page)
    chart_in_page = [chart_each_page] * (pages_number - 1) + [
        len(plot_dict['title_res']) - chart_each_page * (pages_number - 1)]

    report = PdfPages(pdfpath)

    print('chart_in_each_page=', chart_in_page)
    print("CREATING RETURNS PAGE")

    # split into ech page!
    for i,n in enumerate(chart_in_page):
        chart_rows, chart_cols = chart_rows, chart_cols
        fig, axarr = plt.subplots(chart_rows, chart_cols, figsize=(18.27, 12.69), dpi=100)
        # add the main title
        if page_name != '':
            fig.suptitle(page_name, fontsize=16)
        start_idx = sum(chart_in_page[:i])
        end_idx = start_idx + n
        dfs_in_this_page,title_itp,chart_style_itp,rate_rise_fall_path_itp = df_pair[start_idx:end_idx],title_list[start_idx:end_idx],chart_style[start_idx:end_idx],rate_rise_fall[start_idx:end_idx]

        for i in range(chart_rows):
            for j in range(chart_cols):
                if i * chart_cols + j < len(title_itp):
                    ax = axarr[i, j]
                    current_dfs,current_title,current_style,current_rate_rise_path = dfs_in_this_page[i * chart_cols + j],title_itp[i * chart_cols + j],chart_style_itp[i * chart_cols + j],rate_rise_fall_path_itp[i * chart_cols + j]

                    if current_style in ['raw_vs_trend']:
                        df1,df2 = current_dfs[0].dropna(),current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1 , x2 = pd.to_datetime(df1.index).date,pd.to_datetime(df2.index).date
                        y1 , y2 = df1.iloc[:,0],df2.iloc[:,0]

                        line1 = ax.plot(x1,y1,color='blue',ls = 'solid',lw=0.9,label=df1.columns[0])
                        line2 = ax.plot(x2, y2, color='black', ls='dashed', lw=0.9,
                                        label=df2.columns[0])
                        lns = line1+line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min

                        l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min

                        ymax = (l1_max + l1_diff * .2) if l1_max > l2_max else (l2_max + l2_diff * .2)
                        ymin = (l1_min - l1_diff * .2) if l1_min < l2_min else (l2_min - l2_diff * .2)
                        ax.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='blue')

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()

                        # set date max
                        x_longer = x1

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                    if current_style in ['raw_only','z_score']:
                        df1= current_dfs.dropna()
                        #print (current_dfs)
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]

                        x1 = pd.to_datetime(df1.index).date
                        y1 = df1.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='blue', ls='solid', lw=0.9, label=df1.columns[0])
                        lns = line1
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=1, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min

                        ymax = (l1_max + l1_diff * .2)
                        ymin = (l1_min - l1_diff * .2)
                        ax.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='blue')

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()

                        # set date max
                        x_longer = x1

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['smooth_z_score']:
                        df1, df2 = current_dfs[0].dropna(), current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1, x2 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date
                        y1, y2 = df1.iloc[:, 0], df2.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='blue', ls='solid', lw=0.9, label=df1.columns[0])
                        line2 = ax.plot(x2, y2, color='red', ls='dashed', lw=0.9,
                                        label=df2.columns[0])
                        lns = line1 + line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min

                       l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min

                        ymax = (l1_max + l1_diff * .2) if l1_max > l2_max else (l2_max + l2_diff * .2)
                        ymin = (l1_min - l1_diff * .2) if l1_min < l2_min else (l2_min - l2_diff * .2)
                        ax.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='blue')

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()

                        # set date max
                        x_longer = x1

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['gauge_vs_yield']:
                        df1,df2 = current_dfs[0].dropna(),current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1, x2 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date
                        y1, y2 = df1.iloc[:, 0], df2.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='red', ls='solid', lw=0.9, label=df1.columns[0])
                        ax2 = ax.twinx()
                        line2 = ax2.plot(x2, y2, color='blue', ls='solid', lw=0.9, label=df2.columns[0])
                        lns = line1 + line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the axis limit
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min
                        ymax = l1_max + l1_diff * .2
                        ymin = l1_min - l1_diff * .2
                        ax.set_ylim(ymin, ymax)

                        l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min
                        ymax = l2_max + l2_diff * .2
                        ymin = l2_min - l2_diff * .2
                        ax2.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')
                        ax2.set_xlabel('')
                        ax2.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax2.tick_params(labelsize=12, width=0.1)

                        ax.tick_params(axis='y', labelcolor='red')
                        ax2.tick_params(axis='y', labelcolor='blue')

                        ax2.axhline(linewidth=0.5, color='k')
                        if if_contains_zero(ax.get_ylim()):
                            ax.axhline(linewidth=0.5, color='k')

                        ax.set_zorder(10)
                        ax.patch.set_visible(False)

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # ax2.tick_params(axis='y', labelcolor='deepskyblue')
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()
                        ax2.yaxis.tick_left()

                        # set date max
                        x_longer = x1 if x1[-1] > x2[-1] else x2

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['gauge_vs_parent']:
                        df1, df2 = current_dfs[0].dropna(), current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1, x2 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date
                        y1, y2 = df1.iloc[:, 0], df2.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='red', ls='solid', lw=0.9, label=df1.columns[0])
                        line2 = ax.plot(x2, y2, color='blue', ls='solid', lw=0.9,
                                        label=df2.columns[0])
                        lns = line1 + line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min

                        l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min

                        ymax = (l1_max + l1_diff * .2) if l1_max > l2_max else (l2_max + l2_diff * .2)
                        ymin = (l1_min - l1_diff * .2) if l1_min < l2_min else (l2_min - l2_diff * .2)
                        ax.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='blue')

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()

                        # set date max
                        x_longer = x1

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['raw_vs_trend_vs_yield']:
                        df1, df2, df3 = current_dfs[0].dropna(), current_dfs[1].dropna(),current_dfs[2].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]
                        mask = (df3.index >= chart_start_dt) & (df3.index <= chart_end_dt)
                        df3 = df3.loc[mask, :]

                        x1, x2, x3 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date,pd.to_datetime(df3.index).date
                        y1, y2, y3 = df1.iloc[:, 0], df2.iloc[:, 0],df3.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='blue', ls='solid', lw=0.9, label=df1.columns[0])
                        line2 = ax.plot(x2, y2, color='black', ls='dashed', lw=0.9,
                                        label=df2.columns[0])
                        ax2 = ax.twinx()
                        line3 = ax2.plot(x3, y3, color='lightgrey', ls='solid', lw=0.9,
                                         label=df3.columns[0])
                        lns = line1 + line2+line3
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min
                        ymax = l1_max + l1_diff * .2
                        ymin = l1_min - l1_diff * .2
                        ax.set_ylim(ymin, ymax)

                        l3_max = df3.loc[chart_start_dt:].max().values
                        l3_min = df3.loc[chart_start_dt:].min().values
                        l3_diff = l3_max - l3_min
                        ymax = l3_max + l3_diff * .2
                        ymin = l3_min - l3_diff * .2
                        ax2.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')
                        ax2.set_xlabel('')
                        ax2.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='blue')
                        ax2.tick_params(labelsize=12, width=0.1)
                        ax2.tick_params(axis='y', labelcolor='grey')
                        ax2.tick_params(labelsize=12, width=0.1)

                        ax.set_zorder(10)
                        ax.patch.set_visible(False)

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()

                        # set date max
                        x_longer = x1 if x1[-1] > x3[-1] else x3
                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['raw_only_vs_yield']:
                        df1, df2 = current_dfs[0].dropna(), current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1, x2 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date
                        y1, y2 = df1.iloc[:, 0], df2.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='blue', ls='solid', lw=0.9, label=df1.columns[0])
                        ax2 = ax.twinx()
                        line2 = ax2.plot(x2, y2, color='lightgrey', ls='solid', lw=0.9, label=df2.columns[0])
                        lns = line1 + line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the axis limit
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min
                        ymax = l1_max + l1_diff * .2
                        ymin = l1_min - l1_diff * .2
                        ax.set_ylim(ymin, ymax)

                        l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min
                        ymax = l2_max + l2_diff * .2
                        ymin = l2_min - l2_diff * .2
                        ax2.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')
                        ax2.set_xlabel('')
                        ax2.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax2.tick_params(labelsize=12, width=0.1)

                        ax.tick_params(axis='y', labelcolor='red')
                        ax2.tick_params(axis='y', labelcolor='blue')

                        ax2.axhline(linewidth=0.5, color='k')
                        if if_contains_zero(ax.get_ylim()):
                            ax.axhline(linewidth=0.5, color='k')

                        ax.set_zorder(10)
                        ax.patch.set_visible(False)

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # ax2.tick_params(axis='y', labelcolor='deepskyblue')
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()
                        ax2.yaxis.tick_left()

                        # set date max
                        x_longer = x1 if x1[-1] > x2[-1] else x2

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                        date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax, rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)
                    if current_style in ['signal_flip_vs_yield']:
                        df1, df2 = current_dfs[0].dropna(), current_dfs[1].dropna()
                        mask = (df1.index >= chart_start_dt) & (df1.index <= chart_end_dt)
                        df1 = df1.loc[mask, :]
                        mask = (df2.index >= chart_start_dt) & (df2.index <= chart_end_dt)
                        df2 = df2.loc[mask, :]

                        x1, x2 = pd.to_datetime(df1.index).date, pd.to_datetime(df2.index).date
                        y1, y2 = df1.iloc[:, 0], df2.iloc[:, 0]

                        line1 = ax.plot(x1, y1, color='deepskyblue', ls='solid', lw=0.4,
                                        label=df1.columns[0])
                        line_shade1 = ax.fill_between(x1, 0, y1, facecolors='aqua', alpha=0.3, label='_nolabel_')
                        ax2 = ax.twinx()
                        line2 = ax2.plot(x2, y2, color='black', ls='solid', lw=0.4,
                                         label=df2.columns[0])

                        lns = line1 + line2
                        labs = [l.get_label() for l in lns]
                        ax.legend(lns, labs, loc=9, frameon=False, ncol=2, fontsize=8)

                        # set the max and min
                        l1_max = df1.loc[chart_start_dt:].max().values
                        l1_min = df1.loc[chart_start_dt:].min().values
                        l1_diff = l1_max - l1_min
                        ymax = l1_max + l1_diff * .2
                        ymin = l1_min - l1_diff * .2
                        ax.set_ylim(ymin, ymax)

                        l2_max = df2.loc[chart_start_dt:].max().values
                        l2_min = df2.loc[chart_start_dt:].min().values
                        l2_diff = l2_max - l2_min
                        ymax = l2_max + l2_diff * .2
                        ymin = l2_min - l2_diff * .2
                        ax2.set_ylim(ymin, ymax)

                        ax.set_xlabel('')
                        ax.set_ylabel('')
                        ax2.set_xlabel('')
                        ax2.set_ylabel('')

                        # wrap up the title since it can be too long
                        title = "\n".join(wrap(current_title, 60))
                        ax.set_title(title, y=1, fontsize=11, fontweight=600)

                        ax.tick_params(labelsize=12, width=0.1)
                        ax.tick_params(axis='y', labelcolor='deepskyblue')
                        ax2.tick_params(labelsize=12, width=0.1)
                        ax2.tick_params(axis='y', labelcolor='black')

                        # add zero line
                        ax.axhline(linewidth=0.5, color='k')

                        if if_contains_zero(ax2.get_ylim()):
                            ax2.axhline(linewidth=0.5, color='k')
                        else:
                            ax.axhline(linewidth=0.5, color='k')
                        if if_contains_zero(ax.get_ylim()):
                            ax.axhline(linewidth=0.5, color='k')

                        # set border color and width
                        for spine in ax.spines.values():
                            spine.set_edgecolor('grey')
                            spine.set_linewidth(0.5)

                        ax.set_zorder(10)
                        ax.patch.set_visible(False)

                        # add year tickers as minor tick
                        years = mdates.YearLocator()
                        yearsFmt = mdates.DateFormatter('%Y')
                        ax.xaxis.set_major_formatter(yearsFmt)
                        ax.xaxis.set_minor_locator(years)
                        # set the width of minor tick
                        ax.tick_params(which='minor', width=0.1)
                        # set y-label to the right hand side
                        ax.yaxis.tick_right()
                        ax2.yaxis.tick_left()

                        # set date max
                        x_longer = x1 if x1[-1] > x2[-1] else x2

                        datemax = np.datetime64(x_longer[-1], 'Y')
                        datemin = np.datetime64(x1[0], 'Y') - np.timedelta64(1, 'Y')
                        x_tick_overrive = [datemin, datemax]
                       date_cursor = datemin
                        while date_cursor + np.timedelta64(5, 'Y') < datemax:
                            date_cursor = date_cursor + np.timedelta64(5, 'Y')
                            x_tick_overrive.append(date_cursor)

                        ax.xaxis.set_ticks(x_tick_overrive)
                        ax.set_xlim(datemin, datemax + np.timedelta64(1, 'Y'))
                        add_rates_rise_fall(ax,'type2',rise_dates_path=current_rate_rise_path)
                        add_vertical_line_on_date(ax)

                else:
                    set_ax_invisible(axarr[i, j])
        plt.tight_layout()
        if page_name != '':
            fig.tight_layout()
            fig.subplots_adjust(top=0.88)
        report.savefig(fig, bbox_inches='tight')  # the current page is saved
    report.close()

    # make a copy to bt_backup_dir
    if bt_backup_dir:
        try:
            shutil.copy(pdfpath,bt_backup_dir)
        except:
            pass

 






post_sig_genr.py





import pandas as pd
import csv
import numpy as np
import os
import pickle
from panormus.quant.portfolio import annual_return,annual_vol,calmar_ratio
from qs_common.trading.tradeutils import get_min_trade_within_tolerance

from Analytics.scoring_methods import scoring_method_collection as smc
from Analytics.series_utils import date_utils_collection as s_util
from qs_common.trading import tradeutils as trade_util
import warnings
#from qm_common.reporting.rp_pairwise_correl import PwCorrReport

SM = smc()
SU = s_util()

def dump_wf_obj_to_csv(work_file,dir):
    l_df = []
    existing_col = []
    for k,v in work_file.df.items():
        col = v.columns
        new_col = []
        for i in col:
            if i in existing_col:
                i = i+'_('+k+')'
                new_col.append(i)
            else:
                new_col.append(i)
            existing_col.append(i)
        v.columns = new_col
        l_df.append(v)
    pd.concat(l_df,axis=1).to_csv(dir)

def dump_perf_to_output(work_file,dir):
    create_folder(dir)
    cumprof_csv_dir = os.path.join(dir,'cumprof.csv')
    pd.concat([work_file.df['cumprof'].dropna(),work_file.df['ret'].dropna()],axis=1).to_csv(cumprof_csv_dir)

def create_folder(path):
    "Creates all folders necessary to create the given path."
    try:
        if not os.path.exists(path):
            os.makedirs(path)
    except IOError as io:
        print("Cannot create dir %s" % path)
        raise io

def write_pars_to_csv(param,param_csv_dir):
    with open(param_csv_dir, 'w') as f:  # Just use 'w' mode in 3.x
        w = csv.DictWriter(f, param.keys())
        w.writeheader()
        w.writerow(param)

def LS_filtered(df_indicator, df_sci, method='s_curve',country_order=[],tc_reduction_method='none',inertia_torlerance=[0]):
    '''
    translate the indicator group (filtered by sci) into a signal group.
    Note that the indicator group and sci group must have the same number of columns.
    If method is s_curve, then the position size is the score map to the sigmoid function

    param: exec_torlerance: the tolerance parameter in the t-cost reducing algorithms. The larger the torlerance, the smoothier the signals
    '''
    assert len(df_indicator.columns) == len(df_sci.columns), "Sorry, indicator group has different column with sci"
    assert len(df_indicator.index) == len(df_sci.index), "Sorry, indicator group has different index with sci"
    if len(df_indicator.columns)>1.01 :
        assert len(country_order)==len(df_indicator.columns), 'Sorry, please provide the correct order of countries in the indicator group'

    df_indicator_ = df_indicator.copy()
    df_sci_ = df_sci.copy()

    # step1: filtering out the period where the sci index doesn't exist.
    # last index should shift 1 day forward. e.g. allow signal exist for 1 day after the last sci day.

    first_idx_list = [df_sci_[[s]].first_valid_index() for s in df_sci_.columns]
    last_idx_list = [df_sci_[[s]].shift(1).last_valid_index() for s in df_sci_.columns]

    counter = 0
    for f, l in zip(first_idx_list, last_idx_list):
        col = df_indicator_.columns.tolist()[counter]
        mask = (df_indicator_.index > l) | (df_indicator_.index < f)
        df_indicator_.loc[mask, col] = np.nan

    signal_group = df_indicator_.copy()
    signal_group.loc[:, :] = np.nan

    if method == 's_curve':
        df_indicator_ = SM.sigmoid_array(df_indicator_)
        signal_group = df_indicator_ * 1000*2
    elif method=='binary':
        signal_group = ((df_indicator_>0)*2-1)*1000
    elif method=='origin_z':
        signal_group = df_indicator_

    signal_group.columns = [i for i in df_indicator_.columns]
    signal_group_flip = -signal_group
    signal_group_flip.columns = ['condition-pricing'] if len(signal_group_flip.columns)<1.01 else ['condition-pricing ('+iso+')' for iso in country_order]

    actual_trade_group = tc_reduction(signal_group,tc_reduction_method=tc_reduction_method,inertia_tolerance=inertia_torlerance)

    return {
            'signal_group':signal_group,
            'signal_group_flip': signal_group_flip,
            'actual_trade_group':actual_trade_group,
           }

def HmL_filtered(df_indicator,df_sci,type='number',cross=1,method='equal',capital=1000):
    # HmL basket which returns the high ranking to be positive and low ranking to be negative
    # all convert into matrix
    _mat = df_indicator.as_matrix().astype(np.float)
    _sci_mat = df_sci.as_matrix().astype(np.float)
    _precondition = (~np.isnan(_sci_mat))
    print ('the shape of mat are',_mat.shape, _sci_mat.shape)

    # check the shape
    if _mat.shape != _precondition.shape:
        print ("The matrix and mask matrix have different size! Please check!!!")

    else:
        output = _mat.copy()
        output[:] = np.nan

        if type == 'number':
            minimum_avail = cross * 2
           for i in range(_mat.shape[0]):
                this_mask = _precondition[i]
                if np.sum(this_mask) < minimum_avail:
                    pass
                else:
                    this_row = _mat[i]
                    # get the decending order of the masked matrix, select the first n of the index.
                    selec_index = np.argsort(-this_row[this_mask])[:cross]  # descending order ranking index, select the top quantile
                    # initialise an empty signal row
                    this_row_sig = this_row[this_mask].copy()
                    this_row_sig[:] = np.nan
                    if method == 'equal':
                        this_row_sig[selec_index] = capital/cross

                    # now the bottom part
                    selec_index = np.argsort(this_row[this_mask])[:cross]
                    if method == 'equal':
                        this_row_sig[selec_index] = -capital / cross
                    # fill in the output
                    output[i][this_mask] = this_row_sig
                    output[i][np.isnan(output[i])] = 0
        print ('output shape is ', output.shape)
        signal_group = pd.DataFrame(data=output,index=df_indicator.index,columns=['signal_'+i for i in df_indicator.columns])
        for i in range(len(signal_group.columns)):
            first_idx = signal_group.iloc[:,[i]].first_valid_index()
            last_idx = signal_group.iloc[:,[i]].last_valid_index()
            signal_group.loc[first_idx:last_idx,:].iloc[:,i].fillna(0,inplace=True)
        signal_group_flip = -signal_group
        signal_group_flip.columns = ['(flipped)' + i for i in signal_group.columns]
        return signal_group , -signal_group

def profit(signal_panel,sci_panel,riskcapital,profsmpl):
    '''
    the function uses yesterday's signal and return from yesterday to today to calculate the return
    important: note that the date in return series refer to the day later than the signal
    '''
    assert len(signal_panel.columns) == len(
        sci_panel.columns), "Sorry, the number of signal col is differnt from number of sci col"
    assert len(signal_panel.index) == len(sci_panel.index), "Sorry, the number of index is different"

    ret_panel = sci_panel.pct_change(periods=1)
    ret_panel.columns = range(len(ret_panel.columns))

    sig_panel_1 = signal_panel.shift(1)
    sig_panel_1.columns = range(len(sig_panel_1.columns))

    sig_panel_1 = sig_panel_1 * ret_panel
    last_dt = sig_panel_1.last_valid_index()
    sig_panel_1['pnl'] = sig_panel_1.sum(axis=1)
    mask = (sig_panel_1.index>last_dt)
    sig_panel_1.loc[mask,:]=np.nan
    pnl = sig_panel_1[['pnl']]  # dollar pnl

    profsmpl = [pd.to_datetime(i) for i in profsmpl]
    mask = (pnl.index <= profsmpl[0]) | (pnl.index > profsmpl[1])
    pnl.loc[mask, :] = np.nan

    pnl['reinvestprof'] = (pnl['pnl']/riskcapital+1).cumprod()*riskcapital
    pnl.loc[pnl.index==profsmpl[0],'reinvestprof']=riskcapital
    pnl['cumprof'] = pnl['pnl'].cumsum()
    pnl['ret'] = pnl['pnl']/riskcapital

    return pnl

def tc_reduction(raw_signal_group,tc_reduction_method = 'none', inertia_tolerance = [0]):
    if tc_reduction_method == 'none':
        return raw_signal_group
    elif tc_reduction_method == 'inertia':
        assert len(raw_signal_group.columns)==len(inertia_tolerance), 'Please make sure the number of signal is the same as the number of transaction cost reduction torlerance provided'
        df_list = []
        for tor,sig in zip(inertia_tolerance,raw_signal_group.columns):
            actual_trade = raw_signal_group.loc[:,[sig]].dropna().copy()
            actual_trade['current'] = np.nan
            actual_trade.loc[:, 'current'].iloc[0] = actual_trade.iloc[0, 0]
            actual_trade_matrix = actual_trade.as_matrix()
            for i in range(actual_trade_matrix.shape[0] - 1):
                amount = trade_util.get_min_trade_within_tolerance(actual_trade_matrix[i + 1][0], actual_trade_matrix[i][1], tor)
                actual_trade_matrix[i + 1][1] = actual_trade_matrix[i][1] + amount
            actual_trade.loc[:, :] = actual_trade_matrix
            df = actual_trade.iloc[:,[1]]
            df.columns = [actual_trade.columns[0]+' (actual_trading_position)']
            df_list.append(df)
        actual_trade_group = pd.concat(df_list,axis=1)
        return actual_trade_group
    elif tc_reduction_method == 'dynamic_inertia':
        pass

def apply_risk_management_rule(df_underlying_signal_group,df_underlying_ret_group, rule_type=None,param_dict=None):
    '''
    :param df_underlying_signal_group: the underlying signal gorup
    :param df_underlying_ret_group: the underlying ret generated by the underlying signal group
    :param rule_type:
                    a- if_works_top_up:
                        if making money from close_T-1 to close_T, topping up, the logic is that:
                        we know that the model is calibrated to capture the big move in the market, and we know that the model
                        is faded away too quickly. keep the model running when model is making money

                    param_dict : rolling_win,z_thres

                    b- if_works_cash_in:

    :return: the actual trading group
    '''
    df_underlying_signal_group,df_underlying_ret_group = df_underlying_signal_group.copy(),df_underlying_ret_group.copy()
    if rule_type in ['None',None]:
        return df_underlying_signal_group
    elif rule_type in ['if_higher_top_up']:
        # remove the zeros at the beginning
        df_underlying_signal_group_list = []
        for col in df_underlying_signal_group.columns:
            df_underlying_signal_group_list.append(SU.delete_zero_beginning(df_underlying_signal_group.loc[:,[col]]))
        if len(df_underlying_signal_group_list) > 1.01:
            df_underlying_signal_group = pd.concat(df_underlying_signal_group_list,axis=1)
        df_underlying_ret_group_list = []
        for col in df_underlying_ret_group.columns:
            df_underlying_ret_group_list.append(SU.delete_zero_beginning(df_underlying_ret_group.loc[:, [col]]))
        if len(df_underlying_ret_group_list) > 1.01:
            df_underlying_ret_group = pd.concat(df_underlying_signal_group_list, axis=1)

        assert df_underlying_ret_group.shape[1] == df_underlying_signal_group.shape[1], ' sorry, the number of column number of underlying signal and underlying ret series are not the same'
        signal_group_res_list = []
        for signal_col,ret_col in zip(df_underlying_signal_group.columns,df_underlying_ret_group.columns):
            this_signal,this_ret = df_underlying_signal_group.loc[:,[signal_col]].dropna(),df_underlying_ret_group.loc[:,[ret_col]].dropna()
            # calc the z_score of return series, to check if the underlying model works
            this_ret['rolling_ret'] = this_ret.iloc[:,0].rolling(window=param_dict['rolling_win'],min_periods=param_dict['rolling_win']).mean()
            new_name = 'rolling_z'
            this_ret = SM.z(this_ret,mean_type='custom',custom_mean=0,sd_type='rolling',roll_sd=252*2,new_name = new_name)
            this_ret['higher'] = 0
            mask = this_ret['rolling_z']>=param_dict['z_thres']
            this_ret.loc[mask,'higher'] = 1
            df_comb = pd.merge(this_signal,this_ret.loc[:,['higher']],left_index=True,right_index=True,how='left')
            df_comb.fillna(0,inplace=True)
            # find the max position in the past 21 days that have the same sign as signal_today
            df_comb['rolling_max'] = df_comb.iloc[:,0].rolling(window=42).max()
            df_comb['rolling_min'] = df_comb.iloc[:,0].rolling(window=42).min()
            df_comb['final'] = np.nan
            mask = (df_comb['higher']==1) & (df_comb.iloc[:,0]>0)
            df_comb.loc[mask,'final']= df_comb['rolling_max']
            mask = (df_comb['higher'] == 1) & (df_comb.iloc[:, 0] < 0)
            df_comb.loc[mask, 'final'] = df_comb['rolling_min']
            df_comb['final'].fillna(df_comb.iloc[:,0],inplace=True)
            res = df_comb.loc[:,['final']]
            res.columns = this_signal.columns
            signal_group_res_list.append(res)
        new_signal_group = pd.concat(signal_group_res_list,axis=1)
        return new_signal_group

def returnstats_dict(df_equitycurve,riskcapital,profsmpl,benchmark=None):
    '''
    return ann_mean, ann_std, ann_sharpe, drawdown_series, and rolling correlation
    '''
    mask = (df_equitycurve.index < profsmpl[0]) | (df_equitycurve.index > profsmpl[1])
    df1 = df_equitycurve.iloc[:, [0]] # get the dollar pnl
    df1.loc[mask, :] = np.nan

    df1 = df1.dropna()/riskcapital

    ann_mean = annual_return(df1).values[0]*100
    ann_std = annual_vol(df1).values[0]*100
    ann_sharpe = ann_mean / ann_std
    calmar = calmar_ratio(df1).values[0]
    #print (ann_mean,ann_std)

    # get the cumprof
    df2 = df_equitycurve.loc[:, ['cumprof']]
    df2.loc[mask, :] = np.nan
    df1['drawdown'] = (df2 - df2.cummax()) / riskcapital

    if benchmark is None:
        return {
                'ann_mean':ann_mean,
                'ann_std':ann_std,
                'ann_sharpe':ann_sharpe,
                'drawdown':df1[['drawdown']],
                'calmar':calmar
                }

    else:
        # calculate the rolling correlation (5 business days return, rolling 1 year)
        df_comb = pd.merge( df_equitycurve[['reinvestprof']],benchmark,left_index=True,right_index=True,how='outer')
        df_comb.dropna(inplace=True)
        # get 5 business days interval
        mask = (df_comb.index.dayofweek == 3)
        df_comb = df_comb.loc[mask,:]
        df_comb = df_comb.pct_change(1)
        df_comb['1y_corr_benchmark'] = df_comb.iloc[:,0].rolling(52).corr(df_comb.iloc[:,1])
        mask = (df_comb.index < profsmpl[0]) | (df_comb.index > profsmpl[1])
        df_comb.loc[mask, :] = np.nan
        # correlation with benchmark
       df_comb['avg_corr'] = df_comb.iloc[:,[0,1]].dropna().corr().values[0][1]
        return {
                'ann_mean':ann_mean,
                'ann_std':ann_std,
                'ann_sharpe':ann_sharpe,
                'drawdown':df1[['drawdown']],
                '1y_corr':df_comb[['1y_corr_benchmark']],
                'avg_corr':df_comb[['avg_corr']],
                'calmar':calmar
        }

def returnstats(df_equitycurve,riskcapital,profsmpl,benchmark=None):
    warnings.warn(
        "This function will be going away soon. Use the function returnstats_dict from this module instead.",
        PendingDeprecationWarning)
    '''
    return ann_mean, ann_std, ann_sharpe, drawdown_series, and rolling correlation
    '''
    mask = (df_equitycurve.index < profsmpl[0]) | (df_equitycurve.index > profsmpl[1])
    df1 = df_equitycurve.iloc[:, [0]] # get the dollar pnl
    df1.loc[mask, :] = np.nan

    df1 = df1.dropna()/riskcapital

    ann_mean = annual_return(df1).values[0]*100
    ann_std = annual_vol(df1).values[0]*100
    ann_sharpe = ann_mean / ann_std
    #print (ann_mean,ann_std)

    # get the cumprof
    df2 = df_equitycurve.loc[:, ['cumprof']]
    df2.loc[mask, :] = np.nan
    df1['drawdown'] = (df2 - df2.cummax()) / riskcapital

    if benchmark is None:
        return ann_mean, ann_std, ann_sharpe, df1[['drawdown']],

    else:
        # calculate the rolling correlation (5 business days return, rolling 1 year)
        df_comb = pd.merge( df_equitycurve[['reinvestprof']],benchmark,left_index=True,right_index=True,how='outer')
        df_comb.dropna(inplace=True)
        # get 5 business days interval
        mask = (df_comb.index.dayofweek == 3)
        df_comb = df_comb.loc[mask,:]
        df_comb = df_comb.pct_change(1)
        df_comb['1y_corr_benchmark'] = df_comb.iloc[:,0].rolling(52).corr(df_comb.iloc[:,1])
        mask = (df_comb.index < profsmpl[0]) | (df_comb.index > profsmpl[1])
        df_comb.loc[mask, :] = np.nan
        # correlation with benchmark
        df_comb['avg_corr'] = df_comb.iloc[:,[0,1]].dropna().corr().values[0][1]
        return ann_mean, ann_std, ann_sharpe, df1[['drawdown']],df_comb[['1y_corr_benchmark']],df_comb[['avg_corr']],

if __name__ == '__main__':
    pass

def pickle_result(temp_pickle,strategy_work_file):
    # pickle the result to a temp folder
    with open(temp_pickle, 'wb') as handle:
        pickle.dump(strategy_work_file, handle, protocol=pickle.HIGHEST_PROTOCOL)

 








basket_USA_rates.py





# refactoring: refactoring the strategy to a tree structure
# import sys when runing from the batch code
import sys
import os
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)),"../.."))
import pandas as pd
import numpy as np
from datetime import datetime,timedelta

import Analytics.loess_filter as lf
import Analytics.series_utils as s_util
from Analytics.scoring_methods import scoring_method_collection as smc
import Analytics.finance as fin_lib
import backtesting_utils.chart as TCT
import backtesting_utils.post_signal_genr as post_signal_genr

import Analytics.wfcreate as wf
from Analytics.abstract_sig import abstract_sig_genr_for_rates_tree as abs_sig

from Analytics.wfcreate import rates_model_tree

class signal3(abs_sig):
    def __init__(self):
        super(signal3,self).__init__()

    def add_strat_info(self):
        # this is the part to override
        self.basket_ID = 'basket_0017'
        self.Short_Name = 'basket_USA_Rates1c'
        self.Description = '''
              Variation of USA rates model :  using in-house forward growth estimates
              Backtesting infrastructure which enables to prototype the USA strategy quickly
              The inputs are: level, change, forward of the economic fundamentals.
              
              Level: economic slack; growth vs trend; unemployment vs trend; wage vs trend; capacity utilization vs trend; inflation vs target
              Change: change in wage growth; change in growth rate
              Forward growth: FCI impulse; Case Shiller housing price second order diff
              Forward CPI: Brent oil impulse; Effective FX impulse.  
              Global growth: global ex-USA CAI and FCI
              '''

    def initialise_and_run(self,run_charting=True):
        # for risk1 in [1,2,3,4,5,8,10,13,16,20,25,30,35]:
        #     for risk2 in [i/2 for i in range(6)]:
        self.add_strat_info()
        self.add_dir_info()
        self.new_wf = wf.initialise_wf(self.TEMP_LOCAL_PICKLE)
        # in this case, both Econ and csv file data are used
        self.raw_data_new_fmt = self.download_data_3(self.MASTER_INPUT_DIR, self.Short_Name)
        self.import_parse_param(self.MASTER_INPUT_DIR,self.Short_Name)
        # self.trans_param_df['risk_rule_1'] = [int(risk1)]
        # self.trans_param_df['risk_rule_2'] = [float(risk2)]
        self.create_folder(self.INDICATOR_EXP_DIR)
        self.out_folder = self.create_tearsheet_folder(self.RPTDIR)
        self.new_wf.create_folder(self.BT_BACKUP_ROOT_DIR)
        self.rates_rise_fall_path = os.path.join(self.PROJ_DIR, r"basket\USA_Rates1\usaratesfalling_rising_period.xlsx")
        self.tree = rates_model_tree()
        self.customed_titles = {}
        self.SM = smc()
        self.SU = s_util.date_utils_collection()
        self.run_step1()
        self.run_step2()

        if run_charting:
            self.run_step3()

    def run_step1(self):
        self.run_from_scratch = True
        self.run_level_Growth_Slack()
        self.run_level_Growth_vs_Pot()
        self.run_level_URate_trend()
        self.run_level_CapU_trend()
        self.run_level_wages_trend()
        self.run_level_inf_targ()
        self.run_level_inf_targ_GS_tracker()
        self.run_level_bei5_targ()
        self.run_level_wages_atlant_trend()
        self.run_change_in_growth()
        self.run_change_in_growth_citi_econ()
        self.run_chg_on_wage_growth()
        self.run_chg_in_cpi()
        self.run_chg_in_cpi_pce()
        self.run_surprise_in_cpi()
        self.run_chg_in_GS_cpi_tracker()
        self.run_chg_in_cpi_bei5()
        self.run_HP_impulse()
        self.run_total_cai_impulse()
        self.run_oil_impulse()
        self.run_fx_impulse()
        self.run_pricing()
        self.run_pricing_2y()
        self.run_glob_level_growth()
        self.run_glob_FCI()
        self.run_glob_chg_growth()

        self.pickle_result(self.TEMP_LOCAL_PICKLE, self.new_wf)

    def run_step2(self):
        COMPONENT_NODE_DICT = self.tree.read_node_info_from_excel(import_dir=self.MASTER_INPUT_DIR,sheet_name=self.Short_Name)
        self.tree.run_build_tree(COMPONENT_NODE_DICT,self.new_wf.df)
        root = self.tree.nodes['condition_m_pricing']
        self.tree.print_structure(root,export_to_csv_dir=self.EXPORT_TREE_STRUCT_DIR)
        self.tree.sum_up_Z_on_each_branch(root,self.new_wf)

        chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_component_chart(root)
        # component chart 2000
        TCT.rates_tree_component_plot(plot_dict=chart_pack_dict,chart_start_dt='2000-01-01',chart_end_dt='2030-01-01',rate_rise_fall=self.rates_rise_fall_path,bt_backup_dir=self.RPT_COMPONENT_DIR,pdfpath=self.BT_BACKUP_DIR1)

        #calculate the pnl
        root = self.tree.nodes['condition_m_pricing']
        self.tree.get_rid_of_candidate_node(root)
        self.tree.print_structure(root)
        self.run_indicator_and_post_sig_genr()

    def run_step3(self):
        # plot pnl since 2000
        root = self.tree.nodes['condition_m_pricing']
        self.series_name_dict = {'yield_series':'IRS_5Y',
                                 'signal_flip':'signal_group_flip',
                                 'indicator_group':'indicator_group',
                                 'cumprof':'cumprof',
                                 'TRI':'USA_5y_TRI',
                                 'drawdown':'drawdown',
                                 'corr':'rolling_corr_1y_bm_2010'
                                 }
        self.alpha_name_dict = {'ann_mean':'ann_mean',
                                'ann_std':'ann_std',
                                'ann_sharpe':'ann_sharpe',
                                'calmar':'calmar',
                                'avg_corr':'avg_corr'
                                 }
        pnl_chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_pnl_chart(root,self.new_wf.df,self.series_name_dict,self.new_wf.alpha,self.alpha_name_dict)
        TCT.rates_tree_component_plot(plot_dict=pnl_chart_pack_dict, chart_start_dt='2000-01-01', chart_end_dt='2030-01-01',rate_rise_fall=self.rates_rise_fall_path, bt_backup_dir=self.RPT_PNL2000_DIR ,pdfpath=self.BT_BACKUP_DIR2)

        # plot pnl since 2010
        self.series_name_dict.update({'drawdown': 'drawdown_2010'})
        self.alpha_name_dict = {'ann_mean': 'ann_mean_2010',
                                'ann_std': 'ann_std_2010',
                                'ann_sharpe': 'ann_sharpe_2010',
                                'calmar': 'calmar_2010',
                                'avg_corr': 'avg_corr_2010'
                                }
        pnl_chart_pack_dict = self.tree.expand_tree_below_a_node_and_return_pnl_chart(root, self.new_wf.df,
                                                                                      self.series_name_dict,
                                                                                      self.new_wf.alpha,
                                                                                      self.alpha_name_dict)
        TCT.rates_tree_component_plot(plot_dict=pnl_chart_pack_dict, chart_start_dt='2010-01-01',
                                      chart_end_dt='2030-01-01', rate_rise_fall=self.rates_rise_fall_path,
                                      bt_backup_dir=self.RPT_PNL2010_DIR, pdfpath=self.BT_BACKUP_DIR2010_2)

        # dump to csv
        post_signal_genr.write_pars_to_csv(self.trans_param_df, self.PARS_DIR)
        post_signal_genr.write_pars_to_csv(self.new_wf.alpha, self.ALPHA_DIR)
        #post_signal_genr.write_pars_to_csv(self.new_wf.alpha, os.path.join(self.SCRATCH_DIR,'BT_BACKUP','ALPHA_2_'+'_'+str(self.trans_param_df['risk_rule_1'][0])+'_'+str(self.trans_param_df['risk_rule_2'][0])+'.csv'))
        post_signal_genr.dump_wf_obj_to_csv(self.new_wf, self.DATA_DIR)

    def run_indicator_and_post_sig_genr(self):
        # check if condition minus pricing exists:
       assert 'condition_m_pricing' in self.new_wf.df.keys(), 'sorry, condition minus pricing z-score is not in the new_wf!!!'
        z_diff = self.new_wf.df['condition_m_pricing'].dropna()
        new_name = 'condition - pricing'
        z_diff.columns = [new_name]
        z_diff = z_diff.dropna().rolling(window=self.trans_param_df['con_smooth_window'][0]).apply(np.mean)

        self.new_wf.update_df(new_name, z_diff[[new_name]])
        self.new_wf.add_df('indicator_group', -z_diff[[new_name]].shift(self.trans_param_df['indicator_lag'][0]), repeat=True)
        self.post_indicator_genr()

    def post_indicator_genr(self):
        self.sci = self.sci_panel()
        signal_dict = post_signal_genr.LS_filtered(self.new_wf.df['indicator_group'], self.sci, method='s_curve',
                                                   tc_reduction_method='inertia', inertia_torlerance=[50])
        self.new_wf.add_df('signal_group', signal_dict['signal_group'])
        self.new_wf.add_df('signal_group_flip', signal_dict['signal_group_flip'])
        # calc the underlying pnl and apply the risk management rule
        self.profit_sample = ['1990-01-01', datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')]
        self.underlying_pnl = post_signal_genr.profit(self.new_wf.df['signal_group'], self.sci, 1000, self.profit_sample)
        # apply the risk management rule to the underlying signal
        signal_group_post_risk_rule = post_signal_genr.apply_risk_management_rule(signal_dict['signal_group'],self.underlying_pnl.loc[:,['ret']],rule_type='if_higher_top_up',param_dict={'rolling_win':self.trans_param_df['risk_rule_1'][0],'z_thres':self.trans_param_df['risk_rule_2'][0]})
        #pd.concat([signal_dict['signal_group'],signal_group_post_risk_rule],axis=1).to_csv('new_signal.csv')

        #benchmark
        #self.new_wf.add_df('actual_trade_group', signal_dict['signal_group'])
        # new signal
        self.new_wf.add_df('actual_trade_group', signal_group_post_risk_rule)

        self.profit_sample = ['2000-01-01', datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')]
        # profit_sample = ['2000-01-01', '2016-12-31']
        self.pnl = post_signal_genr.profit(self.new_wf.df['actual_trade_group'], self.sci, 1000, self.profit_sample)
        self.new_wf.add_df('equity_curve', self.pnl)
        self.new_wf.add_df('cumprof', self.pnl[['cumprof']])

        retstats_dict = post_signal_genr.returnstats_dict(self.new_wf.df['equity_curve'], 1000, self.profit_sample, benchmark=self.new_wf.df['USA_5y_TRI'])
        self.new_wf.alpha['ann_mean'] = retstats_dict['ann_mean']
        self.new_wf.alpha['ann_std'] = retstats_dict['ann_std']
        self.new_wf.alpha['ann_sharpe'] = retstats_dict['ann_sharpe']
        self.new_wf.alpha['calmar'] = retstats_dict['calmar']
        self.new_wf.add_df('drawdown', retstats_dict['drawdown'])
        self.new_wf.add_df('rolling_corr_1y_bm', retstats_dict['1y_corr'])
        self.new_wf.add_df('avg_corr_bm', retstats_dict['avg_corr'])

        self.profit_sample_2010 = ['2010-01-01', datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')]
        # profit_sample_2010 = ['2010-01-01', '2016-12-31']
        retstats_dict = post_signal_genr.returnstats_dict(self.new_wf.df['equity_curve'], 1000,self.profit_sample_2010, benchmark=self.new_wf.df['USA_5y_TRI'])

        self.new_wf.alpha['ann_mean_2010'] = retstats_dict['ann_mean']
        self.new_wf.alpha['ann_std_2010'] = retstats_dict['ann_std']
        self.new_wf.alpha['ann_sharpe_2010'] = retstats_dict['ann_sharpe']
        self.new_wf.alpha['calmar_2010'] = retstats_dict['calmar']
        self.new_wf.add_df('drawdown_2010', retstats_dict['drawdown'])
        self.new_wf.add_df('rolling_corr_1y_bm_2010', retstats_dict['1y_corr'])
        self.new_wf.add_df('avg_corr_bm_2010', retstats_dict['avg_corr'])
        return

    def sci_panel(self):
        importdir = os.path.join(self.PROJ_DIR, 'SCI', 'Market_data', 'USA_5y_TRI.csv')
        self.new_wf.importts(importdir, filetype='csv')
        return self.new_wf.df['USA_5y_TRI']

    def import_parse_param(self,master_input_dir,short_name):
        '''
        :param master_input_dir:
        :param short_name:
        :return:
        '''
        param_df = pd.read_excel(master_input_dir, sheet_name=short_name, index_col=False, header=0,
                                 na_values=['NA', ''])
        self.all_ISO = param_df.iloc[:,0].values.tolist()
        self.trans_param_df = param_df.loc[:,'Param Table':'type'].set_index('Param Table').dropna().T.to_dict('list')

        # convert to int
        for key,value in self.trans_param_df.items():
            self.trans_param_df[key][0]=int(self.trans_param_df[key][0]) if self.trans_param_df[key][1]=='int' else self.trans_param_df[key][0]

    def run_level_Growth_Slack(self):
        # should return things: 1: signal 2: name of the chart 3: tuple of all the data in the chart
        df = self.raw_data_new_fmt['RGDP'].dropna()
        df = self.SU.conversion_to_q(df)
        self.new_wf.add_df('RGDP(raw index)',df,to_freq='Q')

        y_ticker = df.columns.tolist()[0]
        Lo_Filter = lf.loess_filter(df, None, y_ticker, None, False, self.trans_param_df['pot_gdp_loess_frac'][0])
        df_trend = Lo_Filter.estimate()[['y_fitted']]

        df_trend.columns = ['USA_potential_GDP']
        self.new_wf.add_df('RGDP(index trend line)',df_trend, to_freq='Q')

        df_gap = self.SU.minus_df1_df2(self.new_wf.df['RGDP(raw index)'], self.new_wf.df['RGDP(index trend line)'])
        df_gap = self.SU.divide_df1_df2(df_gap, self.new_wf.df['RGDP(index trend line)'])
        df_gap = self.SU.conversion_down_to_m(df_gap) # be prepared to be extended with CAI

        # extend the gap with GDP Nowcast - potential
        self.new_wf.add_df('GDP Nowcast',self.raw_data_new_fmt['RGDP_EXT_yoy'],to_freq='M')
        self.new_wf.add_df('potential_yoy',self.raw_data_new_fmt['potential_yoy'],to_freq='M')

        df_gap_cai = self.SU.minus_df1_df2(self.new_wf.df['GDP Nowcast'], self.new_wf.df['potential_yoy']) / 12 / 100
        df_gap_cai.columns = ['cai-gdp_month']
        df_gap_cai = df_gap_cai.cumsum()

        # splice together
        last_date = df_gap.last_valid_index()
        shift = df_gap.loc[last_date].values - df_gap_cai.loc[last_date].values
        df_gap_cai = df_gap_cai + shift
        df_gap_cai = df_gap_cai.loc[last_date:, :]
        df_gap.columns = df_gap_cai.columns
        df_gap = pd.concat([df_gap, df_gap_cai], axis=0)
        df_gap = self.SU.drop_duplicate(df_gap)

       # adding raw data: series + trend line
        self.new_wf.add_df('growth rate (raw)', self.new_wf.df['RGDP(raw index)'].dropna().pct_change(4) * 100,
                           to_freq='Q')
        self.new_wf.add_df('growth rate (trend)', self.new_wf.df['RGDP(index trend line)'].dropna().pct_change(4) * 100,
                           to_freq='Q')

        new_name = 'USA_GDP_slack_z'
        df_gap = self.SM.z(df_gap,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['rgdp_pot_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('Growth Slack (z score)',df_gap[[new_name]])

        return

    def run_level_Growth_vs_Pot(self):
        #calculate the difference between growth and potential
        df = self.SU.minus_df1_df2(self.new_wf.df['GDP Nowcast'],self.new_wf.df['potential_yoy'])
        # calculate the  score
        new_name = 'Growth_Pot_z'
        df = self.SM.z(df,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['cai_pot_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('growth - potential(z score)',df[[new_name]],to_freq='bday')

    def run_level_URate_trend(self):
        '''
        :return:economic slack is the combination of the unemployment rate, capacity utility, real gdp growth, wage and inflation
        '''
        # import the unemloyment rate (URATE)
        df_urate = self.SU.conversion_down_to_m(self.raw_data_new_fmt['Unemployment rate'])
        df_urate.loc[:'1989-12-31'] = np.nan
        y_ticker = df_urate.columns.tolist()[0]
        Lo_Filter = lf.loess_filter(df_urate, None, y_ticker, None, False, 0.65)
        df_trend_urate = Lo_Filter.estimate()[['y_fitted']]
        df_trend_urate.columns = ['USA_urate_trend']
        # adding raw data and trend line
        self.new_wf.add_df('unemployment rate (trend)',df_trend_urate,to_freq='M')
        self.new_wf.add_df('unemployment rate (raw)',df_urate,to_freq='M')

        df_URATEmTREND = self.SU.minus_df1_df2(self.new_wf.df['unemployment rate (raw)'],self.new_wf.df['unemployment rate (trend)'])

        # calc the z_score
        new_name = 'URATEmTREND_z'
        df_urate_z = -self.SM.z(df_URATEmTREND, mean_type='custom',sd_type='rolling',roll_sd=12*20,custom_mean=0,new_name=new_name)
        self.new_wf.add_df('unemployment rate-trend(z score)',df_urate_z[[new_name]],to_freq='bday')

    def run_level_CapU_trend(self):
        '''
        :return:economic slack is the combination of the unemployment rate, capacity utility, real gdp growth, wage and inflation
        '''
        df_capu = self.SU.conversion_down_to_m(self.raw_data_new_fmt['Capacity_Utilisation'])
        y_ticker = df_capu.columns.tolist()[0]
        Lo_Filter = lf.loess_filter(df_capu, None, y_ticker, None, False, 0.75)
        df_trend_capu = Lo_Filter.estimate()[['y_fitted']]
        df_trend_capu.columns = ['capacity utilisation trend']
        self.new_wf.add_df('capacity utilisation (raw)',df_capu,to_freq='M')
        self.new_wf.add_df('capacity utilisation (trend)',df_trend_capu,to_freq='M')

        # calculate the z score of trailing diff
        df_CAPUmTREND = self.SU.minus_df1_df2(self.new_wf.df['capacity utilisation (raw)'],self.new_wf.df['capacity utilisation (trend)'])
        new_name = 'Cap_Util_Trend_z'
        df_CAPUmTREND = self.SM.z(df_CAPUmTREND, mean_type='custom',sd_type='rolling',roll_sd=12*20,custom_mean=0,new_name=new_name)
        self.new_wf.add_df('capacity utilisation (z score)',df_CAPUmTREND[[new_name]],to_freq='bday')

    def run_level_wages_trend(self):
        # download the wage data
        df = self.SU.conversion_down_to_m(self.raw_data_new_fmt['wage_nfp'])
        df = self.SU.sea_adj(df)

        df = self.SU.smooth_change(df.dropna(),periods=self.trans_param_df['wages_trend_growth'][0],ann=True,ann_type='geo')
        df.columns = ['wages_6m_chg']
        y_ticker = df.columns[0]
        Lo_Filter = lf.loess_filter(df, None, y_ticker, None,False,self.trans_param_df['wages_trend_growth_loess_frac'][0])
        df_trend = Lo_Filter.estimate()[['y_fitted']]
        df_trend.columns = ['wages_6m_trend']

        self.new_wf.add_df('wage_6m (raw)', df, to_freq='M')
        self.new_wf.add_df('wages_6m (trend)',df_trend,to_freq='M')

        df_diff = self.SU.minus_df1_df2(self.new_wf.df['wage_6m (raw)'],self.new_wf.df['wages_6m (trend)'],na='drop')
        new_name = 'wages_trend_6m_z'
        df_diff = self.SM.z(df_diff,mean_type ='custom',sd_type='rolling',roll_sd=self.trans_param_df['wages_trend_z_sd_roll'][0],custom_mean=self.trans_param_df['wages_trend_z_mean'][0],new_name=new_name)

        self.new_wf.add_df('wage_6m (z_score)', df_diff[[new_name]],to_freq='bday')
        return

    def run_level_inf_targ(self):
        # Using a better gauge for inflation
        #FRB cpi
        df_cpi = self.SU.conversion_down_to_m(self.raw_data_new_fmt['USA_FRB_CPI'])
        df_targ = self.SU.conversion_down_to_m(self.raw_data_new_fmt['inflation target'])
        self.new_wf.add_df('CPI FRB-Trimmed mean 6m (raw)',df_cpi,to_freq='M')
        self.new_wf.add_df('inflation target',df_targ,to_freq='M')
        self.new_wf.df['inflation target'] = self.new_wf.df['inflation target'].fillna(method='bfill')

        df1 = self.SU.minus_df1_df2(self.new_wf.df['CPI FRB-Trimmed mean 6m (raw)'],self.new_wf.df['inflation target'],na='drop')
        new_name = 'inflation_target_z'
        df1 = self.SM.z(df1,mean_type = 'custom',sd_type='rolling',roll_sd=self.trans_param_df['core_cpi_targ_z_sd_roll'][0],custom_mean=self.trans_param_df['core_cpi_targ_z_mean'][0],new_name=new_name)

        df1 = df1[[new_name]]
        self.new_wf.add_df('cpi(frb trimmed mean) vs target (z score)',df1)
        return

    def run_level_inf_targ_GS_tracker(self):
        #REF: https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fresearch.gs.com%2Fcontent%2Fresearch%2Fsite%2Fsearch.html%3Fq0%3D%2522GS%2520ECONOMIC%2520INDICATORS%2522%26f0%3Dquery_title%26type1%3DAND%26q1%3Dmodel%26f1%3Dquery_reportTypes%26type2%3DNOT%26q2%3DGLOBAL%26f2%3Dquery_title%26page%3D1%26frompg%3D1%26topg%3D999%26minpg%3D1%26maxpg%3D201%26lang%3Den%26sort%3Dtime&data=02%7C01%7Cjyang%40caxton.com%7Cf54564e01adb4ba8fb1308d6e4f97a77%7C70c99524736c49f4b73e75d7fa126cf9%7C0%7C0%7C636948157507900657&sdata=7XyWEKfLd7rM9RHJTytSUWBVzAZxkLvrEFbMklcTk6c%3D&reserved=0
        if not 'GS Core Inflation Tracker' in self.new_wf.df.keys():
            import_dir = os.path.join(self.LOCAL_MACRO_DATA_DIR,"GS porttal indicators","model.xlsx")
            df = pd.read_excel(import_dir, sheet_name='Core Inflation Tracker',index_col=0)
            df.index = pd.to_datetime(df.index)
            self.new_wf.add_df('GS Core Inflation Tracker (raw)', df[['GS Core Inflation Tracker']], to_freq='M')
        if not 'inflation target' in self.new_wf.df.keys():
            df_targ = self.SU.conversion_down_to_m(self.raw_data_new_fmt['inflation target'])
            self.new_wf.add_df('inflation target', df_targ, to_freq='M')
            self.new_wf.df['inflation target'] = self.new_wf.df['inflation target'].fillna(method='bfill')

        df1 = self.SU.minus_df1_df2(self.new_wf.df['GS Core Inflation Tracker (raw)'], self.new_wf.df['inflation target'], na='drop')
        new_name = 'inflation_target_z(GS_tracker)'
        df1 = self.SM.z(df1, mean_type='custom', sd_type='rolling', roll_sd=self.trans_param_df['core_cpi_targ_z_sd_roll'][0],
                   custom_mean=self.trans_param_df['core_cpi_targ_z_mean'][0], new_name=new_name)

        df1 = df1[[new_name]]
        self.new_wf.add_df('cpi(GS tracker) vs target (z score)', df1)

    def run_level_bei5_targ(self):
        # concept: breakeven inflation compared with the inflation target
        if not 'inflation target' in self.new_wf.df.keys():
            df_targ = self.SU.conversion_down_to_m(self.raw_data_new_fmt['inflation target'])
            self.new_wf.add_df('inflation target', df_targ, to_freq='M')
            self.new_wf.df['inflation target'] = self.new_wf.df['inflation target'].fillna(method='bfill')

        self.new_wf.add_df('Inf_Targ_daily',self.new_wf.df['inflation target'],to_freq='bday')
        self.new_wf.add_df('BEI 5Y (raw)',self.raw_data_new_fmt['BEI_5Y'])

        df = self.SU.minus_df1_df2(self.new_wf.df['BEI 5Y (raw)'],self.new_wf.df['Inf_Targ_daily'],na='drop')
        new_name = 'Breakeven_CPI_Targ_z'
        df = self.SM.z(df,mean_type='custom',sd_type='rolling',roll_sd=252*20,custom_mean=0,new_name=new_name)
        self.new_wf.add_df('BEI 5y (z score)',df[[new_name]])

    def run_level_wages_atlant_trend(self):
        df = self.SU.conversion_down_to_m(self.raw_data_new_fmt['wage_atlanta_tracker'])
        df = self.SU.sea_adj(df)
        df.columns = ['wage_atlanta_yoy']
        self.new_wf.add_df('wage(atlanta) yoy (raw)', df, to_freq='M')

        y_ticker = df.columns[0]
        Lo_Filter = lf.loess_filter(df, None, y_ticker, None, False, self.trans_param_df['wages_trend_growth_loess_frac'][0])
        df_trend = Lo_Filter.estimate()[['y_fitted']]
        df_trend.columns = ['wages_atl_yoy_trend']

        self.new_wf.add_df('wages_atlanta_yoy (trend)', df_trend, to_freq='M')

        df_diff = self.SU.minus_df1_df2(self.new_wf.df['wage(atlanta) yoy (raw)'],self.new_wf.df['wages_atlanta_yoy (trend)'], na='drop')
        new_name = 'wages(atlanta)_trend_z'
        df_diff = self.SM.z(df_diff, mean_type='custom', sd_type='rolling', roll_sd=self.trans_param_df['wages_trend_z_sd_roll'][0],
                       custom_mean=self.trans_param_df['wages_trend_z_mean'][0], new_name=new_name)

        self.new_wf.add_df('wages(atlanta)_trend (z score)', df_diff[[new_name]], to_freq='bday')
        return

    def run_change_in_growth(self):
        # x months of level change in now cast
        df = self.SU.smooth_change(self.new_wf.df['GDP Nowcast'],self.trans_param_df['cai_chg_roll'][0],ann_type='aris',ann=True)
        self.new_wf.add_df('growth_3x6 (raw)',df,to_freq='M')
        new_name = 'growth_3x6_z'
        df = self.SM.z(df,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['cai_chg_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('growth 3x6 (z score)',df[[new_name]],to_freq='bday')
        return

    def run_change_in_growth_citi_econ(self):
        # level change in citi change index
        df = self.raw_data_new_fmt['Citi_Econ_Chg_Index'].dropna()
        df = self.SU.smooth_change(df,periods=self.trans_param_df['citi_chg_index_1st'][0],ann_type='aris',ann=True)
        self.new_wf.add_df('Citi_Econ_Chg_Index_1st (raw)',df)
        new_name = 'Citi_Econ_Chg_Index_1st_z'
        df = self.SM.z(df,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['citi_chg_index_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('Citi_Econ_Chg_Index_6m (z score)', df[[new_name]], to_freq='bday')
        return

    def run_chg_on_wage_growth(self):
        df = self.SU.smooth_change(self.new_wf.df['wage_6m (raw)'], self.trans_param_df['wage_chg_in_growth_roll_second'][0], ann_type='aris',ann=True)
        self.new_wf.add_df('wage_6x6m (raw)',df,to_freq='M')
        new_name = 'wage_growth_6x6_z'
        df = self.SM.z(df, mean_type='custom', sd_type='rolling', custom_mean=self.trans_param_df['wage_chg_in_growth_z_mean'][0],
                  roll_sd=self.trans_param_df['wage_chg_in_growth_z_sd_roll'][0], new_name=new_name)
        self.new_wf.add_df('wage growth 6x6 (z score)', df[[new_name]])
        return

    def run_chg_in_cpi(self):
        df = self.SU.smooth_change(self.new_wf.df['CPI FRB-Trimmed mean 6m (raw)'],self.trans_param_df['core_cpi_chg'][0],ann_type='aris')
        df = self.SU.smooth_change(df, self.trans_param_df['core_cpi_chg_second'][0], ann_type='aris')

        new_name = 'CPI FRB-timmed_mean_2nd_z'
        df = self.SM.z(df,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=12*20,new_name=new_name)

        self.new_wf.add_df('CPI FRB-timmed_mean_2nd (z score)', df[[new_name]])

    def run_chg_in_cpi_pce(self):
        df = self.SU.conversion_down_to_m(self.raw_data_new_fmt['Core_PCE_CPI'])
        df_1st = self.SU.smooth_change(df,self.trans_param_df['cpi_pce_1st'][0])
        self.new_wf.add_df('cpi pce_1st (raw)', df_1st, to_freq='M')
        df_2nd = self.SU.smooth_change(df_1st,self.trans_param_df['cpi_pce_2nd'][0],ann_type='aris')
        df_2nd.columns = ['cpi_pce_2nd']
        self.new_wf.add_df('cpi pce 2nd (raw)',df_2nd,to_freq='M')

        new_name = 'cpi_pce_2nd_z'
        df_z = self.SM.z(df_2nd,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['cpi_pce_chg_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('cpi(pce)_2nd (z score)',df_z[[new_name]],to_freq='bday')

    def run_surprise_in_cpi(self):
        df = self.SU.conversion_down_to_m(self.raw_data_new_fmt['inflation_surprise'])
        df = self.SU.smooth_change(df,self.trans_param_df['cpi_surprise_first'][0],ann_type='aris',ann=True)
        df.columns = ['cpi surprise first']
        self.new_wf.add_df('cpi surprise first (raw)',df,to_freq='M')
        new_name = 'CPI_Surprise_diff_z'
        df = self.SM.z(df,mean_type='custom',sd_type='full',custom_mean=0,new_name=new_name)
        df.fillna(method='bfill', inplace=True)
        self.new_wf.add_df('CPI surprise diff (z score)', df[[new_name]])

    def run_chg_in_GS_cpi_tracker(self):
        # using the core inflation tracker from GS
        if not 'GS Core Inflation Tracker' in self.new_wf.df.keys():
            import_dir = os.path.join(self.LOCAL_MACRO_DATA_DIR,"GS porttal indicators","model.xlsx")
            df = pd.read_excel(import_dir,sheet_name = 'Core Inflation Tracker',index_col=0,header=0)
            df.index = pd.to_datetime(df.index)
            self.new_wf.add_df('GS Core Inflation Tracker',df[['GS Core Inflation Tracker']],to_freq='M')

        new_name = 'USA_CPI_diff(GS_Tracker)'
        df = self.SU.smooth_change(self.new_wf.df['GS Core Inflation Tracker'],periods =self.trans_param_df['core_cpi_chg'][0],ann_type='aris')
        df.columns = [new_name]
        self.new_wf.add_df('CPI_diff(GS_Tracker) (raw)',df[[new_name]],to_freq='M')

        df = self.SM.z(df,mean_type='custom',custom_mean=0,sd_type='rolling',roll_sd=12*20,new_name=new_name)
        self.new_wf.add_df('CPI_diff(GS_Tracker)(z score)',df[[new_name]], to_freq='bday')

    def run_chg_in_cpi_bei5(self):
        # run the change of cpi
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['BEI_5Y'])
        df = self.SU.smooth_change(df,periods=6*21,ann_type='aris')
        self.new_wf.add_df('Chg_on_Breakeven_CPI (raw)',df)
        new_name = 'Chg_on_Breakeven_CPI_z'
        df = self.SM.z(df,mean_type='custom',custom_mean=0,sd_type='rolling',roll_sd=252*20,new_name = new_name)
        self.new_wf.add_df('Chg_on_Breakeven_CPI (z score)',df[[new_name]])

    def run_HP_impulse(self):
        # 6 month change in HP
        # download the data
        df = self.SU.conversion_down_to_m(self.raw_data_new_fmt['housing_prices_CS'])
        df.columns = ['CS_housing_index']

        # add 6m_annualised chg
        df = self.SU.smooth_change(df, periods=self.trans_param_df['hp_impulse_trans_first'][0], ann_type='geo',ann=True)
        new_name = 'CS_housing_index_1st'
        df.columns = ['CS_housing_index_1st']
        self.new_wf.add_df('CS_housing_index_1st (raw)',df[[new_name]],to_freq='M')

        # adding housing price trend
        y_ticker = df.columns[0]
        Lo_Filter = lf.loess_filter(df, None, y_ticker, None, False,0.75)
        df_trend = Lo_Filter.estimate()[['y_fitted']]
        df_trend.columns = ['housing_price_1st_trend']
        self.new_wf.add_df('housing_prices_1st (trend)',df_trend,to_freq='M')

        # z score
        df_diff = self.SU.minus_df1_df2(self.new_wf.df['CS_housing_index_1st (raw)'],self.new_wf.df['housing_prices_1st (trend)'],na='drop')
        new_name = 'housing_price_1st_z'
        df_diff = self.SM.z(df_diff,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['hp_impulse_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('housing_prices_1st (z score)',df_diff[[new_name]],to_freq='bday')

        # add 2nd order transformation, and converting to z score
        df2 = self.SU.smooth_change(self.new_wf.df['CS_housing_index_1st (raw)'], periods=self.trans_param_df['hp_impulse_trans_second'][0], ann_type='aris',ann=True)
        self.new_wf.add_df('CS_housing_index_2nd (raw)',df2,to_freq='M')

        new_name = 'CS_housing_2nd_z'
        df2 = self.SM.z(df2,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['hp_impulse_z_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('CS_housing_2nd (z score)', df2[[new_name]])

    def run_total_cai_impulse(self):
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['GS_CAI_Impulse']).dropna()
        self.new_wf.add_df('GS_CAI_Impulse (raw)',df)
        new_name = 'GS_CAI_Impulse_z'
        df = self.SM.z(df,mean_type='custom',sd_type='ewm',ewm_alpha=0.01,custom_mean=0,new_name=new_name)
        self.new_wf.add_df('GS_CAI_Impulse (z score)', df[[new_name]])

        new_name = 'GS_CAI_Impulse_z_smooth'
        df['GS_CAI_Impulse_z_smooth'] = df.iloc[:,-1].rolling(window=self.trans_param_df['fci_impulse_z_smooth'][0]).mean()
        self.new_wf.add_df('GS_CAI_Impulse (z score smooth)',df[[new_name]])

        return

    def run_oil_impulse(self):
        df = self.raw_data_new_fmt['brent']
        df.columns = ['Oil']
        self.new_wf.add_df('Oil (raw)',df)

        df = self.SU.smooth_change(df.dropna(), periods=self.trans_param_df['oil_impulse_trans_first'][0])
        df = self.SU.smooth_change(df, periods = self.trans_param_df['oil_impulse_trans_second'][0],ann_type='aris')
        self.new_wf.add_df('Oil_impulse_Lc_2nd (raw)',df)

        new_name = 'Oil_Impulse'
        df = self.SM.z(df,mean_type='custom',custom_mean=0,sd_type='ewm',new_name=new_name)

        self.new_wf.add_df('Oil_Impulse (z score)', df[[new_name]])

    def run_fx_impulse(self):
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['FX_NEER'])
        self.new_wf.add_df('FX (raw)',df)

        df = -self.SU.smooth_change(df.dropna(), periods=self.trans_param_df['fx_impulse_trans_first'][0])
        df = self.SU.smooth_change(df,periods=self.trans_param_df['fx_impulse_trans_second'][0],ann_type='aris')
        self.new_wf.add_df('FX_impulse_2nd (raw)',df)

        new_name = 'FX_Impulse'
        df = self.SM.z(df,mean_type='custom',custom_mean=0,sd_type='ewm',new_name=new_name)
        self.new_wf.add_df('FX_Impulse (z score)',df[[new_name]])

    def run_pricing_2y(self):
        import_dir = r"C:\_CODE\JYang\JY_Project\macro_data\rates\ratesTable.csv"
        df = pd.read_csv(import_dir, index_col=0, header=0)
        df.index = pd.to_datetime(df.index)
        df_1w = df[['USA_Swap_1wk']].dropna()
        df_pr = self.new_wf.df['policy_rate'].dropna()
        df_1w = self.SU.extend_backward_df1_by_df2(df_1w,df_pr,method='aris')

        df_1w_od = self.SU.conversion_to_bDay(self.raw_data_new_fmt['IRS_1W_OIS_OPENDATA'])
        df_1w = self.SU.extend_df1_with_most_recent_df2(df_1w,df_1w_od,method='aris')

        # prepare the 2y1d data
        df_2y1d, df_3y = self.SU.conversion_to_bDay(self.raw_data_new_fmt['IRS_2y1m_OIS_Citi'].dropna()), self.SU.conversion_to_bDay(
            df[['USA_Swap_3yr']].dropna())
        df_2y1d = self.SU.extend_backward_df1_by_df2(df_2y1d,df_3y,method='aris')
        df_2y1d = self.SU.remove_outlier(self.SU.conversion_to_bDay(self.SU.drop_duplicate(df_2y1d)).dropna())

        df_comb = pd.concat([df_2y1d, df_1w], axis=1)
        self.new_wf.add_df('ois_1w', df_1w)
        self.new_wf.add_df('2y1d', df_2y1d)

        df_pricing = df_comb.iloc[:, 0] - df_comb.iloc[:, 1]
        df_pricing = df_pricing.to_frame()
        df_pricing = df_pricing - 0.2
        df_pricing.columns = ['2y1d-1w-20bps']

        self.new_wf.add_df('2y1d-1w-20bps', df_pricing)
        new_name = 'pricing_2y (z score)'
        df_z = self.SM.z(df_pricing, mean_type='custom', sd_type='rolling',
                    custom_mean=0, roll_sd=self.trans_param_df['priced_z_sd_roll'][0], new_name=new_name)
        self.new_wf.add_df(new_name, df_z[[new_name]])

    def run_pricing(self):
        df_pr = self.SU.conversion_to_bDay(self.raw_data_new_fmt['policy_rate'])
        self.new_wf.add_df('policy_rate',df_pr)
        df_irs = self.SU.conversion_to_bDay(self.raw_data_new_fmt['IRS_5Y'])
        self.new_wf.add_df('IRS_5Y',df_irs)

        delta_5y_pr = self.SU.minus_df1_df2(self.new_wf.df['IRS_5Y'],self.new_wf.df['policy_rate'])
        delta_5y_pr.columns = ['5y-PR']
        self.new_wf.add_df('5y-PR',delta_5y_pr,to_freq='bday')

        df_prm = self.new_wf.df['IRS_5Y'].copy()
        df_prm.columns = ['IRS_5Y']
        _5y_values = df_prm.iloc[:, 0].values
        _5y_values = [fin_lib.calc_duration(i, 5) for i in _5y_values]
        df_prm['_5y_duration'] = _5y_values
        df_prm['IRS_5Y'] = df_prm['IRS_5Y']/100
        df_prm['premium'] = df_prm['IRS_5Y'].diff(1).rolling(window = self.trans_param_df['pricing_z_premium_sdRoll'][0]).apply(np.std)*np.sqrt(252)*0.25*df_prm['_5y_duration']*100
        self.new_wf.add_df('premium', df_prm[['premium']])

        delta_5y_pr_premium = self.SU.minus_df1_df2(self.new_wf.df['5y-PR'], df_prm[['premium']])
        delta_5y_pr_premium.columns = ['5y-PR-Premium']
        self.new_wf.add_df('5y-PR-Premium',delta_5y_pr_premium)

        new_name = 'pricing_z'

        df_z = self.SM.z(self.new_wf.df['5y-PR-Premium'], mean_type='rolling', sd_type='rolling',
                    roll_mean=self.trans_param_df['priced_z_mean_roll'][0], roll_sd=self.trans_param_df['priced_z_sd_roll'][0], new_name=new_name)
        self.new_wf.add_df('pricing (z score)',df_z[[new_name]])

    def run_glob_level_growth(self):
        # using global CAI ex USA as a measurement global growth
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['World_ex_USA_avg_GDP'])
        df2 = self.SU.conversion_to_bDay(self.raw_data_new_fmt['World_ex_USA_avg_pot_GDP'])

        df_comb = pd.concat([df,df2],axis=1)
        df_comb['gap'] = df_comb.iloc[:,0]-df_comb.iloc[:,1]
        df_gap = df_comb[['gap']]
        self.new_wf.add_df('glob growth vs pot (raw)',df_gap)

        new_name = 'Glob_Growth_Pot_z'
        df_gap = self.SM.z(df_gap,mean_type='custom',sd_type='rolling',custom_mean=0,roll_sd=self.trans_param_df['glob_growth_pot_sd_roll'][0],new_name=new_name)
        self.new_wf.add_df('glob growth vs pot (z score)',df_gap[[new_name]])

    def run_glob_FCI(self):
        # using global FCI ex USA as a measurement of forward global growth
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['World_avg_FCI'])
        self.new_wf.add_df('World_avg_FCI (raw)', df)
        #note raw and z score are the same thing
        self.new_wf.add_df('World_avg_FCI (z score)',df)

    def run_glob_chg_growth(self):
        #change on the global growth, using the same parameter as in the USA change on growth
        df = self.SU.conversion_to_bDay(self.raw_data_new_fmt['World_ex_USA_avg_GDP'])

        df = self.SU.smooth_change(df,periods=self.trans_param_df['glob_chg_growth_1st'][0],ann_type='aris',ann=True)
        self.new_wf.add_df('Glob_Growth_3x6 (raw)',df,to_freq='no_change')

        new_name = 'Glob_Growth_3x6'
        # calc the z score manually for 2nd diff
        df['sd'] = self.SU.rolling_ignore_nan(df.iloc[:,[0]],_window=252*20,_func=np.std)
        df['sd'].fillna(inplace=True,method='bfill')
        sd_2010 = df.loc['2010-01-01':,:].iloc[:,0].std()
        df.loc['2010-01-01':, 'sd'] = sd_2010
        df[new_name] = df.iloc[:,0]/df.iloc[:,1]
        self.new_wf.add_df('Glob_Growth_3x6 (z score)',df[[new_name]])

# this should be called from a separate master module called 'bt'
if __name__=='__main__':
    USA = signal3()
    USA.initialise_and_run()

 




signal USA forward growth.py



# import sys when runing from the batch code
import sys
import os
sys.path.append(os.path.join(os.path.dirname(os.path.realpath(__file__)),"../.."))
import pandas as pd
import numpy as np
from datetime import datetime,timedelta
from textwrap import wrap
# visualisation packages
import math
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import collections
from matplotlib.font_manager import FontProperties

import Analytics.series_utils as s_util
import Analytics.abstract_sig as abs_sig
import Analytics.wfcreate as wf
from Analytics.wfcreate import use_cached_in_class
from Analytics.wfcreate import fwd_growthTree
from signals.RATES_FWDGROW_USA.run_signal import signal3 as base_signal3

class signal3(base_signal3):
    def __init__(self):
        super(signal3, self).__init__()

    def add_strat_info(self):
        # this is the part to override
        self.signal_ID = 'sig_0028'
        self.Short_Name = 'RATES_FWDGROW_USA_6m'
        self.Description = '''
                        Variation of the USA forward growth model: double the length of changes
                        
                       Cutting through the USA forward growth, i.e estimate the growth of different component individually with the impulse methodology.
                       GDP = C + I + G + X - M. In theory estimating each component and sum up to figure will give the best estimates on the forward growth.
                       The target is to estimate the forward 6m growth, but ideally 3m.
                       More specifically, to estimate the change of a variable A impact on the change of variable B. A should be normalised (maybe a second order diff is a good starting point).
                       B should be the change on the individual component of GDP.

                       C: bond prices, housing prices, equity prices, credit
                       I (residential): housing prices, housing start, mortgage rate
                       I (business): corporate credit spread
                       G: Fiscal impulse
                       X: trading partners growth, commodity, FX
                       M: mainly depends on the domestic demand. maybe we don't really care too much of this because the trend should in theory be the same with consumption.

                       The weight: should be able to decide the weight based on regression analysis.
                       However, maybe it is not a bad idea to decide the weight simply with correlation or visualisation??

                       Visualisation is also a key part of the analysis, especially the impulse responsive function is in particular a 
                       good starting point of visualisation already.
                       '''

    def add_dir_info(self):
        self.WKDIR = os.path.dirname(os.path.realpath(__file__))
        self.PROJ_DIR = os.path.join(self.WKDIR, "../..")
        self.MASTER_INPUT_DIR = os.path.join(self.PROJ_DIR, "input/master_input.xlsx")
        self.OUTPUT_DIR = os.path.join(self.PROJ_DIR, "output")
        self.SCRATCH_DIR = os.path.join(self.PROJ_DIR, "zzz_NO_commit_folder", self.Short_Name)
        self.INDICATOR_EXP_DIR = os.path.join(self.PROJ_DIR, "zzz_NO_commit_folder", self.Short_Name, 'indicator_group')
        self.TEMP_LOCAL_PICKLE = os.path.join(os.environ['TEMP'], 'TEMP_' + self.Short_Name + '_local_db.pickle')
        # Firstly export all the relevant result into the csv format. Secondly plot into the charts
        self.SHARE_DIR = r'Y:\MacroQuant\JYang\JY_Project'
        self.RPTDIR = os.path.join(self.SHARE_DIR, 'reporting', self.Short_Name) if os.access(self.SHARE_DIR, os.W_OK) else os.path.join(
            self.PROJ_DIR, 'reporting', self.Short_Name)
        #######
        self.IMPORT_DATA_DIR = os.path.join(self.WKDIR, '   ')

def run():
    GP = signal3()
    GP.initialise_and_run()

if __name__ == "__main__":
    run()
    print ('Done!')

 






portfolio.py





from collections import OrderedDict
from enum import IntEnum

import numpy as np
import pandas as pd
import datetime as dt
import panormus.quant.alib.utils as qau
import warnings
from scipy import stats


class ReturnMethod(IntEnum):
    DIFF = 1
    RATIO = 2
    LOG = 3


def get_returns(before, after, return_method=ReturnMethod.DIFF):
    '''
    :param before: scalar, numpy array, or DataFrame of starting values
    :param after: scalar, numpy array, or DataFrame of ending values
    :param ReturnMethod return_method: enumerated return method.
    :return: returns (e.g. after - before)
    '''
    if return_method == ReturnMethod.DIFF:
        res = after - before
    elif return_method == ReturnMethod.RATIO:
        res = after / before - 1
    elif return_method == ReturnMethod.LOG:
        res = np.log(after / before)
    else:
        raise ValueError(f'Bad return_method {return_method}')

    return res


def accumulate_returns(rets, return_method=ReturnMethod.DIFF):
    '''
    :param pd.DataFrame|pd.Series rets: returns to accumulate downward!
    :param ReturnMethod return_method: enumerated return method.
    :return: cumulative returns
    '''
    if return_method == ReturnMethod.DIFF:
        res = rets.cumsum()
    elif return_method == ReturnMethod.RATIO:
        res = (1 + rets).cumprod() - 1
    elif return_method == ReturnMethod.LOG:
        res = rets.cumsum()
    else:
       raise ValueError(f'Bad return_method {return_method}')

    return res


def date_index_yearfrac(idx):
    n = len(idx)
    yf = qau.ac_day_cnt_frac(min(idx), max(idx), 'act/act') * (n + 1) / n
    return yf


def df_series_yearfrac(rets):
    """
   Yearfrac that spans rets.
    If rets is pd.Series, returns a scalar with the yearfrac spanning all non-nan observations
    If rets is pd.DataFrame, returns a pd.Series with the yearfrac spanning all non-nan observations for each column

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts or %-age)
    """
    if isinstance(rets, pd.Series):
        yf = date_index_yearfrac(rets.dropna().index)
    else:
        yf = pd.Series([date_index_yearfrac(rets[c].dropna().index) for c in rets.columns], index=rets.columns)
    return yf


def df_series_n(rets):
    """
    Number of observations in rets.
    If rets is pd.Series, returns a scalar with the number of non-nan observations
    If rets is pd.DataFrame, returns a pd.Series with the yearfrac spanning all non-nan observations for each column

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts or %-age)
    """
    if isinstance(rets, pd.Series):
        n = len(rets.dropna())
    else:
        n = pd.Series([len(rets[c].dropna().index) for c in rets.columns], index=rets.columns)
    return n


def annual_return(rets, year_frac=None):
    """
    Compute the annual return based on a set of arithmetic returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """
    yf = year_frac or df_series_yearfrac(rets)
    return rets.sum() / yf


def vol_of_returns(rets):
    '''
    Compute the vol (standard deviation) of returns without demeaning.

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    '''
    return (rets ** 2).mean() ** 0.5


def annual_vol(rets, year_frac=None):
    """
    Compute the annual vol based on a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """
    daily_vol = vol_of_returns(rets)

    n = df_series_n(rets)
    yf = year_frac or df_series_yearfrac(rets)
    return daily_vol * np.sqrt(n / yf)


def sharpe_ratio(rets, year_frac=None):
    """
    compute the annual sharpe ratio based on a set of daily arithmetic returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """

    return annual_return(rets, year_frac) / annual_vol(rets, year_frac)


def z_score(rets):
    """
    compute the z-score of a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts or %-age)
    """
    zscore = rets.mean() / vol_of_returns(rets)
    return zscore


def t_stat(rets):
    """
    compute the t-stat of a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts or %-age)
    """

    n = df_series_n(rets)
    tstat = z_score(rets) * n ** 0.5
    return tstat


def p_value(rets, target=0, test='at_least', measure='sr', year_frac=None):
    """
    compute the p-value that a sample Sharpe ratio or t-stat is [at_least/at_most/equal/unequal] the target
    Sharpe ratio or t-stat

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional)
    :param float target: target Sharpe Ratio that you want to test the return series has
    :param test: direction of the test: options ('at_least', 'at_most', 'equal', 'unequal')
    :param measure: the tested measure (for nonzero target, 'sr' targets Sharpe, 'rets' targets individual returns)
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """

    if measure in ('returns', 'rets'):
        t_statistic = t_stat(rets - target)
    elif measure in ('sr', 'sharpe', 'sharpe-ratio'):
        sr = sharpe_ratio(rets, year_frac)
        if isinstance(sr, pd.Series):
            yf = year_frac or pd.Series([date_index_yearfrac(rets[c].dropna().index) for c in rets.columns],
                                        index=rets.columns)
        else:
            yf = year_frac or date_index_yearfrac(rets.index)
        t_statistic = yf ** 0.5 * (sr - target)
    else:
        raise ValueError(f'Measure \'{measure}\' not recognized.')

    n = df_series_n(rets)

    if isinstance(t_statistic, pd.Series):
        p_onesided = pd.Series([stats.t.sf(t_statistic[c], n[c] - 1) for c in rets.columns], index=t_statistic.index)
    else:
        p_onesided = stats.t.sf(t_statistic, n - 1)

    if test == 'at_least':
        pvalue = 1 - p_onesided  # one-sided pvalue = Prob(sr > target)
   elif test == 'at_most':
        pvalue = p_onesided  # one-sided pvalue = Prob(sr < target)
    elif test == 'equal':
        pvalue = min(p_onesided, 1 - p_onesided) * 2  # two-sided pvalue = Prob(sr == target)
    elif test == 'unequal':
        pvalue = 1 - min(p_onesided, 1 - p_onesided) * 2  # two-sided pvalue = Prob(sr != target)

    return pvalue


def rolling_dd(rets):
    """
    compute the max drawdown of a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    """

    dd = rets.cumsum() - rets.cumsum().expanding().max()
    return dd


def max_dd(rets):
    """
    compute the max drawdown of a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    """

    return rolling_dd(rets).min()


def calmar_ratio(rets, year_frac=None):
    """
    compute annual return over max drawdown of a set of returns

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """

    return annual_return(rets, year_frac) / -max_dd(rets)


def annual_costs(rets_net, rets_gross, year_frac=None):
    """
    Compute the annual costs based on a net and gross return series

    :param rets_net: pd.Series or pd.DataFrame of daily arithmetic returns net of costs
    :param rets_gross: pd.Series or pd.DataFrame of daily arithmetic returns gross of costs
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """

    if not rets_net.index.equals(rets_gross.index):
        raise ValueError('Indices of rets_net and rets_gross are not aligned.')

    # here
    total_costs = rets_net.sum() - rets_gross.sum()
    yf = year_frac or date_index_yearfrac(rets_net.index)
    return total_costs / yf


def perf_metrics_old(rets, rets_gross=None, year_frac=None):
    """
    returns a dict of the main performance metrics

    :param rets: pd.Series or pd.DataFrame of daily arithmetic returns (e.g. actual USD amounts returned or %-age \
    of a fixed notional.
    :param rets_gross: returns series excluding trading costs (rets must be net of trading costs in this case). If \
    this variable is passed, return metrics will include costs. The indices of rets and rets_gross must be aligned.
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """

    warnings.warn(
        "This function will be going away soon. Use the function perf_metrics(ret) from this module instead.",
        PendingDeprecationWarning)

    if rets_gross is None:
        ac = np.nan
    else:
        ac = annual_costs(rets, rets_gross, year_frac)

    metrics = dict(
        annual_return=annual_return(rets, year_frac),
        annual_vol=annual_vol(rets, year_frac),
        annual_costs=ac,
        sharpe_ratio=sharpe_ratio(rets, year_frac),
        max_dd=max_dd(rets),
        calmar_ratio=calmar_ratio(rets, year_frac),
    )

    return metrics


def perf_metrics(rets, rets_gross=None, year_frac=None):
    """
    returns a dict of performance metrics

    :param pd.Series rets: pd.Series of daily arithmetic returns
    :param pd.Series rets_gross: returns series excluding trading costs \
    (rets must be net of trading costs in this case). \
    If this variable is passed, return metrics will include costs. The indices of rets and rets_gross must be aligned.
    :param float year_frac: year fraction for returns sample. If None: infer from min and max dates in index.
    """
    if rets_gross is None:
        ac = np.nan
    else:
        ac = annual_costs(rets, rets_gross, year_frac)

    n = len(rets)
    yf = year_frac or date_index_yearfrac(rets.index)
    avg_worst_5 = rets.nsmallest(5).mean() if n > 5 else rets.mean()
    metrics = dict(
        trade_return=rets.mean(),
        trade_vol=vol_of_returns(rets),
        zscore=z_score(rets),
        zscore_scaled=t_stat(rets),
        worst_return=rets.min(),
        avg_worst_5=avg_worst_5,
        return_to_worst_5=rets.mean() / -avg_worst_5,
       prct_positive=len(rets[rets > 0]) / n,
        prct_negative=len(rets[rets < 0]) / n,
        trades_per_year=n / yf,
        annual_costs=ac,
        annual_return=annual_return(rets, year_frac),
        annual_vol=annual_vol(rets, year_frac),
        max_dd=max_dd(rets),
        calmar_ratio=calmar_ratio(rets, year_frac),
        sharpe_ratio=sharpe_ratio(rets, year_frac),
    )

    return metrics


def get_random_rets(seed=None, n=1000, vol=0.2, ret=0.03, start_date=dt.datetime(2010, 1, 1)):
    """
    get random normal returns series (for testing or showing examples)

    :param seed: optionally provide a fixed seed to reproduce the same random numbers each time.
    :param n: number of returns to draw
    :param vol: annualized vol
    :param ret: annualized return
    :param start_date: start date of return series
    """
    if seed is not None:
        np.random.seed(seed)

    dates = pd.bdate_range(start_date, periods=n)
    days_per_year = n / date_index_yearfrac(dates)

    z = np.random.randn(n) * vol / np.sqrt(days_per_year) + ret / days_per_year
    rets = pd.Series(z, dates)
    return rets


def get_random_perf(seed=None, n=1000, vol=0.2, ret=0.03, start_date=dt.datetime(2010, 1, 1)):
    """
    get random arith brownian motion perf series

    :param seed: optionally provide a fixed seed to reproduce the same random numbers each time.
    :param n: number of returns to draw
    :param vol: annualized vol
    :param ret: annualized return
    :param start_date: start date of return series
    """
    x = get_random_rets(seed, n, vol, ret, start_date)
    return x.cumsum()


# TODO: Functions below this line are not properly vetted.
def get_rolling_max_drawdown(cum_rets):
    """
    :description: rolling max drawdown time series and max drawdown float since start
    :param cum_rets: array-like cumulative returns
    :return: np.ndarray for rolling drawdown and max drawdown
    """
    warnings.warn(
        "This function will be going away soon. Use the function rolling_dd(ret).expanding().min() from this module " +
        "instead.", PendingDeprecationWarning)

    mdd = 0
    peak = cum_rets[0]
    roll_dd = np.empty(0)
    for cum_ret in cum_rets:
        peak = cum_ret if cum_ret > peak else peak
        dd = 100 * (cum_ret / peak - 1)
        mdd = dd if dd < mdd else mdd
        roll_dd = np.append(roll_dd, mdd)
    return roll_dd, mdd


def get_rolling_drawdown(cum_rets):
    """
    :description: rolling drawdown time series
    :param cum_rets: array-like cumulative returns
    :return: np.ndarray
    """

    warnings.warn(
        "This function will be going away soon. Use the function rolling_dd(ret) from this module instead.",
        PendingDeprecationWarning)

    return -(cum_rets.expanding().max() - np.array(cum_rets))


def get_perf_metrics(rets):
    """
    :description: calculate performance metrics given daily returns
    :param rets: daily return series
    :return: dataframe of scalar performance metrics and rolling drawdown
    """

    warnings.warn(
        "This function will be going away soon. Use the function perf_metrics from this module instead.",
        PendingDeprecationWarning)

    average_returns = rets.mean() * 252
    ir = average_returns / (rets.std() * np.sqrt(252))
    portfolio = 100 + rets.cumsum()
    rolling_dd, mdd = get_rolling_max_drawdown(portfolio)
    calmar = average_returns / abs(mdd)
    return pd.DataFrame(
        {'average_returns (%)': round(average_returns, 2), 'ir': round(ir, 2), 'max_dd (%)': round(mdd, 2),
         'calmar': round(calmar, 2)}, index=['performance']), rolling_dd

 








trade utils.py

def get_min_trade_within_tolerance(target_position, current_position, tolerance):
    """
    find the amount to be traded when applying position inertia

    i.e. trade as little as possible subject to final position being within tolerance


    ---Input Arguments---
    :param float target_position:   target position
    :param float current_position:  current position
    :param float tolerance:         tolerance, trade is given such that new position will be in tolerance from target

    ---Returns---
    float   Amount to be traded
    """

    min_amount_to_trade, max_amount_to_trade = get_min_max_trade(target_position, current_position, tolerance)

    amount_to_trade = min(max_amount_to_trade, 0) + max(min_amount_to_trade, 0)

    return amount_to_trade


def get_max_roll_within_tolerance(target_position, current_position, tolerance, expiring_position):
    """find the amount to be roll when applying position inertia
    always roll as much as possible up to the target (but no more), within the tolerance
    (used when rolling is cheaper than closing a positions)

    IMPORTANT:  Traded amount will be capped at roll amount. I.e. you will still need to apply the
                get_min_trade_within_tolerance function

    ---Input Arguments---
    :param float target_position:   target position
    :param float current_position:  current position
    :param float tolerance:         tolerance, trade is given such that new position will be in tolerance from target
    :param float expiring_position: expiring position, i.e. its cheaper to roll this than to close it out

    ---Returns---
    float   Amount to be traded
    """

    min_amount_to_trade, max_amount_to_trade = get_min_max_trade(target_position, current_position, tolerance)

    if min_amount_to_trade >= 0:
        amount_to_roll = max(min(expiring_position, max_amount_to_trade), 0)
    elif min_amount_to_trade < 0 < max_amount_to_trade:
        amount_to_roll = max(min(expiring_position, max_amount_to_trade), min_amount_to_trade)
    elif max_amount_to_trade <= 0:
        amount_to_roll = max(min(expiring_position, 0), min_amount_to_trade)
    else:
        raise RuntimeError(f'Error in min_amount_to_roll or max_amount_to_roll, possible input error.')

    return amount_to_roll


def get_min_max_trade(target_position, current_position, tolerance):
    """find the min and max amounts that can still be traded within the tolerance

    ---Input Arguments---
    :param float target_position:   target position
    :param float current_position:  current position
    :param float tolerance:         tolerance, trade is given such that new position will be in tolerance from target

    ---Returns---
    2-tuple of floats   Lower and upper amount that can be traded to stay within tolerance
    """

    if tolerance < 0:
        ValueError(f'Tolerance must be non-negative. Input was: {tolerance}')

    net_position = current_position - target_position

    upper_limit = tolerance
    lower_limit = -tolerance

    # find max and min trades such that exposure remains within tolerance from 0
    min_amount_to_trade = lower_limit - net_position
    max_amount_to_trade = upper_limit - net_position

    return min_amount_to_trade, max_amount_to_trade

 




reporting_pair_wise_correl.py


"""Pairwise correlation report"""

 

import pandas as pd

import numpy as np

import utils_pvk.lib_plot_fns as plot_fns

from utils_pvk.cl_email import Email

import utils_pvk.lib_html_fns as html_fns

 

 

class PwCorrReport:

    def __init__(self, perf_1, perf_2, name_1_short=None, name_2_short=None, **kwargs):

        """perf_1 and perf_2 should be series of arithmetic cumulative returns and appropriately named"""

 

        # align the two series

        sd = max(perf_1.index[0], perf_2.index[0])

        ed = min(perf_1.index[-1], perf_2.index[-1])

 

        p_1, p_2 = perf_1[sd:ed].align(perf_2[sd:ed], join='outer')

 

        self.perf_1 = p_1.ffill() - p_1[0]

        self.perf_2 = p_2.ffill() - p_2[0]

 

        if name_1_short is None:

            self.name_1_short = perf_1.name

        else:

            self.name_1_short = name_1_short

 

        if name_2_short is None:

            self.name_2_short = perf_2.name

        else:

            self.name_2_short = name_2_short

 

        p = dict()

        p['rets_ndays'] = kwargs.pop('rets_ndays', 3)

        p['use_exponential_corr'] = kwargs.pop('use_exponential_corr', False)

        p['spanwindow'] = kwargs.pop('spanwindow', 252)

 

        self.p = p

        self.report = None

 

   def get_rolling_corr(self, rets_ndays=None, use_exponential_corr=None,  spanwindow=None):

 

        # parse input

        if rets_ndays is None:

            rets_ndays = self.p['rets_ndays']

        if use_exponential_corr is None:

            use_exponential_corr = self.p['use_exponential_corr']

        if spanwindow is None:

            spanwindow = self.p['spanwindow']

 

        # compute rolling corr

        rets = self.get_rets(rets_ndays)

 

        if use_exponential_corr:

            rolling_corr = rets.iloc[:, 0].ewma(span=spanwindow).corr(rets.iloc[:, 1])

        else:

            rolling_corr = rets.iloc[:, 0].rolling(window=spanwindow).corr(rets.iloc[:, 1])

 

        return rolling_corr

 

    def get_sample_corr(self, rets_ndays=None, start_date=None,  end_date=None):

 

        # parse input

        if rets_ndays is None:

            rets_ndays = self.p['rets_ndays']

        if start_date is None:

            start_date = self.perf_1.index[0]

        if end_date is None:

            end_date = self.perf_1.index[-1]

 

        # compute rolling corr

        corr = pd.concat([self.perf_1, self.perf_2], axis=1).diff(rets_ndays)[start_date:end_date].corr().iloc[0, 1]

 

        return corr

 

    def get_rets(self, rets_ndays=None):

        """DF of returns of both columns"""

 

        if rets_ndays is None:

            rets_ndays = self.p['rets_ndays']

 

        return pd.concat([self.perf_1, self.perf_2], axis=1).ffill().diff(rets_ndays)

 

    def get_rolling_corr_plot(self, title=None, figsize=None):

        """get rolling return"""

 

        if title is None:

            rets_ndays = self.p['rets_ndays']

            title = f'Rolling Correlation {self.perf_1.name} vs. {self.perf_2.name} ({rets_ndays}b overlapping returns)'

 

        rolling_corr = self.get_rolling_corr()

        img_path = plot_fns.plt_linelabels(rolling_corr.to_frame(), title=title, plot_close_after_use=True,

                                           use_overlabels=False, use_grid=True, rhs_axis=True, figsize=figsize,

                                           edge=dict(l=0.06, r=0.06, b=0.13, t=0.08), hline=0)

        return img_path

 

    def get_cumulative_return_plot(self, title=None, figsize=None):

        """get rolling return"""

        if title is None:

            title = f'Cumulative Returns {self.perf_1.name} vs. {self.perf_2.name}'

 

        plot_data = pd.concat([self.perf_1, self.perf_2], axis=1)

        img_path = plot_fns.plt_linelabels(plot_data, title=title, plot_close_after_use=True,

                                           use_overlabels=False, use_grid=True, rhs_axis=True,

                                           legend=list(plot_data.columns), figsize=figsize,

                                           edge=dict(l=0.06, r=0.06, b=0.13, t=0.08), hline=0)

        return img_path

 

    def get_rolling_return_plot(self, title=None, lookback_ndays=252, figsize=None):

        """get rolling return"""

        if title is None:

            title = f'{lookback_ndays}b Rolling Returns {self.perf_1.name} vs. {self.perf_2.name}'

 

        plot_data = pd.concat([self.perf_1, self.perf_2], axis=1).diff(lookback_ndays)

        img_path = plot_fns.plt_linelabels(plot_data, title=title, plot_close_after_use=True,

                                           use_overlabels=False, use_grid=True, rhs_axis=True,

                                           legend=list(plot_data.columns), figsize=figsize,

                                           edge=dict(l=0.06, r=0.06, b=0.13, t=0.08), hline=0)

        return img_path

 

    def get_3m_rolling_return_quartile_table(self, n_quantiles=10, name_primary='Strategy', name_secondary='CI'):

        rolling_3m_return = self.get_rets(63).dropna(axis=0)

        rolling_3m_corr = self.get_rolling_corr(3, spanwindow=63).dropna()

        rolling_3m_corr.name = 'corr'

        all = pd.concat([rolling_3m_return, rolling_3m_corr], axis=1).dropna(axis=0, how='any')

 

        quantile_rng = [i / n_quantiles for i in range(n_quantiles)] + [1]

        quantiles = all[self.perf_2.name].quantile(quantile_rng)

        quantiles.iloc[-1] += 1

 

        table_out = pd.DataFrame(index=np.array(range(n_quantiles)),

                                 columns=range(5))

 

        for i in range(n_quantiles):

            # i = 0

            q_lo = quantiles.iloc[i]

            q_hi = quantiles.iloc[i+1]

            idx = (q_lo <= all.iloc[:, 1].values) & (all.iloc[:, 1].values < q_hi)

            table_out.iloc[i, 0] = q_lo

            table_out.iloc[i, 1] = q_hi

            table_out.iloc[i, 2] = all.iloc[idx, 1].mean()

            table_out.iloc[i, 3] = all.iloc[idx, 0].mean()

            table_out.iloc[i, 4] = all.iloc[idx, 2].mean()

 

        colnames = [name_secondary + '<br> From', name_secondary + '<br> To',

                    name_secondary + '<br> Avg', name_primary + '<br> Avg',

                    'Corr<br> Avg']

 

        table_out.columns = colnames

        table_out.index = [f'{round(q/n_quantiles, 3)}-{round((q+1)/n_quantiles, 3)} quantile' for q in range(n_quantiles)]

 

        return table_out

 

    def send_email_report(self, to, report_type='default', title=None, cc='', bcc='', extra_corr_periods=None,

                          n_quantiles=4):

        if title is None:

            title = f'Correlation report {self.perf_1.name} vs. {self.perf_2.name}'

 

        if report_type == 'default':

            p = dict(rolling_plot=True, cumret_plot=True, rolret_plot=True, quantiles=True)

        elif report_type == 'ex_quantiles':

            p = dict(rolling_plot=True, cumret_plot=True, rolret_plot=True, quantiles=False)

        elif report_type == 'only_quantiles':

            p = dict(rolling_plot=False, cumret_plot=False, rolret_plot=False, quantiles=True)

        else:

            raise ValueError(f'report_type {report_type} not supported.')

 

        email = Email(to, title, cc=cc, bcc=bcc)

        html = email.initial_html()

 

        html += f'Correlation report between {self.perf_1.name} and {self.perf_2.name}<br><br>'

        corr_full_sample = self.get_sample_corr(3)

        html += f'Full Sample correlation: <b>{corr_full_sample:.2f}</b><br>'

 

        if extra_corr_periods is not None:

            for nm, rng in extra_corr_periods.items():

                i_corr = self.get_sample_corr(start_date=rng[0], end_date=rng[1])

                html += f'{nm} correlation: <b>{i_corr:.2f}</b><br>'

 

        html += '<br>'

 

        html += 'Please see correlation  below:<br>'

        if p['rolling_plot']:

            html += '- correlation per quantile table<br>'

        if p['rolling_plot']:

            html += f'- rolling returns chart<br>'

        if p['cumret_plot']:

            html += '- cumulative returns chart<br>'

        if p['rolling_plot']:

            html += '- rolling 1y returns chart<br>'

 

        html += '<br><br>'

 

        if p['quantiles']:

            table_df = self.get_3m_rolling_return_quartile_table(n_quantiles, self.name_1_short, self.name_2_short)

 

            if n_quantiles == 4:

                html += '<b><u>Correlation per return Quartile:</u></b><br>'

                table_df.index = [f'Bottom {self.name_2_short} 3m Return Quartile', '2nd', '3rd',

                                  f'Top {self.name_2_short} 3m Return Quartile']

                quantile_name = 'quartile'

            elif n_quantiles == 10:

                html += '<b><u>Correlation per return Decile:</u></b><br>'

                table_df.index = [f'Bottom {self.name_2_short} 3m Return Decile', '2nd', '3rd', '4th', '5th', '6th',

                                  '7th', '8th', '9th', f'Top {self.name_2_short} 3m Return Decile']

                quantile_name = 'decile'

            else:

                html += '<b><u>Correlation per return Quantile:</u></b><br>'

                quantile_name = 'quantile'

 

            table_df = table_df.iloc[:, 2:]

            html += html_fns.create_html_table(table_df.applymap(lambda x: f'{x:.2f}'), cell_width=80)

            html += (f'<i>{self.name_2_short} 3m rolling performance is split into 4 {quantile_name}s. The table ' +

                     f'shows the averages of the 3m rolling returns and 3m rolling correlations conditional on CI ' +

                     f'3m rolling return being in the respective {quantile_name}.</i>')

            html += '<br><br>'

 

        if p['rolling_plot']:

            html += '<b><u>Rolling 1y returns chart:</u></b><br>'

            rolling_title = f'1y Rolling Correlation of {self.perf_1.name} vs {self.perf_2.name}'

            img_path = self.get_rolling_corr_plot(title=rolling_title, figsize=[10, 3.5])

            html = email.add_inline_image(html, img_path)

            html += '<br>'

 

        if p['cumret_plot']:

            html += '<b><u>Cumulative returns chart:</u></b><br>'

            cumret_title = f'Cumulative Returns of {self.perf_1.name} vs {self.perf_2.name}'

            img_path = self.get_cumulative_return_plot(title=cumret_title, figsize=[10, 5])

            html = email.add_inline_image(html, img_path)

            html += '<br>'

 

        if p['cumret_plot']:

            html += '<b><u>1y rolling returns chart:</u></b><br>'

            rollret_title = f'Rolling 1y Returns of {self.perf_1.name} vs {self.perf_2.name}'

            img_path = self.get_rolling_return_plot(title=rollret_title, figsize=[10, 3.5])

            html = email.add_inline_image(html, img_path)

            html += '<br>'

 

        email.add_html_body(html)

 

        email.send()

 








pvk cl_email.py



import win32com.client as win32
import datetime as dt

import smtplib
from email.mime.application import MIMEApplication
from email.mime.image import MIMEImage
from email.mime.multipart import MIMEMultipart
from email.message import EmailMessage
from email.mime.text import MIMEText
from io import BytesIO
from os.path import basename

# panormus imports
import panormus.utils.chrono as puc

# utils_pvk imports
import utils_pvk.lib_string_fns as string_fns
import utils_pvk.lib_date_fns as date_fns


def process_recipient_arg(recipients):
    """
    :param str|iterable[str] recipients: comma separated. semicolon separated string, or iterable of strings.
    :return: comma-separated string of recipients
    """
    if not recipients:
        return ''
    elif isinstance(recipients, str):
        return recipients.replace(';', ',')
    else:
        return ','.join(recipients)


class Email:
    def __init__(self,
                 to="pvklooster@caxton.com",
                 subject="email subject",
                 body="error: could not read html email. please enable html to view this email or read in an html-compatible email client.",
                 htmlbody="html email body",
                 from_email="pvklooster@caxton.com",
                 cc='',
                 bcc=''):

        self.from_email = from_email
        self.to = to
        self.cc = cc
        self.bcc = bcc
        self.subject = subject
        self.htmlbody = htmlbody
        self.nohtml_body = body
        self.attachments = []
        self.inline_images = []
        self.msg = None

    def add_attachment(self, path):
        self.attachments.append(path)

    @staticmethod
    def initial_html():
        html = """<!--[if mso]>
        <style type="text/css">
            body, table, td {font-family: Calibri, sans-serif !important;, font-size:11pt !important}
        </style>
        <![endif]-->
        <div style="font-family: Calibri; font-size: 11pt;">
        """
        return html

    def add_inline_image(self, html_body, image_path):
        # image_cid = string_fns.hash(image_path)

        image_cid = (str(puc.now())).replace(' ', '')

        html_body += "<img src=\"cid:{0}\">".format(image_cid)

        with open(image_path, 'rb') as fp:
            msgImage = MIMEImage(fp.read())

        msgImage.add_header('Content-ID', image_cid)
        self.inline_images.append(msgImage)

        return html_body

    def add_html_body(self, htmlbody, add_autogen_msg=False):
        if add_autogen_msg:
            htmlbody += "<br><br><br>This is an automated message (generated {}utc)</i>".format(date_fns.to_str(dt.datetime.utcnow(), "dd-mmm-yyyy hh:mm"))

        htmlbody += "</div>"
        self.htmlbody = htmlbody

    def build(self):
        msg = MIMEMultipart()
        msg['Subject'] = self.subject
        msg['From'] = self.from_email
        msg['TO'] = process_recipient_arg(self.to)
        msg['CC'] = process_recipient_arg(self.cc)
       msg['BCC'] = process_recipient_arg(self.bcc)
        msg.attach(MIMEText(self.htmlbody, 'html'))

        for fpath in self.attachments or []:
            with open(fpath, "rb") as file:
                part = MIMEApplication(
                    file.read(),
                    Name=basename(fpath)
                )
            # After the file is closed
            part['Content-Disposition'] = 'attachment; filename="%s"' % basename(fpath)
            msg.attach(part)

        if self.inline_images:
            for item in self.inline_images:
                msg.attach(item)

        self.msg = msg

    def send(self):
        print("sending email")
        if self.msg is None:
            self.build()

        with smtplib.SMTP('smtp.caxton.com') as s:
            s.send_message(self.msg)


if __name__ == "__main__":
    mail = Email(to="pvklooster@caxton.com", subject="test_dual_email_addresses")
    html = mail.initial_html()
    html += "Test image <br><br>"
    path = r"H:\python_local\storage\temp\plot_2019-06-24_10-26-09_bwnaqqm1ni.png"
    html = mail.add_inline_image(html, path)
    mail.add_html_body(html, False)
    mail.send()

 








pvk_data_bloomberg.py





import pdblp
import blpapi
import pandas as pd
import re
import datetime as dt
import os
import json
import sqlite3

# utils_pvk imports
from utils_pvk.cl_email import Email
import utils_pvk.lib_date_fns as date_fns
import utils_pvk.lib_data_fns as data_fns

BDH_EOD_CACHE_FOLDER = "H:\\python_local\\storage\\BBG_EoD\\"
ILLEGAL_CHARS = [":", "/", "\/", "\""]


eri_tickers = {
    "ERI Global": 'CGERGLOB Index',
    "ERI DM": 'CGERGLDM Index',
    "ERI EM": 'CGERGLEM Index',
    "ERI US": 'CGERUSA1 Index',
    "ERI EU ex UK": 'CGEREUXU Index',
    "ERI UK": 'CGERUKUK Index',
    "ERI Japan": 'CGERJAPN Index',
    "ERI Pacific ex Japan": 'CGERAPXJ Index',

    "ERI Global Financials": 'CGERGLFN Index',
    "ERI EM Financials": 'CGEREMFN Index',
    "ERI US Financials": 'CGERUSFN Index',
    "ERI EU ex UK Financials": 'CGEREXFN Index',
    "ERI UK Financials": 'CGERUKFN Index',
    "ERI Japan Financials": 'CGERJPFN Index',
}

def bbg_static(bbg_tickers, field):
    return get_bdp_static(bbg_tickers, field).loc[:, 'value'].values


def get_bbg(tickers, fields, start_date=None, end_date=None, live=True, raise_on_missing_symbol=True, get_new_data=True,
            use_filecache=True, ovrds=None, write=True, storage_location=None, read_last_modified=True,
            apply_cutoff=True, **_):
    """get bloomberg data, use cache and various options. no postprocessing of result

            ovrds: list of tuples
            List of tuples where each tuple corresponds to the override
            field and value
    """
    if start_date is None:
        start_date = dt.datetime(1900, 1, 1)
    # use_filecache=False
    if end_date is None:
        # end_date = dt.datetime.today()
        end_date = dt.datetime(2100, 1, 1)

    def get(x_ticker, x_field, i_end_date):

        if x_ticker.lower() in [s.lower() for s in eri_tickers.values()] and i_end_date > dt.datetime.today():
            i_end_date = dt.datetime.today()

        dat_out = __bdhx(x_ticker, x_field, start_date, i_end_date, not live, raise_on_missing_symbol, get_new_data,
                         use_filecache, ovrds=ovrds, write=write, storage_location=storage_location,
                         read_last_modified=read_last_modified, apply_cutoff=apply_cutoff)
        return dat_out

    # zip ticker/field pairs, either can be string or list
    if isinstance(fields, str):
        if isinstance(tickers, str):
            params = [(tickers.lower(), fields.lower())]
        else:
            params = [(i_ticker.lower(), fields.lower()) for i_ticker in tickers]
    else:
        if isinstance(tickers, str):
            params = [(tickers.lower(), i_field.lower()) for i_field in fields]
        else:
            params = [(i_ticker.lower(), i_fields.lower()) for i_ticker, i_fields in zip(tickers, fields)]

    listdata = [get(i_ticker, i_field, end_date) for i_ticker, i_field in params]
    dfdata = pd.concat(listdata, axis=1, sort=False)
    return dfdata


def get_bdp_static(tickers, fields, use_filecache=True, get_new_data=True, drop_existing_nan=True,
                   **_):
    """get bloomberg data, use cache and various options. no postprocessing of result"""

    def get(x_ticker, x_field):
        return __bdpx(x_ticker, x_field, use_filecache, get_new_data, drop_existing_nan)

    # zip ticker/field pairs, either can be string or list
    if isinstance(fields, str):
        if isinstance(tickers, str):
            params = [(tickers.lower(), fields.lower())]
        else:
            params = [(i_ticker.lower(), fields.lower()) for i_ticker in tickers]
   else:
        if isinstance(tickers, str):
            params = [(tickers.lower(), i_field.lower()) for i_field in fields]
        else:
            params = [(i_ticker.lower(), i_fields.lower()) for i_ticker, i_fields in zip(tickers, fields)]

    listdata = [get(i_ticker, i_field) for i_ticker, i_field in params]
    dfdata = pd.concat(listdata, axis=0, sort=False)

    return dfdata


# def sql_store(dfdata_new, dbpath, table_name):


# def get_bdp_static_2(tickers, field, use_filecache=True, get_new_data=True,
#                      **_):
#     """get bloomberg data, use cache and various options. no postprocessing of result"""
#
#     dbpath = BDH_EOD_CACHE_FOLDER + "static_fields.db"
#     table_name = "static_fields"
#
#     if use_filecache:
#         conn = sqlite3.connect(dbpath)
#         c = conn.cursor()
#         sql = "SELECT ticker, {} FROM {}".format(field, table_name)
#         dfdata_stored = c.execute(sql).fetchall()
#         dfdata_new = __bdp(tickers_new, field)
#         dfdata = pd.concat(dfdata_new, dfdata_stored)
#     else:
#         dfdata = __bdp(tickers, field)
#
#     if use_filecache and not dfdata_new.empty:
#         sql_success = sql_store(dfdata_new, dbpath, 'static_fields')
#         if not sql_success:
#             raise ValueError("Error storing SQL data. Db not updated")
#
#     dfdata = pd.concat(listdata, axis=0, sort=False)
#     return dfdata


def get_bds(ticker, field, **kwargs):
    """load bbg data through the (ported) bdh function - no use of bbg options
    (this should be handled in wrapper function)"""

    use_filecache = kwargs.pop("use_filecache", True)
    get_new_data = kwargs.pop("get_new_data", True)

    # step 1 - load existing data and compare
    base = ticker+"-"+field+"-"+json.dumps(kwargs)

    for char in ILLEGAL_CHARS:
        base = re.sub(char, "-", base)

    ticker_filename = BDH_EOD_CACHE_FOLDER + base + ".pickle"

    if os.path.isfile(ticker_filename) and use_filecache:
        return pd.read_pickle(ticker_filename)
    else:
        if get_new_data:
            data = __bds(ticker, field, **kwargs)
        else:
            data = pd.DataFrame()
        if use_filecache:
            data.to_pickle(ticker_filename)
        return data


def __bdhx(ticker, field, start_date, end_date, exclude_data_asof_today, raise_on_missing_symbol, get_new_data=True,
           use_filecache=True, ovrds=None, write=True, storage_location=None, read_last_modified=True,
           apply_cutoff=True):
    """cashed get of bloomberg data"""

    # step 1 - load existing data and compare
    base = ticker + "-" + field

    if ovrds is not None:
        for ovrd in ovrds:
            base += "-" + str(ovrd[0]) + str(ovrd[1])

    for char in ILLEGAL_CHARS:
        base = re.sub(char, "-", base)

    if not storage_location:
        ticker_filename = BDH_EOD_CACHE_FOLDER + base + ".pickle"
    else:
        ticker_filename = storage_location + base + ".pickle"

    load_fn = lambda start_date_x, end_date_x: __bdh(ticker, field, start_date_x, end_date_x, ovrds=ovrds)

    if use_filecache:
        return data_fns.cached_get(load_fn, ticker_filename, start_date, end_date, exclude_data_asof_today,
                                   raise_on_missing_symbol, get_new_data, write=write,
                                   read_last_modified=read_last_modified, apply_cutoff=apply_cutoff)
    else:
        return load_fn(start_date, end_date)


def __bdpx(ticker, field, use_filecache=True, get_new_data=True, drop_existing_nan=True):
    """cashed get of bloomberg data"""

    # step 1 - load existing data and compare
    base = ticker+"-"+field+"-bdp"

    for char in ILLEGAL_CHARS:
        base = re.sub(char, "-", base)

    ticker_filename = BDH_EOD_CACHE_FOLDER + base + ".pickle"

    if os.path.isfile(ticker_filename) and use_filecache:
        data = pd.read_pickle(ticker_filename)
    else:
        data = None

    if drop_existing_nan:
        condition = data is not None and not data.empty and not pd.isnull(data['value'].values[0])
    else:
        condition = data is not None

    if condition:
        return data
    else:
        if get_new_data:
            data = __bdp(ticker, field)
        else:
            data = pd.DataFrame()
        if use_filecache and not data.empty:
            data.to_pickle(ticker_filename)
        return data


def __bdh(ticker, field, start_date, end_date="", ovrds=None):
    """load bbg data through the (ported) bdh function - no use of bbg options
    (this should be handled in wrapper function)"""

    global BBG_CONN

    print("bbg_get: {}, {}".format(ticker, field))

    # parse data
    # field = "EQY_DVD_YLD_IND"
    if field.upper() in ("EQY_DVD_YLD_IND") and date_fns.to_datetime(start_date) < dt.datetime(1930, 1, 1):
        start_date_str = "19300101"
    else:
        start_date_str = date_fns.to_str(start_date, "bbg")
    end_date_str = date_fns.to_str(end_date, "bbg")

    # get data through existing bbg connection object

    try:
        con = BBG_CONN
        bbg_data = con.bdh(ticker, field, start_date_str, end_date_str, ovrds=ovrds)
        print("success")
    except NameError:
        BBG_CONN = pdblp.BCon(debug=False, port=8194, timeout=30000)
        con = BBG_CONN
        con.start()
        bbg_data = con.bdh(ticker, field, start_date_str, end_date_str, ovrds=ovrds)
        print("defined and started bbg")
    except AttributeError:
        con.start()
        bbg_data = con.bdh(ticker, field, start_date_str, end_date_str, ovrds=ovrds)
        print("started bbg")
    except blpapi.InvalidStateException:
        Email(subject="BLOOMBERG RESTART ATTEMPT", htmlbody="Attempting to restart Bloomberg").send()
        try:
            con.stop()
        except:
            print('no bbg connection to stop')

        BBG_CONN = pdblp.BCon(debug=False, port=8194, timeout=30000)
        con = BBG_CONN
        con.start()
        bbg_data = con.bdh(ticker, field, start_date_str, end_date_str, ovrds=ovrds)
        print("started bbg")
        Email(subject="BLOOMBERG RESTARTED SUCCESSFULLY", htmlbody="BLOOMBERG RESTARTED SUCCESSFULLY").send()
    # con.stop()

    if type(ticker) is not list:
        ticker = [ticker]

    columns = [i_ticker + "-" + field for i_ticker in ticker]
    if len(bbg_data) == 0:
        bbg_data = pd.DataFrame(columns=columns)
    else:
        bbg_data.columns = columns

    return bbg_data


def get_hist_bdp(tickers, field, start_date=None, end_date=None):
    """load time-series datapoint through bdp and store as a time-series (e.g. for CDS spreads)"""

    # get data through existing bbg connection object
    bbg_data = __bdp(tickers, field)

    def dummy_data_fn(start_date, end_date, ticker):
        if date_fns.to_date(start_date) <= dt.date.today() - 1 and date_fns.to_date(end_date) >= dt.date.today():
            return pd.Series([bbg_data[ticker]], index=[dt.date.today()])
        else:
            return pd.Series()

    data = pd.DataFrame(columns=tickers)
    for ticker in tickers:
        # step 1 - load existing data and compare
        base = ticker + "-" + field + "-bdp"

        for char in ILLEGAL_CHARS:
            base = re.sub(char, "-", base)

        full_path = BDH_EOD_CACHE_FOLDER + base + ".pickle"

        get_data_fn = lambda s, e: dummy_data_fn(s, e, ticker)

        data = data_fns.cached_get(get_data_fn, full_path, start_date, end_date,
                                   exclude_data_asof_today=False, raise_on_missing_symbol=False, get_new_data=True)

        data[:, ticker] = data

    return data


def __bdp(tickers, fields, ovrds=None):
    """load bbg data through the (ported) bdh function - no use of bbg options
    (this should be handled in wrapper function)"""

    # get data through existing bbg connection object
    con = __get_conn()
    bbg_data = con.ref(tickers, fields, ovrds=ovrds)

    return bbg_data


def __bds(ticker, field, **overrides):
    """load bbg data through the (ported) bdh function - no use of bbg options
    (this should be handled in wrapper function)"""

    # get data through existing bbg connection object
    overrides_list = [(key, val) for key, val in overrides.items()]
    con = __get_conn()
    bbg_data = con.bulkref(ticker, field, overrides_list)

    return bbg_data


def __get_conn():
    global BBG_CONN_VARIABLE

    def __conn_exists():
        try:
            if BBG_CONN_VARIABLE is None:
                return False
            else:
                return True
        except:
            return False

    if not __conn_exists():
        BBG_CONN_VARIABLE = pdblp.BCon(debug=False, port=8194, timeout=15000)
        BBG_CONN_VARIABLE.start()
    return BBG_CONN_VARIABLE


if __name__ == "__main__":
    # __bds('SPX Index', 'INDX_MWEIGHT_HIST', END_DT='19991231')

    data = get_bds('SPX Index', 'INDX_MWEIGHT_HIST', END_DT='19991231')

    import datetime
    ticker = ["SPY US Equity", "NKY Equity"]
    field = "PX_LAST"
    start_date = datetime.date(2018, 1, 1)
    end_date = datetime.date(2018, 5, 17)

    # data = get_bbg(ticker, field, start_date, end_date)
    data = get_bdp_static(ticker, field)
    data = get_bds(ticker, field)

    print(data)

 








pvk_data_citivelocity.py





import pandas as pd
import datetime as dt
import re

from panormus.data import citi_velocity as citi_velocity
# from panormus.data import citi_velocity_old as citi_velocity

# utils_pvk imports
import utils_pvk.lib_date_fns as date_fns
import utils_pvk.lib_data_fns as data_fns
import utils_pvk.lib_string_fns as string_fns


CITI_EOD_CACHE_FOLDER = "H:\\python_local\\storage\\CITI\\"


class CitiVelocityHist:
    def __init__(self):
        self.citi_client = citi_velocity.CitiClient('abc1fe56-7d84-41ac-a915-311e5020ccf4',
                                                    'Y1jX1qC3mS5jH5wF7aR1dG2qJ4wB6fS2pX0gU1kB2wS4vG1tV1')

    def get_hist_data(self, tags, start_date=None, end_date=None):
        if start_date is None:
            start_date = dt.date(1900, 1, 1)

        if end_date is None:
            end_date = dt.date(2100, 1, 1)

        def chunks(l, n=100):
            """Yield successive n-sized chunks from l."""
            for i in range(0, len(l), n):
                yield l[i:i + n]

        data_list = []
        for chunk in chunks(tags, 100):
            hist_data = self.citi_client.get_hist_data(
                tags=chunk,
                start_date=start_date, end_date=end_date
            )
            data_list.append(hist_data)

        data_out = pd.concat(data_list, axis=1, sort=True)
        return data_out


def get(tickers, start_date=None, end_date=None, live=True, raise_on_missing_symbol=True, get_new_data=True,
        use_filecache=True, read_last_modified=True, **kwargs):
    """get jpmdq data, use cache and various options. no postprocessing of result"""

    if isinstance(tickers, str):
        tickers = [tickers]

    tickers = [s.upper() for s in tickers]

    def get_single_cached(ticker):
        load_fn = lambda start_date_x, end_date_x: get_uncached(ticker, start_date_x, end_date_x, **kwargs)

        ticker_filename = CITI_EOD_CACHE_FOLDER + re.sub(r""":\/""", "-", ticker) + ".pickle"

        if use_filecache:
            return data_fns.cached_get(load_fn, ticker_filename, start_date, end_date, not live,
                                       raise_on_missing_symbol, get_new_data, read_last_modified=read_last_modified)
        else:
            return load_fn(start_date, end_date)

    listdata = [get_single_cached(i_ticker) for i_ticker in tickers]
    dfdata = pd.concat(listdata, axis=1, sort=False)
    return dfdata


def get_uncached(tags, start_date=None, end_date=None, **kwargs):
    if start_date is None:
        start_date = dt.datetime(1900, 1, 1)
    if end_date is None:
        end_date = dt.datetime(2099, 12, 31)

    if isinstance(tags, str):
        tags = [tags]

    tags = [s.upper() for s in tags]

    conn = CitiVelocityHist()

    print(tags)
    data = conn.get_hist_data(tags, start_date, end_date)

    # data.index = [date_fns.from_str(x) for x in data.index]
    cols = []
    for col in data.columns:
        if isinstance(col, str):
            cols.append(col)
        else:
            cols.append(col[1])
    data.columns = cols
# data2 = data
    if kwargs.get('cut_weekends', False):
        if len(data.index) > 0:
            data = data[data.index.dayofweek < 5]

    return data


if __name__ == "__main__":
    tags = ['RATES.VOL.EUR.ATM.NORMAL.DAILY.1M.3M', 'CREDIT.CDS.APLINC.SNRFOR.USD.MR14.5Y.BLENDED']

    # ccy = 'USD'
    # expiry = '1m'
    # tenor = '10y'
    # tickers_citi = {"premium":   f"RATES.VOL.{ccy}.ATM.FWDPREMIUM.{expiry}.{tenor}",
    #                 "vol":       f"RATES.VOL.{ccy}.ATM.NORMAL.ANNUAL.{expiry}.{tenor}",
    #                 "fwd_rate":  f"RATES.SWAP.{ccy}.FWD.{expiry}.{tenor}",
    #                 "spot_rate": f"RATES.SWAP.{ccy}.PAR.{tenor}",
    #                 }
    # tags = tickers_citi.values()
    tags = ["RATES.VOL.USD.ATM.FWDPREMIUM.1M.10Y"]
    start_date = dt.date(2017, 1, 1)
    end_date = dt.date.today()
    # data = get(tags, start_date, end_date)
    data = get_uncached(tags, start_date, end_date)
    data.head()
    print('done')

 






pvk data_jpm.py





import pandas as pd
import datetime as dt
import re
import pickle

import panormus.data.jpmdq as jpmdq
# import panormus.data.jpmdqold as jpmdqold

# utils_pvk imports
import utils_pvk.lib_date_fns as date_fns
import utils_pvk.lib_data_fns as data_fns
import utils_pvk.lib_string_fns as string_fns

JPMDQ_EOD_CACHE_FOLDER = "H:\\python_local\\storage\\JPMDQ\\"
JPMDQ_USER = "vantklooster_api"
JPMDQ_PW = "JdfIjdieDjfiejD78131"


def get(tickers, start_date=None, end_date=None, live=True, raise_on_missing_symbol=True, get_new_data=True,
        use_filecache=True, read_last_modified=True, force_full_refresh=False, **kwargs):
    """get jpmdq data, use cache and various options. no postprocessing of result"""

    if isinstance(tickers, str):
        tickers = [tickers]
    # if end_date is None:
    #     end_date = dt.datetime.today()

    def get_single_cached(ticker):
        load_fn = lambda start_date_x, end_date_x: get_uncached(ticker, start_date_x, end_date_x, **kwargs)

        ticker_filename = JPMDQ_EOD_CACHE_FOLDER + re.sub("[/\\\\:*]", "-", ticker) + ".pickle"

        if use_filecache:
            return data_fns.cached_get(load_fn, ticker_filename, start_date, end_date, not live,
                                       raise_on_missing_symbol, get_new_data, read_last_modified=read_last_modified,
                                       force_full_refresh=force_full_refresh)
        else:
            return load_fn(start_date, end_date)

    listdata = [get_single_cached(i_ticker) for i_ticker in tickers]
    dfdata = pd.concat(listdata, axis=1, sort=False)

    return dfdata


# def get_uncached(tickers, start_date=None, end_date=None, **kwargs):
#     """
#     :description: Get JPM dataquery data
#     :param str expr: dataquery expression
#     :param str sd: YYYYMMDD
#     :param str ed: YYYYMMDD
#     :param str user: jpmdq api credentials
#     :param str password: jpmdq api credentials
#     :param kwargs: cal, freq, conv, na
#     :return: dictionary with keys 'df' and 'errorMessage'
#     """
#     # tickers = tickers[:3]
#     if start_date is None or date_fns.to_datetime(start_date) < dt.datetime(1971, 1, 1):
#         start_date = dt.datetime(1971, 1, 1)
#
#     if end_date is None:
#         end_date = dt.datetime(2047, 12, 31)
#
#     start_date_str = date_fns.to_str(start_date, "yyyymmdd")
#     end_date_str = date_fns.to_str(end_date, "yyyymmdd")
#
#     drop_errors = kwargs.pop("drop_errors", True)
#     errors = pd.Series()
#
#     def __get_fn(ticker):
#         print("jpmdq get: {}".format(ticker))  # ticker = "induce_error"
#         # dat = jpmdq.fetch_df(ticker, start_date_str, end_date_str, JPMDQ_USER, JPMDQ_PW, **kwargs)
#         # data_out = (jpmdq.JpmdqClient(JPMDQ_USER, JPMDQ_PW)
#         #                  .fetch_df(ticker, start_date_str, end_date_str, **kwargs))
#         dat = jpmdqold.fetch_df(ticker, start_date_str, end_date_str, JPMDQ_USER, JPMDQ_PW, **kwargs)
#         data_out = dat['df']
#
#         if dat['err_raw'] is not None:
#             errors[ticker] = dat['err_raw']
#             return pd.DataFrame(columns=[ticker])
#         elif data_out is None:
#             return pd.DataFrame(columns=[ticker])
#         else:
#             if string_fns.is_number(data_out.index[0]):
#                 formatstring = '%Y%m%d'
#             elif data_out.index[0][4] == "-" and data_out.index[0][7] == "-":
#                 formatstring = '%Y-%m-%d'
#
#             data_out.index = [date_fns.from_str(s, formatstring) for s in data_out.index]
#
#             if drop_errors:
#                 data_out_ex_errors = data_out[data_out < 1e38].dropna()
#
#             data_cleaned = jpmdq_clean_data(ticker, data_out_ex_errors)
#
#             return data_cleaned
#
#     if isinstance(tickers, str):
#         data_out = __get_fn(tickers)
#     else:
#         dat = [__get_fn(i_ticker) for i_ticker in tickers]
#         data_out = pd.concat(dat, axis=1, join='outer')
#
#     if len(errors) > 0:
#         errors.to_csv("jpmdq_error_log.csv")
#         from cl_email import Email
#         old_width = pd.get_option('display.max_colwidth')
#         pd.set_option('display.max_colwidth', -1)
#         errors_html = pd.Series([str(v) for v in errors.values], index=errors.index).to_frame().to_html()
#         pd.set_option('display.max_colwidth', old_width)
#         Email(subject="errors while loading from JPM DataQuery", htmlbody=errors_html).send()
#
#     if kwargs.get('cut_weekends', True):
#         if len(data_out.index) > 0:
#             data_out = data_out[data_out.index.dayofweek < 5]
#     # dat_out.iloc[-1]
#
#
#     return data_out


def get_uncached(tickers, start_date=None, end_date=None, **kwargs):
    """
    :description: Get JPM dataquery data
    :param str expr: dataquery expression
    :param str sd: YYYYMMDD
    :param str ed: YYYYMMDD
    :param str user: jpmdq api credentials
    :param str password: jpmdq api credentials
    :param kwargs: cal, freq, conv, na
    :return: dictionary with keys 'df' and 'errorMessage'
    """
    # tickers = tickers[:3]
    if start_date is None or date_fns.to_datetime(start_date) < dt.datetime(1971, 1, 1):
        start_date = dt.datetime(1971, 1, 1)

    if end_date is None:
        end_date = dt.datetime(2047, 12, 31)

    start_date_str = date_fns.to_str(start_date, "yyyymmdd")
    end_date_str = date_fns.to_str(end_date, "yyyymmdd")

    def __get_fn(ticker):
        print("jpmdq get: {}".format(ticker))  # ticker = "induce_error"

        data_out = (jpmdq.JpmdqClient(JPMDQ_USER, JPMDQ_PW)
                         .fetch_df(ticker, start_date_str, end_date_str, **kwargs))

        data_out.index = [date_fns.to_datetime(x) for x in data_out.index]

        data_cleaned = jpmdq_clean_data(ticker, data_out)

        return data_cleaned

    if isinstance(tickers, str):
        data_out = __get_fn(tickers)
    else:
        dat = [__get_fn(i_ticker) for i_ticker in tickers]
        data_out = pd.concat(dat, axis=1, join='outer')

    if kwargs.get('cut_weekends', True):
        if len(data_out.index) > 0:
            data_out = data_out[data_out.index.dayofweek < 5]

    return data_out


def jpmdq_clean_data(ticker, data):

    # USD
    if ticker.lower()[:23] == 'DB(CCV,CRVSWAPMMKT,usd,'.lower():
        data = data.drop([dt.datetime(1994, 4, 8), dt.datetime(1991, 7, 11)], errors='ignore')

    # GBP
    if ticker.lower()[:23] == 'DB(CCV,CRVSWAPMMKT,gbp,'.lower():
        data = data.drop([dt.datetime(1988, 4, 11), dt.datetime(1988, 11, 14)], errors='ignore')

    # AUD
    if ticker.lower()[:25] == 'DB(CCV,CRVSWAPMMKT_6,aud,'.lower():
        data = data.drop([dt.datetime(1991, 7, 10), dt.datetime(1991, 7, 11)], errors='ignore')

    # CAD
    if ticker.lower()[:27] == 'DB(CCV,CRVSWAPMMKT,cad,10Y,'.lower():
        data = data.drop([dt.datetime(1993, 5, 3), dt.datetime(1993, 12, 28)], errors='ignore')

    if ticker.lower()[:27] == 'DB(CCV,CRVSWAPMMKT,cad,30Y,'.lower():
        data = data.drop([dt.datetime(1992, 12, 24),
                          dt.datetime(1993, 1, 12),
                          dt.datetime(1993, 5, 3),
                          dt.datetime(1993, 8, 4),
                          dt.datetime(1993, 8, 6),
                          dt.datetime(1993, 8, 11),
                          dt.datetime(1993, 8, 19),
                          dt.datetime(1993, 8, 27),
                          dt.datetime(1993, 8, 30),
                          dt.datetime(1993, 9, 9),
                          dt.datetime(1993, 12, 28)
                          ], errors='ignore')

    if ticker.lower()[:27] == 'DB(CCV,CRVSWAPMMKT,cad,10Y,'.lower() and \
            ticker.lower()[-11:] == 'AM_MOD_DUR)'.lower():
        data = data.drop([dt.datetime(1993, 5, 3), dt.datetime(1993, 12, 28),
                          dt.datetime(1995, 4, 17), dt.datetime(1995, 5, 8),
                          dt.datetime(1995, 8, 28), dt.datetime(1996, 4, 8),
                          dt.datetime(1996, 5, 6)], errors='ignore')

    # NOK
    if ticker.lower()[:23] == 'DB(CCV,CRVSWAPMMKT,nok,'.lower():
        data = data[dt.datetime(1999, 6, 4):]

    # NZD
    if ticker.lower()[:25] == 'DB(CCV,CRVSWAPMMKT_3,nzd,'.lower():
        data = data[dt.datetime(1996, 10, 21):]

    # SEK
    if ticker.lower()[:23] == 'DB(CCV,CRVSWAPMMKT,sek,'.lower():
        data = data.drop([dt.datetime(1991, 9, 23), ], errors='ignore')

    return data


def get_jpm_swap_base(ccy):
    # use jpmdq tickers
    if ccy.lower() == "aud":
        base_jpmdq_ticker = "DB(CCV,CRVSWAPMMKT_6,{},{},{},{})"
    elif ccy.lower() == "nzd":
        base_jpmdq_ticker = "DB(CCV,CRVSWAPMMKT_3,{},{},{},{})"
    else:
        base_jpmdq_ticker = "DB(CCV,CRVSWAPMMKT,{},{},{},{})"

    return base_jpmdq_ticker.lower()


def tck_cms_curve_vol(ccy, tenor_a, tenor_b, expiry):
    base = {'usd': 'FDER,YCSO', 'eur': 'COV,YCSO,eur'}[ccy]
    return f'DB({base},{tenor_a}x{tenor_b},{expiry},S,ATMF,0,BPVOL)*SQRT(252)'


def tck_swaption_vol(ccy, tenor, expiry):
    if ccy.lower() == 'usd':
        ticker = f'DB(FDER,SWAPTION,{tenor.zfill(3)},{expiry.zfill(3)},3PT,Receiver,ATMF,0,BPVOL)*SQRT(252)'
    else:
        ticker = f'DB(COV,VOLSWAPTION,{ccy},{tenor.zfill(3)},{expiry.zfill(3)},PAYER,VOLBPVOL)'
    return ticker


def tck_swap_rate(ccy, tenor, expiry, metric='RT_MID'):
    return f'DB(CCV,CRVSWAPMMKT,{ccy},{tenor},{expiry},{metric})'


if __name__ == "__main__":

    import lib_ticker_fns as tck
    tenors= ['10y', '20y', '30y']
    tickers = [tck.jpm_swap_rate('eur', x) for x in tenors]

    data_out = (jpmdq.JpmdqClient(JPMDQ_USER, JPMDQ_PW)
                .fetch_df(tickers, dt.datetime(1980, 1, 1), dt.datetime(2049, 6, 14)))

    data = data_out.dropna(axis=0, how='all')

    data.columns = tenors

    fly10s20s30s = data['20y'] - data['10y'] / 2 - data['30y'] / 2
    crv20s30s = data['20y'] - data['30y']
    fly10s20s30s[dt.date(2000, 1, 1):].plot()
    crv20s30s[dt.date(2000, 1, 1):].plot()

    fly10s20s30s[dt.date(2000, 1, 1):].plot()

    base_eur = "DB(CCV,CRVSWAPMMKT_3,EUR,03M,{},RT_MID)"
    base_jpy = "DB(CCV,CRVSWAPMMKT_3,JPY,03M,{},RT_MID)"

    tenors = ["", "01M", "03M", "06M", "09M", "01Y", "02Y", "03Y", "04Y", "05Y", "06Y", "07Y", "08Y", "09Y", "10Y", "15Y", "20Y"]
    tenors = ["", "01M"]
    tickers_eur = [base_eur.format(s) for s in tenors]
    tickers_jpy = [base_eur.format(s) for s in tenors]


    start_date = dt.datetime(1971, 1, 1)
    end_date = dt.datetime.today()

    print("loading eur")
    dat_eur = get(tickers_eur, start_date, end_date)
    print("loading jpy")
    dat_jpy = get(tickers_jpy, start_date, end_date)

    s = get("DB(CCV,CRVSWAPMMKT,USD,10Y,,AM_MOD_DUR)", use_filecache=False)

    dat = get(source_tickers, start_date, end_date)
    print(dat_eur)



 








pvk data_opendata.py





import panormus.data.open_data as opendata


def load_uncached(source_tickers, start_date, end_date):
    data = opendata.df_for_observable_strings(source_tickers, start_date, end_date)
    data.columns = ['|'.join(x) for x in data.columns]
    data.index = [i_dt.replace(tzinfo=None) for i_dt in data.index]
    return data


if __name__ == "__main__":
    import lib_ticker_fns as tck
    ccy = 'usd'
    tenor = '10y'
    metric = 'par'
    expiries_str = ['', '1m', '3m']
    start_date = None
    end_date = None
    source_tickers = [tck.od_swap_rate(ccy, tenor, i_exp, metric) for i_exp in expiries_str]

 






pvk_color_fns.py



from colour import Color


def name2hex(name):
    return Color(name).hex


def housecolor(color=None):
    colors = dict(
        dblu=( 23/255,  54/255,  93/255),
        mred=(192/255,   0/255,   0/255),
        grey=(127/255, 127/255, 127/255),
        lblu=(141/255, 179/255, 226/255),
        pink=(229/255, 185/255, 183/255),
        purp=( 95/255,  73/255, 122/255),
        blck=(  0/255,   0/255,   0/255),
        mrsh=(148/255, 138/255,  84/255),
        mblu=( 66/255, 130/255, 208/255),
        lred=(188/255, 105/255, 105/255),
        lgry=(167/255, 167/255, 167/255),
        bdux=(152/255,  60/255,  54/255),
        lprp=(158/255, 137/255, 184/255),

        dgry=(105/255, 105/255, 105/255),
        llgr=(208/255, 208/255, 208/255),
    )

    if color is None:
        return colors
    else:
        return colors[color]

 






pvk data_fns.py





import pandas as pd
import numpy as np
import datetime as dt
import os.path
import hashlib
import os
import shutil
import json
import pathlib

# utils_pvk imports
import utils_pvk.lib_date_fns as date_fns

PATH_LIVECACHE = "C:\\cache\\livecache\\"
PATH_FN_CACHE = "C:\\cache\\fn_cache\\"


def h5store(filename, df, **kwargs):
    store = pd.HDFStore(filename)
    store.put('mydata', df)
    store.get_storer('mydata').attrs.metadata = kwargs
    store.close()


def h5load(filename):
    store = pd.HDFStore(filename)
    data = store['mydata']
    metadata = store.get_storer('mydata').attrs.metadata
    store.close()
    return data, metadata


def cached_get(get_data_fn, full_path, start_date, end_date=dt.datetime.today(),
               exclude_data_asof_today=True, raise_on_missing_symbol=False, get_new_data=True, write=True,
               read_last_modified=True, force_full_refresh=False, apply_cutoff=True, **_):
    """
    reads output from load_fn, but caches to disk such that data is only loaded once

    load_fn should take only 2 parameters: start_date and end_date
    """

    if end_date is None:
        end_date = dt.datetime.today()

    if start_date is None:
        start_date = dt.datetime(1900, 1, 1)

    if isinstance(end_date, dt.date):
        end_date = dt.datetime(end_date.year, end_date.month, end_date.day, 23, 59, 59)

    # step 1 - load existing data and compare
    if exclude_data_asof_today and end_date >= pd.to_datetime(dt.date.today() - pd.tseries.offsets.BDay(1)):
        yesterday = dt.date.today() - pd.tseries.offsets.BDay(1)
        end_date = dt.datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59)

    path, file = os.path.split(full_path)
    filename, file_extension = os.path.splitext(file)
    if file.lower() not in os.listdir(path) and os.path.isfile(full_path):
        os.rename(full_path, path + "\\" + filename + "_renamed_not_lcase" + file_extension)

    if force_full_refresh and os.path.isfile(full_path):
        os.remove(full_path)

    if os.path.isfile(full_path):
        current = pd.read_pickle(full_path)
        file_exists = True

        if 'no_data' in current.columns:
            current.drop('no_data', axis=1, inplace=True)

        current.columns = [s.lower() for s in current.columns]

        if pd.read_pickle(full_path).empty or isinstance(current.index[0], dt.date):
            is_daily = True
        else:
            hours = np.all(np.array([t.hour for t in current.index[-1000:]]) == 0)
            mins = np.all(np.array([t.minute for t in current.index[-1000:]]) == 0)
            secs = np.all(np.array([t.second for t in current.index[-1000:]]) == 0)
            is_daily = hours and mins and secs

        if read_last_modified and exclude_data_asof_today:
            start_date_newdata = dt.datetime.fromtimestamp(os.path.getmtime(full_path))
        else:
            if pd.read_pickle(full_path).empty:
                start_date_newdata = dt.date(1900, 1, 1)
            elif type(current.index[-1]) in (dt.date, ) or is_daily:
                start_date_newdata = current.index[-1] + pd.tseries.offsets.BDay(1)
            elif type(current.index[-1]) in (dt.datetime, pd._libs.tslibs.timestamps.Timestamp):
                start_date_newdata = current.index[-1] + dt.timedelta(seconds=60)
            else:
                raise ValueError("Index var type: {} not supported at this time".format(type(current.index[-1])))

        if pd.read_pickle(full_path).empty:
            pass
    else:
        file_exists = False
        start_date_newdata = dt.date(1900, 1, 1)

    # step 2 - get new data, if necessary
    searched_for_new_data = False
    if pd.to_datetime(start_date_newdata) <= end_date and get_new_data:
        try:
            newdata = get_data_fn(start_date_newdata, end_date)

            newdata.columns = [s.lower() for s in newdata.columns]

            idx_datetime = np.array([date_fns.to_datetime(i_d) for i_d in newdata.index])

            newdata = newdata.iloc[idx_datetime >= date_fns.to_datetime(start_date_newdata)]
            searched_for_new_data = True
            if len(newdata.index) == 0:
                updated = False
            else:
                updated = True
        except ValueError as e:
            if str(e) == "Length mismatch: Expected axis has 0 elements, new values have 4 elements":
                updated = False
            else:
                raise
    else:
        updated = False

    if file_exists and updated and not current.empty:
        cur_s = current.iloc[:, 0]
        new_s = newdata.loc[cur_s.index[-1]:, :].iloc[:, 0].drop(cur_s.index[-1], errors='ignore')
        output = pd.concat([cur_s, new_s], axis=0).to_frame()

    elif updated and (not file_exists or current.empty):
        output = newdata
    elif file_exists and not updated:
        output = current
    else:
        if raise_on_missing_symbol:
            raise ValueError("Data non-existent in database or bbg".format(ticker, field,
                         date_fns.to_str(start_date), date_fns.to_str(end_date)))
        else:
            output = pd.DataFrame(columns=['no_data'])

    output = output[~output.index.duplicated(keep='first')]
    output.sort_index(inplace=True)

    # store to HDF, never store today's mark to file as (for bbg) it can be a live mark, not (necessarily) a close mark
    if write:
        if updated:
            if isinstance(output.index[0], pd.datetime):
                if apply_cutoff:
                    cutoff_tmp = (pd.datetime.today() - pd.tseries.offsets.BDay(1))
                    last_date_store = dt.datetime(cutoff_tmp.year, cutoff_tmp.month, cutoff_tmp.day, 23, 59, 59)
                    storedf = output.loc[:last_date_store, :]#
                else:
                    storedf = output.copy()
                output = output.loc[start_date:]
            elif isinstance(output.index[0], dt.date):
                first_date_output = dt.date(start_date.year, start_date.month, start_date.day)
                if apply_cutoff:
                    last_date_store = (dt.date.today() - pd.tseries.offsets.BDay(1))
                    cutoff = dt.date(last_date_store.year, last_date_store.month, last_date_store.day)
                    storedf = output.loc[:cutoff, :]
                else:
                    storedf = output.copy()
                output = output.loc[first_date_output:]
            else:
                raise ValueError("Timeseries 'output' index type: '{}' not supported at this time.".
                                 format(type(output.index[0])))
            storedf.to_pickle(full_path.lower())
        else:
            # update last written date:
            if searched_for_new_data:
                if file_exists:
                    pd.read_pickle(full_path.lower()).to_pickle(full_path.lower())
                else:
                    pd.DataFrame().to_pickle(full_path.lower())

            if output.empty and len(output.columns) == 0:
                return pd.DataFrame(columns=['no_data'])
            if not output.empty:
                if isinstance(output.index[0], pd.datetime):
                    start_date = date_fns.to_datetime(start_date)
                    end_date = date_fns.to_datetime(end_date)
                elif isinstance(output.index[0], dt.date):
                    start_date = date_fns.to_date(start_date)
                    end_date = date_fns.to_date(end_date)
                else:
                    raise ValueError("Timeseries 'output' index type: '{}' not supported at this time.".
                                     format(type(output.index[0])))
                output = output.loc[start_date:end_date]
    return output


def clean_temp_folders(folders=[PATH_LIVECACHE, ]):
    print("cleaning temporary folders")

    for folder in folders:
        for the_file in os.listdir(folder):
            file_path = os.path.join(folder, the_file)
            try:
                if os.path.isfile(file_path):
                    os.unlink(file_path)
            except Exception as e:
                print(e)


def naive_livecache_get(get_data_fn, timeout_secs=10*60, force_reload=False, unique_string=""):
    """naive live cache of a function
    function output needs to be h5 store-able
    it will only load the function if previous run of function is older than timeout"""

    full_path = PATH_LIVECACHE + "\\livecache_hash_" + str(hash(get_data_fn.__code__)) + unique_string

    if not force_reload and os.path.isfile(full_path):
        last_updated = dt.datetime.fromtimestamp(os.path.getmtime(full_path))
        age = (dt.datetime.today() - last_updated).seconds

        if age < timeout_secs:
            # load existing data
            data, _ = h5load(full_path)
            return data

    # otherwise, load data
    data = get_data_fn()
    h5store(full_path, data)
    return data


def param_livecache_get(get_data_fn, params, timeout_secs=10*60, force_reload=False):
    """naive live cache of a function, it will only load the function"""

    paramsx = params.copy()
    if isinstance(paramsx, list):
        for i in range(len(paramsx)):
            elem = paramsx[i]
            if date_fns.is_date(elem):
                paramsx[i] = date_fns.to_str(date_fns.to_date(elem))

    strhash = hashlib.sha1(json.dumps(paramsx, sort_keys=True).encode('utf-8')).hexdigest()
    full_path = PATH_LIVECACHE + "\\livecache_hash_" + strhash

    if not force_reload and os.path.isfile(full_path):
        last_updated = dt.datetime.fromtimestamp(os.path.getmtime(full_path))
        age = (dt.datetime.today() - last_updated).seconds

        if age < timeout_secs:
            # load existing data
            data, _ = h5load(full_path)
            return data

    # otherwise, load data
    data = get_data_fn()
    h5store(full_path, data)
    return data


def cached_fn_get_naive(fn, fn_args=[], fn_kwargs={}, timeout_secs=24*3600, force_reload=False):
    """
    Cached get of a function.
    :description: Cache function output. Function should always return the same output for the set of parameters.
    Function output must be pickle serializable.

    :param fn: function to be loaded
    :param fn_args: function positional arguments
    :param fn_kwargs: function named arguments
    :param timeout_secs: cache timeout in seconds (will reload if this time is exceeded)
    :param force_reload: force refresh of the cached output

    :return: function output

    """

    # make sure start and end date are date type (i.e. do not contain seconds etc.)
    argsx = fn_args.copy()
    if isinstance(argsx, list):
        for i in range(len(argsx)):
            elem = argsx[i]
            if date_fns.is_date(elem):
                argsx[i] = date_fns.to_str(date_fns.to_date(elem))

    kwargsx = fn_kwargs.copy()
    if isinstance(kwargsx, dict):
        for k in kwargsx.keys():
            elem = kwargsx[k]
            if date_fns.is_date(elem):
                kwargsx[k] = date_fns.to_str(date_fns.to_date(elem))

    params = {'args': argsx, 'kwargs': kwargsx}

    strhash = hashlib.sha1(json.dumps(params, sort_keys=True).encode('utf-8')).hexdigest()
    full_path = PATH_FN_CACHE + "\\livecache_hash_" + strhash

    pathlib.Path(PATH_FN_CACHE).mkdir(parents=True, exist_ok=True)

    # If cache file exists, load and return it
    if not force_reload and os.path.isfile(full_path):
        last_updated = dt.datetime.fromtimestamp(os.path.getmtime(full_path))
        age = (dt.datetime.today() - last_updated).seconds

        if age < timeout_secs:
            # load existing data
            data, _ = h5load(full_path)
            return data

    # Otherwise, run the function, store the output, and return it
    data = fn(*fn_args, **fn_kwargs)
    h5store(full_path, data)
    return data


if __name__ == "__main__":
    import datetime as dt
    args = ["SPX Index", "PX_LAST", dt.date(2018, 1, 1), dt.date.today()]

    import data_bloomberg

    from panormus.data.bbg import BbgClient

    ticker = "SPX Index"
    field = "PX_LAST"
    start_date = dt.datetime(2018, 1, 1)
    end_date = dt.datetime.today()
    x = BbgClient().get_historical_data([ticker], [field], start_date, end_date)

    x.pivot(index='date', columns='ticker', values='PX_LAST')
    data_bloomberg.__bdh(ticker, field, start_date, end_date)


    def load_bbg(ticker, field, start_date, end_date):
        return BbgClient().get_historical_data([ticker], [field], start_date, end_date)


    start = dt.datetime.today()
    xz = load_bbg(ticker, field, dt.date(2018, 1, 2), end_date)
    print(dt.datetime.today() - start)
    cached_fn_get_naive(data_bloomberg.__bdh, [ticker, field, start_date, end_date])


 




pvk date_fns.py



import datetime as dt
import numpy as np
import pandas as pd
from dateutil.relativedelta import relativedelta
import traceback
import sys

import QuantLib as ql


DATE_FORMATS = {
    # Excel date time formats
    'dd/mm/yy':          lambda d: d.strftime('%d/%m/%y'),
    'mm/dd/yy':          lambda d: d.strftime('%m/%d/%y'),
    'dd m yy':           lambda d: "{} {} {}".format(d.strftime('%d', d.month), d.strftime('%y')),
    'd mm yy':           lambda d: "{} {} {}".format(d.day, d.strftime('%m'), d.strftime('%y')),
    'd mmm yy':          lambda d: "{} {} {}".format(d.day, d.strftime('%b'), d.strftime('%y')),
    'd mmm yyyy':        lambda d: "{} {} {}".format(d.day, d.strftime('%b'), d.strftime('%Y')),
    'd mmmm yy':         lambda d: "{} {} {}".format(d.day, d.strftime('%B'), d.strftime('%y')),
    'd mmmm yyy':        lambda d: "{} {} {}".format(d.day, d.strftime('%B'), d.strftime('%Y')),
    'd mmmm yyyy':       lambda d: "{} {} {}".format(d.day, d.strftime('%B'), d.strftime('%Y')),
    'dd/mm/yy hh:mm':    lambda d: d.strftime('%d/%m/%y %H:%M:%S'),
    'dd/mm/yy hh:mm:ss': lambda d: d.strftime('%d/%m/%y %H:%M:%S'),
    'dd-mmm-yy hh:mm':   lambda d: d.strftime('%d-%b-%y %H:%M'),
    'dd-mmm-yyyy hh:mm': lambda d: d.strftime('%d-%b-%Y %H:%M'),
    'dd-mm-yy_hh-mm':    lambda d: d.strftime('%d-%m-%Y_%H-%M'),
    'yyyy-mm-dd_hh-mm':  lambda d: d.strftime('%Y-%m-%d_%H-%M'),
    'yy-mm-dd_hh-mm':    lambda d: d.strftime('%y-%m-%d_%H-%M'),
    'yyyymmddHhhMmm':    lambda d: d.strftime('%Y%m%dH%HM%M'),
    'hh:mm':             lambda d: d.strftime('%H:%M'),
    'hh:mm:ss':          lambda d: d.strftime('%H:%M:%S'),
    'hh:mm:ss.000':      lambda d: d.strftime('%H:%M:%S'),
    'yyyymmdd':          lambda d: d.strftime('%Y%m%d'),
    'yyyy-mm-dd':        lambda d: d.strftime('%Y-%m-%d'),
    'dd-mmm-yy':         lambda d: d.strftime('%d-%b-%y'),
    'dd-mmm-yyyy':       lambda d: d.strftime('%d-%b-%Y'),
    # Custom names
    'utc':               lambda d: d.strftime('%Y-%m-%d'),
    'bbg':               lambda d: d.strftime('%Y%m%d'),
    'cax':               lambda d: d.strftime('%d/%m/%Y'),
    }

DATE_FORMATS_READ = {
    # Excel date time formats
    'dd/mm/yy':             '%d/%m/%y',
    'mm/dd/yy':             '%m/%d/%y',
    'mm/dd/yyyy':           '%m/%d/%Y',
    'dd/mm/yy hh:mm':       '%d/%m/%y %H:%M:%S',
    'dd/mm/yy hh:mm:ss':    '%d/%m/%y %H:%M:%S',
    'dd-mm-yy hh:mm:ss':    '%d-%m-%y %H:%M:%S',
    'yyyy-mm-dd hh:mm:ss':  '%Y-%m-%d %H:%M:%S',

    'hh:mm':                '%H:%M:%S',
    'hh:mm:ss':             '%H:%M:%S',
    'hh:mm:ss.000':         '%H:%M:%S',
    'yyyymmdd':             '%Y%m%d',
    'yyyy-mm-dd':           '%Y-%m-%d',
    'dd-mm-yy':             '%d-%m-%y',
    'yy-mm-dd':             '%d-%m-%y',

    # Custom names
    'utc':                  '%Y-%m-%d',
    'bbg':                  '%Y%m%d',
    'cax':                  '%d/%m/%Y',
    }


def to_str(date, format="yyyy-mm-dd"):
    if date is None:
        return None

    formatfn = DATE_FORMATS.get(format, lambda d: d.strftime(format))

    if isinstance(date, dt.date):
        if pd.isnull(date):
            return "-"
        else:
            return formatfn(date)

    elif isinstance(date, dt.datetime):
        raise ValueError("Passed a date.datetime object, this object is reserved for intra-day data, " +
                         "which is not yet supported")
    elif isinstance(date, str):
        return date
    elif pd.isnull(date):
        return "-"
    else:
        raise ValueError("Input of type \'{}\' not supported at this moment.".format(type(date)))


def from_str(date, format='%Y%m%d', raiseonerror=False):
    # format = ["%m-%d-%Y", "%d/%m/%Y"]
    def convert(date, format):
        try:
            format = DATE_FORMATS_READ.get(format, format)
            # format = "%m-%d-%Y"
            return dt.datetime.strptime(str(date), format)
        except ValueError:
            return sys.exc_info()

    if isinstance(format, str):
        output = convert(date, format)
    else:
        for f in format:
            output = convert(date, f)
            if isinstance(output, dt.datetime):
                break

    if raiseonerror and not isinstance(output, dt.datetime):
        raise output[1]
    elif isinstance(output, dt.datetime):
        return output
    else:
        return None


def to_date(date):
    if date is None:
        return date
    else:
        return dt.date(date.year, date.month, date.day)


def to_datetime(date):
    """Convert a date to datetime.datetime"""
    if isinstance(date, dt.datetime):
        return date
    elif isinstance(date, ql.Date):
        return dt.datetime(date.year(), date.month(), date.dayOfMonth())
    else:
        return dt.datetime(date.year, date.month, date.day)


def is_date(date):
    if isinstance(date, dt.datetime):
        return True
    elif isinstance(date, ql.Date):
        return True
    elif hasattr(date, 'year'):
        return True
    else:
        return False


def to_qldate(date):
    """Convert a date to ql.Date"""
    if isinstance(date, ql.Date):
        return date
    else:
        return ql.Date(date.day, date.month, date.year)


def to_relativedelta(str_rd):
    """convert string to relativedelta
    input e.g. 3m, 5y, 12w
    """

    if str_rd[-1] == "m":
        return relativedelta(months=int(str_rd[:-1]))
    elif str_rd[-1] == "y":
        return relativedelta(years=int(str_rd[:-1]))
    elif str_rd[-1] == "w":
        return relativedelta(weeks=int(str_rd[:-1]))


def get_calendar(start_date, end_date, interval, only_weekdays=True):
    """
    return a list of dates between start and end date
    :param start_date:
    :param end_date:
    :param interval: either dt.timedelta or "1b", "4m" etc.
    :return: list of dates
    """
    if isinstance(interval, dt.timedelta):
        step = interval
    else:
        if interval[-1] == "b":
            step = dt.timedelta(days=int(interval[:-1]))
            only_weekdays = True
        elif interval[-1] == "m":
            # step = dt.timedelta(minutes=int(interval[:-1]))
            step = relativedelta(months=int(interval[:-1]))
        elif interval[-1] == "w":
            step = relativedelta(weeks=int(interval[:-1]))
        else:
            raise ValueError("Interval: \'{}\' not recognized.".format(interval))

    dates_out = []
    i_date = start_date
    while i_date <= end_date:
        if not only_weekdays or i_date.weekday() < 5:
            dates_out.append(i_date)
        i_date += step

    return dates_out


def next_imm(i_dt):
    ql_dt = to_qldate(i_dt)
    return ql.IMM.nextDate(ql_dt)


def advance_calendar(date, period, calendar, convention='fol'):
    """advance the day according to a calendar"""

    if period == '':
        return date

    # get the QuantLib date objects
    calendar = {'us': ql.UnitedStates(),
                'eu': ql.TARGET()}[calendar]
    qlperiod = ql.Period(int(period[:-1]), {'b': ql.Days,
                                            'w': ql.Weeks,
                                            'm': ql.Months,
                                            'y': ql.Years}[period[-1]])
    convention = {'modfol': ql.ModifiedFollowing,
                  'fol': ql.Following,
                  'pre': ql.Preceding,
                  'modpre': ql.ModifiedPreceding,
                  'unadj': ql.Unadjusted
                  }[convention]

    # convert the date
    # date_ql_in = ql.Date(date.day, date.month, date.year)
    date_ql_out = calendar.advance(to_qldate(date), qlperiod, convention)

    # convert back to the original type
    date_out = type(date)(date_ql_out.year(), date_ql_out.month(), date_ql_out.dayOfMonth())

    return date_out


def period_to_yearfrac_approx(per):
    """approximate string to period conversion"""
    if per == '':
        return 0
    letter = per[-1]
    counter = int(per[:-1])
    fractions = {'b': 252, 'd': 365, 'w': 52, 'm': 12, 'y': 1}
    return counter / fractions[letter]

 






pvk html_fns.py


def create_html_table(dat_format, include_index=True, index_width=None, cell_width=None, borders='bottom', ulines=[]):
    """create simple html table for in email"""
    html = "<table style=\"font-family: Calibri; font-size: 10pt;\">"
    html += "<tr>"
    # html += "<tr >"
    # html = "<table><tr>"

    if borders == 'bottom':
        b_b = "  style=\"border-bottom:1px solid #000000\""
        b_r = ""
        b_rb = b_b
    elif borders == 'rightbottom':
        b_b = "  style=\"border-bottom:1px solid #000000\""
        b_r = "  style=\"border-right:1px solid #000000\""
        b_rb = "  style=\"border-right:1px solid #000000; border-bottom:1px solid #000000\""

    if include_index:
        html += "<th " + b_rb + "></th>"

    for c in dat_format.columns:
        html += "<th align=\"center\"  {}>{}</th>".format(b_b, c)
    html += "</tr>"

    for i in range(len(dat_format.index)):
        cell_opt = "align=\"center\""
        if cell_width:
            cell_opt += "  width=\"{}\"".format(cell_width)

        index_opt = "align=\"left\""
        # index_opt = "align=\"left\"" + b_r
        if index_width:
            index_opt += "  width=\"{}\"".format(index_width)

        if i in ulines:
            index_opt += b_rb
            cell_opt += b_b
        else:
            index_opt += b_r

        html += "<tr>"
        if include_index:
            html += "<th {}>{}</th>".format(index_opt, dat_format.index[i])
        for x in dat_format.iloc[i, :].values:
            html += "<td {}>{}</td>".format(cell_opt, x)
        html += "</tr>"

    html += "</table>"
    return html

 




pvk label_lines.py





from math import atan2, degrees
import numpy as np
import pandas as pd

from matplotlib.dates import date2num
from datetime import datetime


# Label line with line2D label data
def labelLine(line, x, label=None, align=True, **kwargs):
    '''Label a single matplotlib line at position x

    Parameters
    ----------
    line : matplotlib.lines.Line
       The line holding the label
    x : number
       The location in data unit of the label
    label : string, optional
       The label to set. This is inferred from the line by default
    kwargs : dict, optional
       Optional arguments passed to ax.text
    '''
    ax = line.axes
    xdata = line.get_xdata()
    ydata = line.get_ydata()

    if isinstance(ydata, np.ma.core.MaskedArray):
        ydata = pd.Series(ydata).fillna(method='ffill').fillna(method='bfill').values

    # Convert datetime objects to floats
    # if isinstance(x, datetime):
    try:
        x = date2num(x)
    except AttributeError:
        None

    try:
        x = date2num(x.to_timestamp(how="E"))
    except AttributeError:
        None

    # if isinstance(xdata[0], datetime):
    try:
        xdata = date2num(xdata)
    except AttributeError:
        None
    period = False
    try:
        xdata = date2num([x.to_timestamp(how="E") for x in xdata])
        period = True
    except AttributeError:
        None

    if (x < xdata[0]) or (x > xdata[-1]):
        raise Exception('x label location is outside data range!')

    # Find corresponding y co-ordinate and angle of the
    ip = 1
    for i in range(len(xdata)):
        if x < xdata[i]:
            ip = i
            break

    y = ydata[ip-1] + (ydata[ip]-ydata[ip-1]) * \
        (x-xdata[ip-1])/(xdata[ip]-xdata[ip-1])

    if not label:
        label = line.get_label()

    if align:
        # Compute the slope
        dx = xdata[ip] - xdata[ip-1]
        dy = ydata[ip] - ydata[ip-1]
        ang = degrees(atan2(dy, dx))

        # Transform to screen co-ordinates
        pt = np.array([x, y]).reshape((1, 2))
        trans_angle = ax.transData.transform_angles(np.array((ang, )), pt)[0]

    else:
        trans_angle = 0

    # Set a bunch of keyword arguments
    if 'color' not in kwargs:
        kwargs['color'] = line.get_color()

    if ('horizontalalignment' not in kwargs) and ('ha' not in kwargs):
        kwargs['ha'] = 'center'

    if ('verticalalignment' not in kwargs) and ('va' not in kwargs):
        kwargs['va'] = 'center'

    if 'backgroundcolor' not in kwargs:
        kwargs['backgroundcolor'] = ax.get_facecolor()

    if 'clip_on' not in kwargs:
        kwargs['clip_on'] = True

    if 'zorder' not in kwargs:
        kwargs['zorder'] = 2.5

    if 'bbox' not in kwargs:
        kwargs['bbox'] = dict(alpha=0.65, pad=-0.1, facecolor='white', edgecolor='white', fill=True)
    else:
        kwargs['bbox']['alpha']     = kwargs['bbox'].get('alpha', 0.65)
        kwargs['bbox']['pad']       = kwargs['bbox'].get('pad', -0.1)
        kwargs['bbox']['facecolor'] = kwargs['bbox'].get('facecolor', 'white')
        kwargs['bbox']['edgecolor'] = kwargs['bbox'].get('edgecolor', 'white')
        kwargs['bbox']['fill']      = kwargs['bbox'].get('fill', True)

    if period:
        xdata = np.array(xdata)
        if np.min(xdata) >= x:
            idx = xdata == xdata[xdata <= x][-1]
        else:
            idx = xdata == xdata[xdata >= x][0]
        x = line.get_xdata()[idx]

    ax.text(x, y, label, rotation=trans_angle, **kwargs)


def labelLines(lines, align=True, xvals=None, **kwargs):
    '''Label all lines with their respective legends.

    Parameters
    ----------
    lines : list of matplotlib lines
       The lines to label
    align : boolean, optional
       If True, the label will be aligned with the slope of the line
       at the location of the label. If False, they will be horizontal.
    xvals : (xfirst, xlast) or array of float, optional
       The location of the labels. If a tuple, the labels will be
       evenly spaced between xfirst and xlast (in the axis units).
    kwargs : dict, optional
       Optional arguments passed to ax.text
    '''
    ax = lines[0].axes
    labLines = []
    labels = []

    # Take only the lines which have labels other than the default ones
    for line in lines:
        label = line.get_label()
        if "_line" not in label:
            labLines.append(line)
            labels.append(label)

    if xvals is None:
        x = line.get_xdata()

        # Convert datetime objects to floats
        converted = False
        try:
            x = date2num(x)
            xvals = (x[0], x[-1])
            converted = True
        except AttributeError:
            None

        try:
            x1 = date2num(x[0].to_timestamp(how="E"))
            x2 = date2num(x[-1].to_timestamp(how="E"))
            xvals = (x1, x2)
            converted = True
        except AttributeError:
            None

        if not converted:
            xvals = ax.get_xlim() # set axis limits as annotation limits, xvals now a tuple

    else:
        try:
            xvals = date2num(xvals)
        except AttributeError:
            None

    if type(xvals) == tuple:
        xmin, xmax = xvals
        xscale = ax.get_xscale()
        if xscale == "log":
            xvals = np.logspace(np.log10(xmin), np.log10(xmax), len(labLines)+2)[1:-1]
        else:
            xvals = np.linspace(xmin, xmax, len(labLines)+2)[1:-1]

    for line, x, label in zip(labLines, xvals, labels):
        labelLine(line, x, label, align, **kwargs)

 






pvk plot_fns.py



import matplotlib.lines as mlines

from matplotlib.dates import date2num

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import itertools
import matplotlib.dates as mdates

from utils_pvk.lib_labellines import labelLine, labelLines

import utils_pvk.lib_report_fns as report_fns
import utils_pvk.lib_color_fns as color_fns


def plt_intraday(plot_data, title):
    return plt_linelabels(plot_data, title, intraday=True)


def plt_linelabels(plot_data, title, intraday=False, return_filepath=True, **options):

    if len(plot_data.columns) == 0:
        return None

    # logic for adding lines (using the labelLines package)
    label_zorder = options.pop('label_zorder', 100)
    if 'zorder' in options.keys():
        raise ValueError("Please rename argument zorder to label_zorder")
    align = options.pop('align', False)
    xvals = options.pop('xvals', None)
    edge = options.pop('edge', None)
    label_offset = options.pop('label_offset', 0.03)
    colors = options.pop('colors', None)
    title_color = options.pop('title_color', None)
    rhs_axis = options.pop('rhs_axis', False)
    use_grid = options.pop('use_grid', False)
    outlier_cutoffs = options.pop('outlier_cutoffs', None)
    figsize = options.pop('figsize', None)
    plot_close_after_use = options.pop('plot_close_after_use', True)
    use_house_colors = options.pop('use_house_colors', True)
    drop_all_na = options.pop('drop_all_na', False)
    legend = options.pop('legend', None)
    use_overlabels = options.pop('use_overlabels', True)
    override_path = options.pop('override_path', None)
    cross_out_plot = options.pop('cross_out_plot', False)
    ylabel = options.pop('ylabel', None)
    xlabel = options.pop('xlabel', None)
    line_zorders = options.pop('line_zorders', None)
    add_topleft_text = options.pop('add_topleft_text', '')
    fig = options.pop('fig', None)
    ax = options.pop('ax', None)
    hline = options.pop('hline', None)

    def cutaway(x, min, max):
        if x < min or x > max:
            return np.nan
        else:
            return x

    if outlier_cutoffs is not None:
        cut_ub = 5
        cut_lb, cut_ub = (outlier_cutoffs[0], outlier_cutoffs[1])
        if cut_lb > plot_data.min().min() or cut_ub < plot_data.max().max():
            plot_data = plot_data.applymap(lambda x: cutaway(x, cut_lb, cut_ub))
            if add_topleft_text == "": add_topleft_text += " "
            add_topleft_text += "Values <{} and >{} have been cut from the data.".format(cut_lb, cut_ub)

    # get the first non-nan value for each series in the dataframe
    if xvals is not None:
        if hasattr(xvals, '__iter__') and not isinstance(xvals, str):
            xvals_used = xvals
        else:
            xvals_used = []
            n = len(list(plot_data))
            for i in range(n):
                # i = 1
                if xvals.lower() == 'middle':
                    target = plot_data.index[int(len(plot_data.index) * (i + 1) / (n + 1))]
                elif xvals.lower() == 'first_obs':
                    if plot_data.iloc[:, i].dropna().empty:
                        target = plot_data.index[0]
                    else:
                        target_raw = plot_data.iloc[:, i].dropna().index[0]
                        target_min = plot_data.index[int(label_offset * len(plot_data.index))]
                        target = max(target_raw, target_min)
                elif xvals.lower() == 'last_obs':
                    if plot_data.iloc[:, i].dropna().empty:
                        target = plot_data.index[-1]
                    else:
                        target = plot_data.iloc[:, i].dropna().index[-1]
                elif xvals.lower() == 'full_left':
                    target = plot_data.index[int(label_offset * len(plot_data.index))]
                elif xvals.lower() == 'full_right':
                    target = plot_data.index[-1]
                else:
                    raise ValueError("label location spec (\'xvals\') param \'{}\' not supported".format(xvals))

                try:
                    x = plot_data.iloc[:, i].loc[:target].index[-1]
                except:
                    try:
                        x = plot_data.iloc[:, i].loc[target:].index[0]
                    except:
                        x = np.nan
                if not pd.isnull(x):
                    xvals_used.append(date2num(plot_data.iloc[:, i].loc[:target].index[-1]))
                else:
                    xvals_used.append(np.nan)   # empty columns will be pruned in the next step
    else:
        xvals_used = None

    if drop_all_na:
        plot_data_clean = plot_data.dropna(axis=1, how='all')
    else:
        plot_data_clean = plot_data

    if xvals_used is not None:
        xvals_clean = list(itertools.compress(xvals_used, [x in plot_data_clean.columns for x in plot_data.columns]))
    else:
        xvals_clean = None

    # STEP 1 - create plot
    if fig is None:
        if figsize is None:
            fig, ax = plt.subplots()
        else:
            fig, ax = plt.subplots(figsize=figsize)
    else:
        if ax is None:
            ax = fig.axes[0]

    if use_house_colors and colors is None:
        housecolors = list(color_fns.housecolor().values())
        colors = housecolors[:plot_data_clean.shape[1]]

    if hline is not None:
        ax.axhline(hline, color='#4d4d4d', linewidth=1)

    if plot_data_clean.max().max() > 100000:
        ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))  # scilimits=(0, 0)

    if line_zorders is None:
        line_zorders = np.array(range(len(plot_data_clean.columns), 0, -1))*5
    else:
        line_zorders *= 5

    if colors is None:
        colors = np.zeros(len(plot_data_clean.columns))

    for i in range(len(colors)):
        plot_data_clean.iloc[:, i].plot(ax=ax, title=title, legend=False, color=colors[i], zorder=line_zorders[i])

    # TODO: this needs to be debugged properly
    lines = plt.gca().get_lines()
    tries = 0
    while len(lines) == 0 and tries < 5:
        if figsize is None:
            figr, axr = plt.subplots()
        else:
            figr, axr = plt.subplots(figsize=figsize)

        if use_house_colors and colors is None:
            housecolors = list(color_fns.housecolor().values())
            colors = housecolors[:plot_data_clean.shape[1]]

        if plot_data_clean.max().max() > 100000:
            axr.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))  # scilimits=(0, 0)

        # if rhs_axis:
        #     axr2 = ax.twinx()
        #     # ax.tick_params(labeltop=False, labelright=True)
        #     if plot_data_clean.max().max() > 100000:
        #         axr2.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))  # scilimits=(0, 0)

        if colors is None:
            plot_data_clean.plot(ax=axr, title=title, legend=False, )
        else:
            for i in range(len(colors)):
                plot_data_clean.iloc[:, i].plot(ax=axr, title=title, legend=False, color=colors[i])
        lines = figr.gca().get_lines()
        plt.close(figr)
        tries += 1

    if rhs_axis:
        ax2 = ax.twinx()

        if plot_data_clean.max().max() > 100000:
            ax2.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))  # scilimits=(0, 0)
        ax2.set_ylim(ax.get_ylim())

    if title_color is not None:
        ax.set_title(title, color=title_color)

    ax.set_xlabel("")
    ax.axes.set_xlim(left=np.min(plot_data_clean.index))

    # STEP 2 - create line labels
    if use_overlabels:
        labelLines(lines, zorder=label_zorder, align=align, xvals=xvals_clean, **options)


    if ylabel is not None:
        ax.set_ylabel(ylabel)

    if xlabel is not None:
        ax.set_xlabel(xlabel)

    if intraday:
        hours = mdates.HourLocator(interval=1)
        h_fmt = mdates.DateFormatter('%H:%M:%S')
        ax.xaxis.set_major_locator(hours)
        ax.xaxis.set_major_formatter(h_fmt)

    if edge is not None:
        if isinstance(edge, str) and edge == 'tight':
            fig.tight_layout()
        elif isinstance(edge, dict):
            fig.subplots_adjust(left=edge['l'], bottom=edge['b'], right=1 - edge['r'], top=1 - edge['t'], 
                                wspace=0, hspace=0)
        else:
            fig.subplots_adjust(left=edge, bottom=edge, right=1-edge, top=1-edge, wspace=0, hspace=0)

    if use_grid:
        ax.grid(color="#c9c9c9")

    if legend:
        ax.legend(legend)

    if add_topleft_text is not None:
        x_pos = plt.xlim()[0] + (plt.xlim()[1] - plt.xlim()[0])*0.025
        y_pos = plt.ylim()[1] - (plt.ylim()[1] - plt.ylim()[0])*0.035
        ax.text(x_pos, y_pos, add_topleft_text)

    if cross_out_plot:
        x_lo, x_hi = plt.xlim()
        y_lo, y_hi = plt.ylim()
        l = mlines.Line2D([x_lo, x_hi], [y_lo, y_hi], color='r', linewidth=1)
        ax.add_line(l)
        l = mlines.Line2D([x_hi, x_lo], [y_lo, y_hi], color='r', linewidth=1)
        ax.add_line(l)

        x_pos = plt.xlim()[0] + (plt.xlim()[1] - plt.xlim()[0]) * 0.03
        if add_topleft_text is not None:
            y_pos = plt.ylim()[1] - (plt.ylim()[1] - plt.ylim()[0]) * 0.07
        else:
            y_pos = plt.ylim()[1] - (plt.ylim()[1] - plt.ylim()[0])*0.035
        ax.text(x_pos, y_pos, "unreliable data", color='r')

    if return_filepath:
        filename = report_fns.get_temp_filename(override_path)
        fig.savefig(filename, dpi=80, transparent=True)  # , facecolor=fig.get_facecolor()
        if plot_close_after_use:
            plt.close(fig)
        return filename
    else:
        return fig


def plt_dual(plot_left, plot_right, title=None, y_left_label=None, y_right_label=None, label_axes=False,
             return_filepath=True, **options):

    blue = ( 23/255,  55/255,  94/255)
    mred = (192/255,   0/255,   0/255)

    # logic for adding lables (using the labelLines package)
    label_zorder = options.pop('label_zorder', 100)
    if 'zorder' in options.keys():
        raise ValueError("Please rename argument zorder to label_zorder")
    align = options.pop('align', False)
    edge = options.pop('edge', None)
    plot_close_after_use = options.pop('plot_close_after_use', True)
    ylabel_left = options.pop('ylabel_left', None)
    ylabel_right = options.pop('ylabel_right', None)
    xlabel = options.pop('xlabel', None)

    # STEP 1 - create plot
    fig, ax1 = plt.subplots()
    ax1.plot(plot_left, color=blue, label=y_left_label)
    ax1.axes.set_xlim(left=np.min(plot_left.index))

    # Make the y-axis label, ticks and tick labels match the line color.
    if y_left_label is not None and label_axes:
        ax1.set_ylabel(y_left_label, color=blue)
    ax1.tick_params('y', colors=blue)

    if y_left_label is not None:
        # xvals = (plot_left.index[0], plot_left.index[int(len(plot_left)/4)],)
        xval_target = plot_left.index[int(len(plot_left)*1/6)]
        xval_min = plot_left.dropna().index[0]
        xval_max = plot_left.dropna().index[-1]
        xvals = [min(max(xval_target, xval_min), xval_max)]
        labelLines(plt.gca().get_lines(), zorder=zorder, align=align, xvals=xvals, **options)

    ax2 = ax1.twinx()

    ax2.plot(plot_right, color=mred, label=y_right_label)
    # plot_right.plot(ax=ax2, color=mred, label=y_right_label)
    if y_right_label is not None and label_axes:
        ax2.set_ylabel(y_right_label, color=mred)
    ax2.tick_params('y', colors=mred)
    ax2.axes.set_xlim(left=np.min(plot_right.index))

    if title is not None:
        ax1.set_title(title)

    if y_right_label is not None:
        # xvals = (plot_left.index[int(len(plot_left)*3/4)], plot_left.index[-1],)
        xval_target = plot_left.index[int(len(plot_left)*5/6)]
        xval_min = plot_left.dropna().index[0]
        xval_max = plot_left.dropna().index[-1]
        xvals = [min(max(xval_target, xval_min), xval_max)]
        labelLines(plt.gca().get_lines(), zorder=zorder, align=align, xvals=xvals, **options)

    if ylabel_left is not None:
        ax1.set_ylabel(ylabel_left)

    if ylabel_right is not None:
        ax2.set_ylabel(ylabel_right)

    if xlabel is not None:
        ax1.set_xlabel(xlabel)

    if edge is not None:
        fig.subplots_adjust(left=edge, bottom=edge, right=1-edge, top=1-edge, wspace=0, hspace=0)


    # STEP 2 - get filename and save
    if return_filepath:
        filename = report_fns.get_temp_filename()

        # fig.patch.set_facecolor('xkcd:mint green')
        fig.savefig(filename, dpi=80, transparent=True)  # , facecolor=fig.get_facecolor()
        if plot_close_after_use:
            plt.close(fig)
        return filename
    else:
        return fig, ax1, ax2


if __name__ == '__main__':
    
    print("testing functions")

 






pvk report_fns.py





import os
import random
import string

from dateutil.relativedelta import relativedelta

from matplotlib.dates import date2num
import pandas as pd
import datetime as dt
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

from pylatex import Document, Section, Figure, NoEscape, LongTable, MultiColumn, Package

# utils_pvk imports
from utils_pvk.lib_labellines import labelLine, labelLines

STORAGE_LOCATION = "H:/python_local/storage/temp/"


def randomString(stringLength=6):
    """Generate a random string of fixed length """
    letters = string.ascii_lowercase + "0123456789"
    return ''.join(random.choice(letters) for _ in range(stringLength))


def get_storage_location():
    return STORAGE_LOCATION


def get_temp_filename(override_path=None, name="plot", extension="png"):
    """return filename for a plot"""
    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    rndstr = randomString(10)
    if not override_path:
        filename = STORAGE_LOCATION + "{}_{}_{}.{}".format(name, time, rndstr, extension)
    else:
        filename = override_path + "{}_{}_{}.{}".format(name, time, rndstr, extension)

    return filename


def plt_rangebox(diamond, left, right, min_rng=-1, max_rng=1, centerline=True):
    """plot range box"""
    fig = plt.figure(figsize=(2.8, 0.5))

    ax = fig.add_subplot(111)

    ax.plot([left, right], [0, 0], "k+")
    if centerline:
        ax.plot([0, 0], [2, -2], color=(0.8, 0.8, 0.8), linewidth=1)
    ax.plot([min_rng, max_rng], [0, 0], "k-", linewidth=1)
    ax.plot([diamond], [0], color=(0, 32/252, 96/252), marker="D", linewidth=1)
    ax.axis('off')

    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    i = 0
    file_available = False
    while not file_available and i < 100:
        i += 1
        filename = STORAGE_LOCATION + "{}_plot{:03}.png".format(time, i)
        if not os.path.isfile(filename):
            file_available = True

    fig.savefig(filename, dpi=300, bbox_inches='tight')

    # trim={<left> <lower> <right> <upper>}
    rangebox_command = ((r"""\hspace*{-2mm}""" +
                         r"""\raisebox{-0.8mm}{""" +
                         r"""\includegraphics[trim={12.45mm 10mm 5mm 5mm}, clip, scale=0.55]{[filename]}}""" +
                         r"""\hspace*{-2mm}""")
                        .replace('[filename]', filename))

    return NoEscape(rangebox_command)


def plt_corrhist(excel_data, return_latex=False):

    assetname_a = excel_data.columns[0]
    assetname_b = excel_data.columns[1]
    corrs = excel_data['corr_comb']
    corrs_smt = excel_data['corrs_smt']
    lower_trigger = excel_data['lower_trigger']
    upper_trigger = excel_data['upper_trigger']
    release = excel_data['release']
    event_trigger = excel_data['plot_trigger']
    event_release = excel_data['plot_release']
    price_a = excel_data[assetname_a]
    price_b = excel_data[assetname_b]

    grey = (127/255, 127/255, 127/255)
    dred = (149/255,  55/255,  53/255)
    mred = (192/255,   0/255,   0/255)
    lred = (255/255,   0/255,   0/255)
    green = ( 0/255, 180/255,   0/255)
    blue = ( 23/255,  55/255,  94/255)


    """plot range box"""
    fig = plt.figure(figsize=(14, 5))

    #################################################################
    # SUBPLOT 1 - CORRELATIONS
    #################################################################
    ax = fig.add_subplot(121)

    # linewidth = 2, markersize = 12, marker='o', linestyle='dashed', color='green',

    ax.plot(corrs,         label="Corr",          color='black', linewidth=2)
    ax.plot(corrs_smt,     label="Smooth",        color=grey   , linewidth=1, linestyle='dashed')
    ax.plot(lower_trigger, label="Lower Trigger", color=dred   , linewidth=1)
    ax.plot(upper_trigger, label="Upper Trigger", color=dred   , linewidth=1)
    ax.plot(release,       label="Release",       color=mred   , linewidth=2)
    ax.plot(event_trigger, label="Triggered",     color=lred   , marker='x', markersize=10, linestyle='none')
    ax.plot(event_release, label="Released",      color=green  , marker='x', markersize=10, linestyle='none')

    ax.legend(loc=3)
    ax.axes.set_xlim(left=np.min(corrs.index))
    ax.grid(True)
    ax.set_title("Past 1y Correlations: " + assetname_a + " vs. " + assetname_b)

    #################################################################
    # SUBPLOT 2 - PRICES
    #################################################################

    ay1 = fig.add_subplot(122)

    ay1.plot(price_a, label=assetname_a, color=blue, linewidth=1)
    ay1.xaxis.grid(True)

    ay2 = ay1.twinx()
    invert_ay2 = np.mean(corrs) < -0.10

    if invert_ay2:
        y2label = assetname_b + " (inverted)"
    else:
        y2label = assetname_b

    ay2.plot(price_b, label=y2label, color=mred, linewidth=1)

    if invert_ay2:
        ay2.invert_yaxis()
        # ay2.set_ylim(ay2.get_ylim()[::-1])

    ay1.legend(loc=2)
    ay2.legend(loc=1)
    ay1.axes.set_xlim(left=np.min(price_a.index))
    ay1.set_title("Past 1y Prices: " + assetname_a + " vs. " + assetname_b)

    # ADD TRIGGER VERTICAL LINES
    triggers = event_trigger[~np.isnan(event_trigger)].index
    for trigger in triggers:
        ay1.axvline(x=trigger, color=lred, linewidth=1)

    releases = event_release[~np.isnan(event_release)].index
    for release in releases:
        ay1.axvline(x=release, color=green, linewidth=1)

    # fig.show()
    # STEP 2 - save to latex code
    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    i = 0
    file_available = False
    while not file_available and i < 100:
        i += 1
        filename = STORAGE_LOCATION + "{}_plot{:03}.png".format(time, i)
        if not os.path.isfile(filename):
            file_available = True

    fig.subplots_adjust(hspace=0.1, wspace=0.1)


    if return_latex:
        fig.savefig(filename, dpi=300, bbox_inches='tight')
        # trim={<left> <lower> <right> <upper>}
        rangebox_command = ((r"""\hspace*{-2mm}""" +
                             r"""\raisebox{-0.8mm}{""" +
                             r"""\includegraphics[trim={12.45mm 10mm 5mm 5mm}, clip, scale=0.55]{[filename]}}""" +
                             r"""\hspace*{-2mm}""")
                            .replace('[filename]', filename))

        return NoEscape(rangebox_command)
    else:
        fig.savefig(filename, dpi=80, bbox_inches='tight')
        return filename


def plt_intraday(plot_data, title):
    return plt_linelabels(plot_data, title, intraday=True)


def plt_linelabels(plot_data, title, intraday=False, **options):

    # logic for adding lines (using the labelLines package)
    # options = dict()
    zorder = options.pop('zorder', 2.5)
    align = options.pop('align', False)
    xvals = options.pop('xvals', None)
    edge = options.pop('edge', None)
    ticks = options.pop('ticks', None)
    return_ticks = options.pop('return_ticks', False)

    # DUMMY - get plot labels
    plot_data_dummy = plot_data.copy()
    try:
        plot_data_dummy.index = date2num(plot_data_dummy.index)
    except:
        plot_data_dummy.index = date2num([x.to_timestamp(how="E") for x in plot_data_dummy.index])

    fig, ax = plt.subplots()
    plot_data_dummy.plot(ax=ax, title=title, legend=False, )
    ticksb = plt.xticks()

    # STEP 1 - create plot
    if ticks is not None:
        plt.xticks(ticks[0], ticks[1])
    else:
        fig, ax = plt.subplots()

        plot_data.plot(ax=ax, title=title, legend=False, )
        ax.set_xlabel("")

        ticks = plt.xticks()
        ticks = (ticksb[0], ticks[1])

    labelLines(plt.gca().get_lines(), zorder=zorder, align=align, xvals=xvals, **options)

    if intraday:
        hours = mdates.HourLocator(interval=1)  #
        h_fmt = mdates.DateFormatter('%H:%M:%S')
        ax.xaxis.set_major_locator(hours)
        ax.xaxis.set_major_formatter(h_fmt)

    if edge is not None:
        fig.subplots_adjust(left=edge, bottom=edge, right=1-edge, top=1-edge, wspace=0, hspace=0)


    # if major_formatter is not None:
    #     frmt = TimeSeries_DateFormatter("Y", dynamic_mode=True, minor_locator=False, plot_obj=ax)
    #     ax.xaxis.set_major_formatter(frmt)
    #     fig.gcf().autofmt_xdate()

    # STEP 2 - get filename and save

    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    i = 0
    file_available = False
    while not file_available and i < 100:
        i += 1
        filename = STORAGE_LOCATION + "{}_plot{:03}.png".format(time, i)
        if not os.path.isfile(filename):
            file_available = True

    # fig.patch.set_facecolor('xkcd:mint green')
    fig.savefig(filename, dpi=80)  # , facecolor=fig.get_facecolor()
    if return_ticks:
        return filename, ticks
    else:
        return filename



def plt_dual(plot_left, plot_right, title=None, y_left_label=None, y_right_label=None, x_label=None, label_axes=False,
             **options):

    blue = ( 23/255,  55/255,  94/255)
    mred = (192/255,   0/255,   0/255)

    # logic for adding lables (using the labelLines package)
    # options = dict()
    zorder = options.pop('zorder', 100)
    align = options.pop('align', False)
    edge = options.pop('edge', None)
    ticks = options.pop('ticks', None)
    return_ticks = options.pop('return_ticks', False)

    # STEP 1 - create plot
    fig, ax1 = plt.subplots()
    ax1.plot(plot_left, color=blue, label=y_left_label)
    # plot_left.to_frame().plot(ax=ax1, color=blue, label=y_left_label)

    if x_label is None:
        ticks = plt.xticks()

    # if ticks is not None:
    #     plt.xticks(ticks[0], ticks[1])
    # else:
    #     ticks = plt.xticks()

    # Make the y-axis label, ticks and tick labels match the line color.
    if y_left_label is not None and label_axes:
        ax1.set_ylabel(y_left_label, color=blue)
    ax1.tick_params('y', colors=blue)

    if y_left_label is not None:
        # xvals = (plot_left.index[0], plot_left.index[int(len(plot_left)/4)],)
        xvals = [plot_left.index[int(len(plot_left)*1/6)]]
        labelLines(plt.gca().get_lines(), zorder=zorder, align=align, xvals=xvals, **options)

    ax2 = ax1.twinx()

    ax2.plot(plot_right, color=mred, label=y_right_label)
    # plot_right.plot(ax=ax2, color=mred, label=y_right_label)
    if y_right_label is not None and label_axes:
        ax2.set_ylabel(y_right_label, color=mred)
    ax2.tick_params('y', colors=mred)

    # set or load ticks
    if ticks is not None:
        ax1.set_xticks(ticks[0])
        plt.xticks(ticks[0], ticks[1])

    if title is not None:
        ax1.set_title(title)

    if y_right_label is not None:
        # xvals = (plot_left.index[int(len(plot_left)*3/4)], plot_left.index[-1],)
        xvals = [plot_left.index[int(len(plot_left)*5/6)]]
        labelLines(plt.gca().get_lines(), zorder=zorder, align=align, xvals=xvals, **options)

    if edge is not None:
        fig.subplots_adjust(left=edge, bottom=edge, right=1-edge, top=1-edge, wspace=0, hspace=0)

    # frmt = TimeSeries_DateFormatter(freq="m")
    # if major_formatter is not None:
    #     ax1.xaxis.set_major_formatter(mdates.DateFormatter(major_formatter))

    # STEP 2 - get filename and save
    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    i = 0
    file_available = False
    while not file_available and i < 100:
        i += 1
        filename = STORAGE_LOCATION + "{}_plot{:03}.png".format(time, i)
        if not os.path.isfile(filename):
            file_available = True

    # fig.patch.set_facecolor('xkcd:mint green')
    fig.savefig(filename, dpi=80)  # , facecolor=fig.get_facecolor()
    if return_ticks:
        return filename, ticks
    else:
        return filename


def plt_simple(data, title=None, return_latex=False, figsize=(14, 4.9)):

    grey = (127/255, 127/255, 127/255)
    dred = (149/255,  55/255,  53/255)
    mred = (192/255,   0/255,   0/255)
    lred = (255/255,   0/255,   0/255)
    green = ( 0/255, 180/255,   0/255)
    blue = ( 23/255,  55/255,  94/255)


    """plot range box"""
    fig = plt.figure(figsize=figsize)
    ax1 = fig.add_subplot(111)

    #################################################################
    # SUBPLOT 1 - CORRELATIONS
    #################################################################

    ax1.plot(data)

    # ax1.legend(loc=3)
    ax1.axes.set_xlim(left=np.min(data.index))
    ax1.grid(True)
    if title is not None:
        fig.set_title(title)

    # fig.show()
    # STEP 2 - save to latex code
    time = dt.datetime.today().strftime("%Y-%m-%d_%H-%M-%S")
    i = 0
    file_available = False
    while not file_available and i < 100:
        i += 1
        filename = STORAGE_LOCATION + "{}_plot{:03}.png".format(time, i)
        if not os.path.isfile(filename):
            file_available = True

    fig.subplots_adjust(hspace=0.1, wspace=0.1)

    if return_latex:
        fig.savefig(filename, dpi=300, bbox_inches='tight')
        # trim={<left> <lower> <right> <upper>}
        rangebox_command = ((r"""\hspace*{-2mm}""" +
                             r"""\raisebox{-0.8mm}{""" +  # trim={<left> <lower> <right> <upper>}
                             r"""\includegraphics[trim={12.45mm 20mm 5mm 15mm}, clip, scale=0.55]{[filename]}}""" +
                             r"""\hspace*{-2mm}""")
                            .replace('[filename]', filename))

        return NoEscape(rangebox_command)
    else:
        fig.savefig(filename, dpi=80, bbox_inches='tight')
        return filename


def test_plt_corrhist():
    testfile = r"\\clndata01\home$\pvklooster\excel\02_corr_monitor\example_data.xlsx"
    test_data_full = pd.read_excel(testfile).set_index('date').sort_index()
    test_data = test_data_full.loc[test_data_full.index[-1] - relativedelta(years=1):]

    plt_corrhist(test_data, return_latex=False)


if __name__ == '__main__':


    print("testing functions")
    test_plt_corrhist()

 




pvk string_fns.py





import pandas as pd
import numpy as np
import datetime as dt
import hashlib
import re
import unicodedata


def hash(s):
    return hashlib.sha224(s.encode('utf-8')).hexdigest()


def is_number(s):
    """test if input is a number"""
    try:
        float(s)
        return True
    except (ValueError, TypeError):
        return False


def num_to_string(n, n_fixed_digits=None, decimal_precision=None):

    if n_fixed_digits is None:
        if decimal_precision is None:
            n_fixed_digits = 4
    elif decimal_precision is not None:
        raise ValueError("Conflicting options set: " +
                        "Only one of the options: (fixed_digits, decimal_precision) can be not None")
    """convert number to string"""
    if not is_number(n):
        raise ValueError("passed value \'{}\' is not a number or convertable to a number.".format(n))

    if isinstance(n, int):
        return "{}".format(n)
    else:
        if n_fixed_digits is not None:
            return fixed_digits(n, n_fixed_digits)
        elif decimal_precision is not None:
            return ("{:." + str(decimal_precision) + "f}").format(n)
        else:
            raise ValueError("Error in function num_to_string in lib_string_fns, please check input parsing.")

# date to string in lib_date_fns


def fixed_digits(float_num, max_total_digits=4):
    """
    format a float to string in standard decimal notation, to have a fixed number of **total** digits

    E.g., for max_total_digits=6:
    - 0.00123456789 gives "0.00123"
    - 0.0000000123456789 gives "0.00000"
    - 12345678.9 gives "12345679" (on overflow, all digits left of decimalpoint remain)
    - 4.2 gives "4.2" (no trailing zero's)
    """
    string_out = str(float_num).rstrip("0").rstrip(".")
    if len(string_out) <= max_total_digits or "." not in string_out:
        return string_out
    else:
        n_decimals = max(max_total_digits - len(str(float_num).split(".")[0]), 0)
        try:
            x = ("{:." + str(n_decimals) + "f}").format(float_num)
        except:
            print('stop')
        return ("{:." + str(n_decimals) + "f}").format(float_num)


def recursive_replace(input_cfg, old_str, new_str):
    """apply string replace to all values in nested dict/list"""
    # check whether it's a dict, list, tuple, or scalar
    if isinstance(input_cfg, dict):
        items = input_cfg.items()
    elif isinstance(input_cfg, (list, tuple)):
        items = enumerate(input_cfg)
    else:
        # just a value, split and return
        return str(input_cfg).replace(old_str, new_str)

    # now call ourself for every value and replace in the input
    for key, value in items:
        new = recursive_replace(value, old_str, new_str)
        if new != input_cfg[key]:
            input_cfg[key] = new
    return input_cfg


def slugify(value):
    """
    Normalizes string, converts to lowercase, removes non-alpha characters,
    and converts spaces to hyphens.
    """
    value = unicodedata.normalize('NFKD', value)
    value = re.sub('[^\w\s-]', '', value).strip().lower()
    value = re.sub('[-\s]+', '-', value)
    return value

 






pvk ticker_fns.py


###################################################################
# Internal tickers
###################################################################


def swap_rate(ccy, tenor, fwd_start=''):
    """get default format swap ticker"""
    return f'{ccy}swp_{fwd_start}{tenor}'


def swap_duration(ccy, tenor, fwd_start=''):
    """get default format swap duration ticker"""
    return f'{ccy}swd_{fwd_start}{tenor}'


def swap_vol(ccy, tenor, expiry):
    """get default format swaption vol ticker"""
    return f'{ccy}swo_{expiry}{tenor}'


def curve_vol(ccy, expiry, tenor_a, tenor_b):
    """get default format cms curve vol ticker"""
    return f'{ccy}swo_{expiry}{tenor_a[:-1]}s{tenor_b[:-1]}s'


def fx_rate(quote_ccy, base_ccy='usd', fwd_start=''):
    """get default format fx rate ticker"""
    if fwd_start is None or fwd_start == '':
        return f'{base_ccy}{quote_ccy}'
    else:
        return f'{base_ccy}{quote_ccy}_{fwd_start}'


###################################################################
# Opendata tickers
###################################################################

def od_swap_rate(ccy, tenor, fwd_start='', metric='par'):
    if fwd_start == '':
        fwd_start = '0d'

    if metric.lower() == 'par':
        metric = 'parcoupon'
    elif metric.lower() in ('ann', 'dur'):
        metric = 'annuity'
    else:
        raise ValueError((f'Metric: \'{metric}\' not supported for OpenData data right now ' +
                          f'(only \'par\' and \'ann\'/\'dur\' atm)'))

    if ccy.lower() == 'usd':
        return f'rates_parswap_usd_usdlibor3m_semi_{fwd_start}_{tenor}|{metric}|nyclose|rates_grp_clean'
    elif ccy.lower() == 'eur':
        return f'rates_parswap_eur_euribor6m_annual_{fwd_start}_{tenor}|{metric}|lonclose|rates_grp_clean'
    else:
        raise ValueError(f'Ccy {ccy} not supported right now.')


###################################################################
# JPMDQ tickers
###################################################################

def jpm_swap_rate(ccy, tenor, fwd_start='', metric='par'):

    if metric.lower() == 'par':
        metric = 'RT_MID'
    elif metric.lower() in ('dur', 'ann'):
        metric = 'AM_MOD_DUR'
    else:
        raise ValueError((f'Metric: \'{metric}\' not supported for JPM data right now ' +
                          f'(only \'par\' and \'ann\'/\'dur\' atm)'))

    if len(fwd_start) > 0:
        fwd_start = fwd_start.zfill(3)

    return f'DB(CCV,CRVSWAPMMKT,{ccy},{tenor},{fwd_start},{metric})'


def jpm_curve_vol(ccy, expiry, tenor_a, tenor_b, metric='vol'):

    if metric.lower() == 'vol':
        metric = 'BPVOL'
    else:
        raise ValueError((f'Metric: \'{metric}\' not supported for JPM data right now ' +
                          f'(only \'vol\')'))

    if ccy.lower() != 'usd':
        raise ValueError((f'Ccy: \'{metric}\' not supported for JPM data right now ' +
                          f'(only \'usd\')'))

    return f'DB(FDER,YCSO,{tenor_a}x{tenor_b},{expiry},S,ATMF,0,{metric})'


def jpm_fx_rate(quote_ccy, base_ccy='usd', fwd_start=''):
   """get default format fx rate ticker"""

    quote_ccy = quote_ccy.lower()
    base_ccy = base_ccy.lower()

    if len(fwd_start) > 0:
        fwd_start = fwd_start.zfill(3)

    if quote_ccy == base_ccy:
        raise ValueError(f'Quote ccy ({quote_ccy}) is the same as base ccy ({base_ccy}')

    if base_ccy == 'usd':
        if quote_ccy in ('eur', 'gbp', 'aud', 'nzd'):
            ticker = f"1 / DB(CFX,{quote_ccy.upper()},{fwd_start})"
        else:
            ticker = f"DB(CFX,{quote_ccy.upper()},{fwd_start})"
    else:
        quote_ticker = f"DB(CFX,{quote_ccy.upper()},{fwd_start})"
        base_ticker = f"DB(CFX,{base_ccy.upper()},{fwd_start})"

        if base_ccy in ('eur', 'gbp', 'aud', 'nzd'):
            if quote_ccy in ('eur', 'gbp', 'aud', 'nzd'):
                ticker = quote_ticker + " / " + base_ticker
            else:
                ticker = "1 / " + base_ticker + " / " + quote_ticker
        else:
            if quote_ccy in ('eur', 'gbp', 'aud', 'nzd'):
                ticker = base_ticker + " * " + quote_ticker
            else:
                ticker = base_ticker + " / " + quote_ticker

    return ticker


###################################################################
# CITI tickers
###################################################################

def citi_swap_rate(ccy, tenor, fwd_start='', metric='par'):
    if metric.lower() != 'par':
        raise ValueError('Citi only has par rates data for swap rates')

    if fwd_start == '':
        return f'RATES.SWAP.{ccy}.PAR.{tenor}'
    else:
        return f'RATES.SWAP.{ccy}.FWD.{fwd_start}.{tenor}'

 




pvk timeseries_fns.py



import pandas as pd
import numpy as np
import itertools


# compute returns
def returns_series(price_series, return_type, return_period=1):
    if return_type == 'not':
        price_series = price_series.apply(np.log)
    elif return_type.lower() == 'ed dv01':
        price_series = 10000 - price_series * 100
    elif return_type.lower() == 'dv01':
        price_series = price_series * 100
    elif return_type.lower() == 'bp dv01':
        None
    else:
        raise ValueError('return type \'{}\' not recognized'.format(return_type))

    return price_series.fillna(method='ffill').diff(periods=return_period).apply(
        lambda x: x / np.sqrt(return_period))


def align(pd_objects, join='outer', axis=0):
    """apply align on all combinations of the list of pd objects"""
    for (i, j) in itertools.combinations(range(len(pd_objects)), 2):
        (pd_objects[i], pd_objects[j]) = pd_objects[i].align(pd_objects[j], join, axis)

    return tuple(pd_objects)


def meanreversion_line(srs, periods=range(1, 31), ret_type='log'):
    """Mean reversion line (compare realized vol at different intervals)"""

    if ret_type not in ('log', 'arith', 'pct'):
        raise ValueError(f'Return type must be log, arith, pct (given type \'{ret_type}\' not recognized')

    std = pd.Series(index=periods)

    for per in periods:
        if ret_type == 'arith':
            rets = srs.diff(per)
        elif ret_type == 'log':
            rets = srs.apply(np.log).diff(per)
        elif ret_type == 'pct':
            rets = srs / srs.shift(per) - 1
        else:
            raise ValueError(f'Uncaught error. Return type must be log, arith, pct (given type \'{ret_type}\' not ' +
                             'recognized')

        std[per] = np.std(rets) * np.sqrt(252/per)

    return std

 






ronglin cta.py





import pandas as pd
import numpy as np
import datetime as dt


def rolling_window(p, window):
    '''
    rolling window for rolling calculation
    '''
    
    shape = p.shape[:-1] + (p.shape[-1] - window + 1, window)
    strides = p.strides + (p.strides[-1],)
    return np.lib.stride_tricks.as_strided(p, shape=shape, strides=strides)


def mva(p, n, type = 's'):
    '''
    moving average for series p in window n
    input and output are numpy ndarray
    '''

    p = np.asarray(p)
    length = len(p)
    # change to 1 dimension array
    try:
        if p.shape[1] == 1:
            p = p[:,0]
    except:
        pass
    
    if type == 's':
        # simple moving average
        weights = np.ones(n)
        weights /= weights.sum()
        a = np.convolve(p, weights, mode='full')[:length]
       a[:n-1] = np.nan
        
    elif type == 'e':
        # exponential moving average
        alpha = 2./(n+1)
        #window = np.max([100,n*5])
        # fast but values at the beginning are not accurate
        # extend array to left, fill with the first value
        p_ = np.insert(p,0,p[0]*np.ones(n*5))
        weights = [(1-alpha)**x for x in range(0,n*5)]
        weights = np.asarray(weights)
        weights /= weights.sum()
        a_ = np.convolve(p_, weights, mode='full')[:length+n*5]
        a = a_[n*5:]
        a[:n-1] = np.nan
        
        # slow but values at the beginning are accurate
        # only use this to calculate first 200 values
        #idx = np.where(~np.isnan(p))[0][0]
        idx = 0
        a[n-1+idx] = np.mean(p[idx:n+idx])
        for i in range(n+idx,np.min([length,n+idx+n*5])):
            a[i] = alpha*p[i] + (1-alpha)*a[i-1]
        
    elif type == 'sm':
        # smoothed moving average
        a = np.empty((length,))
        a.fill(np.NaN)
        # first non nan in data
        idx = np.where(~np.isnan(p))[0][0]
        a[n-1+idx] = np.mean(p[idx:n+idx])
        for i in range(n+idx,length):
            a[i] = (a[i-1]*(n-1)+p[i])/n 
    
    return a


def macd(p, s = 12, l = 26, sig_line = 9, ma_type = 'e'):
   '''
    macd, input and output is np array
    '''
    
    mv1 = mva(p,s,ma_type)
    mv2 = mva(p,l,ma_type)
    mvd = mv1-mv2
    mvd_sig = mva(mvd,sig_line,ma_type)
    hist = mvd-mvd_sig
    
    return mvd,mvd_sig,hist


def bollinger(p,window,thr):
    '''
    bollinger band
    '''
    
    mv = mva(p,window)
    sd = np.empty(len(p))
    sd.fill(np.NaN)
    sd[window-1:] = np.std(rolling_window(p, window),axis=1)
    upper_band = mv + thr * sd
    lower_band = mv - thr * sd
    band_width = (upper_band - lower_band) / mv
    pctb = (p - lower_band) / (upper_band - lower_band)
    
    return upper_band, lower_band, band_width, pctb, mv


def donchian(highp, lowp = None, closep = None, window = 20):
    '''
    donchian channel
    '''
    
    if lowp is None and closep is None:
        lowp = highp
        closep = highp
    
    length = len(closep)
    p_max = np.empty(length)
    p_max.fill(np.NaN)
    p_min = np.empty(length)
    p_min.fill(np.NaN)
    don = np.empty(length)
    don.fill(np.NaN)
    
    p_max[window-1:] = np.max(rolling_window(highp, window),axis=1)
    p_min[window-1:] = np.min(rolling_window(lowp, window),axis=1)
    
    don = np.empty(len(closep))
    don.fill(np.NaN)
    
    for i in range(window,len(closep)):
        if closep[i] > p_max[i-1]:
            don[i] = 1
        elif closep[i] < p_min[i-1]:
            don[i] = -1
        else:
            don[i] = don[i-1]            
    
    return p_max, p_min, don


def atr(highp,lowp=None,closep=None,window=14):
    '''
    exp moving average of true range
    '''
    
    if isinstance(highp,np.float64):
        out = tr(highp,lowp,closep)
    else:
        length = len(highp)
        if window > length:
            print('Period of ATR cannot be larger than length of prices.')
            return
        
        if window is None or window == 0:
            true_range = tr(highp,lowp,closep)
            out = np.mean(true_range)
        elif window == 1:
            out = tr(highp,lowp,closep)
        else:
            true_range = tr(highp,lowp,closep)
            out = mva(true_range,window,'e')
    return out


def cta_position():

    
    sdate = dt.datetime.strptime('1990-01-01','%Y-%m-%d')
    edate = dt.datetime.now() - dt.timedelta(days=1)
    weekdays = pd.bdate_range(sdate, edate, freq='B')
    output = pd.DataFrame(index=weekdays)

    # get price
    df = pd.read_csv("Y:\\DataShare\\temp\\TY1.csv")
    df.set_index('Date',inplace=True)
    df.index = pd.to_datetime(df.index, format = '%d/%m/%Y')

    insname = 'TY1'
    cumbasis = np.cumsum(df['Basis'][::-1])[::-1]
    cumbasis = np.append(cumbasis[1:],0)
    highp = df['High'].values + cumbasis
    lowp = df['Low'].values + cumbasis
    closep = df['Close'].values + cumbasis
    dates = df.index

    # 10 indicators
    ind1 = np.zeros(len(closep),dtype=int)
    ind2 = np.zeros(len(closep),dtype=int)
    ind3 = np.zeros(len(closep),dtype=int)
    ind4 = np.zeros(len(closep),dtype=int)
    ind5 = np.zeros(len(closep),dtype=int)
    ind6 = np.zeros(len(closep),dtype=int)
    ind7 = np.zeros(len(closep),dtype=int)
    ind8 = np.zeros(len(closep),dtype=int)
    ind9 = np.zeros(len(closep),dtype=int)
    ind10 = np.zeros(len(closep),dtype=int)

    # long term moving averages
    mv50 = mva(closep,50)
    mv200 = mva(closep,200)
    mvd = mv50 - mv200
    mvd[np.isnan(mvd)] = 0
    mv200[np.isnan(mv200)] = 0
    ind1[mv200 > 0] = 1
    ind1[mv200 < 0] = -1
    ind2[mvd > 0] = 1
    ind2[mvd < 0] = -1

    # donchian channel
    ind3 = donchian(highp, lowp, closep, 120)[2]
    ind4 = donchian(highp, lowp, closep, 200)[2]
    ind3[np.isnan(ind3)] = 0
    ind4[np.isnan(ind4)] = 0

    # rate of change
    roc1 = df['Close'] - df['Close'].shift(200)
    roc2 = df['Close'] - df['Close'].shift(250)
    ind5[roc1 >= 0] = 1
    ind5[roc1 < 0] = -1
    ind6[roc2 >= 0] = 1
    ind6[roc2 < 0] = -1

    # macd
    macd1 = macd(closep, 60, 130, 45)[2]
    macd1[np.isnan(macd1)] = 0
    ind7[macd1 >= 0] = 1
    ind7[macd1 < 0] = -1
    macd2 = macd(closep, 12, 26, 9)[2]
    macd2[np.isnan(macd2)] = 0
    ind8[macd2 >= 0] = 1
    ind8[macd2 < 0] = -1

    # bollinger
    upp_band1, low_band1 = bollinger(closep, 150, 0.5)[:2]
    upp_band2, low_band2 = bollinger(closep, 150, 1.5)[:2]
    upp_band1[np.isnan(upp_band1)] = 0
    low_band1[np.isnan(low_band1)] = 0
    upp_band2[np.isnan(upp_band2)] = 0
    low_band2[np.isnan(low_band2)] = 0
    ind9[closep > upp_band1] = 1
    ind9[closep < low_band1] = -1
    ind10[closep > upp_band2] = 1
    ind10[closep < low_band2] = -1

    ind = ind1+ind2+ind3+ind4+ind5+ind6+ind7+ind8+ind9+ind10
    ind = pd.Series(ind, index=dates)
    ind = ind.reindex(weekdays,method='ffill')
    output[insname] = ind
    #output = output.rolling(5).mean()
    output.to_csv("Y:\\DataShare\\temp\\TY1_cta.csv")


if __name__ == '__main__':

    cta_position()

 




IMM total return swap ronglin.py





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

' this is swap names for ins_name in get_imm_swap()
name_list = ['USD_3M','EUR_6M','GBP_6M','JPY_6M',
            'CHF_6M','AUD_6M','NZD_3M','CAD_3M',
            'NOK_6M','SEK_3M']

def get_imm_swap(ins_name, tenor, imm='IMM1'):
    '''
    get IMM swap price
    Input:
    ins_name: name from name_list
    tenor: '2Y','5Y','10Y'
    imm: the first two IMMs, IMM1 and IMM2

    Output:
    price: the spot price used as entry price
    price_adj: roll adjusted price used for calculating pnl, assumes DV01 constant
    '''
    
    hdf_path = "Y:\\DataShare\\IMM_total_return.h5"
    price_df = pd.read_hdf(hdf_path,ins_name+'_'+tenor+'_'+imm)
    price_df[price_df==-10000] = np.NaN
    price_df.fillna(method='ffill', inplace=True)
    price_df.fillna(method='bfill', inplace=True)
    price = price_df['CloseP']
    price = price.to_frame('closep')
    cumbasis = np.cumsum(price_df['Basis'].values[::-1])[::-1]
    cumbasis = np.append(cumbasis[1:],0)
    price_adj = price_df['CloseP'] + cumbasis
    price_adj = price_adj.to_frame('closep')
    
    return price, price_adj


if __name__ == '__main__':

    price, price_adj = get_imm_swap('USD_3M', '5Y', imm='IMM1')
    plt.plot(price, label='spot')
    plt.plot(price_adj, label='roll adjusted')
    plt.legend()
    plt.show()

 

 

 

StartDate,CloseP,BPV,Basis,IMMDate

1998-01-02,6.062152868801125,3499.931173809804,0.0,1998-03-18

1998-01-05,5.891109761198461,3538.25357114058,0.0,1998-03-18

1998-01-06,5.859155461452731,3545.9820694355294,0.0,1998-03-18

1998-01-07,5.883036585544614,3542.4261443810537,0.0,1998-03-18




quant alib convention.py



from panormus.config.settings import ALIB_HOL_FILE_DIR

HOLIDAY_PATH = ALIB_HOL_FILE_DIR

SWAPITON_CONV_DICT = {
    'USD.3ML': {
        'barc_settlement_type': 'phys',
    },
    'EUR.6ML': {
        'barc_settlement_type': 'cash',
    },
    'GBP.6ML': {
        'barc_settlement_type': 'cash',
    },
}

CONV_DICT = {
    'USD.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+USD.FX',
        'fixing_offset': -2
    },
    'USD.1ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '1M',
        'float_rate_interval': '1M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+USD.FX',
        'fixing_offset': -2,
    },
    'USD.3ML': {
        'fixed_interval': '6M',
        'fixed_dcc': '30/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
       'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+USD.FX',
        'fixing_offset': -2
    },
    'USD.6ML': {
        'fixed_interval': '6M',
        'fixed_dcc': '30/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+USD.FX',
        'fixing_offset': -2
    },
    'USD.3ML.IMM': {
        'fixed_interval': '2i',
        'fixed_dcc': '30/360',
        'float_interval': '1i',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+USD.FX',
        'fixing_offset': -2
    },
    'CHF.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+CHF',
        'fixing_offset': -2
    },
    'CHF.1ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '1M',
        'float_rate_interval': '1M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+CHF',
        'fixing_offset': -2
    },
    'CHF.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+CHF',
        'fixing_offset': -2
    },
    'CHF.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+CHF',
        'fixing_offset': -2
    },
    'EUR.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'EUR.1ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30E/360',
        'float_interval': '1M',
        'float_rate_interval': '1M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'EUR.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30E/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'EUR.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30E/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'EUR.3ML.IMM': {
        'fixed_interval': '4i',
        'fixed_dcc': '30E/360',
        'float_interval': '1i',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'EUR.6ML.IMM': {
        'fixed_interval': '4i',
        'fixed_dcc': '30E/360',
        'float_interval': '2i',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'EUR',
        'fixing_offset': -2
    },
    'GBP.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'GBP.1ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
       'float_interval': '1M',
        'float_rate_interval': '1M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'GBP.3ML': {
        'fixed_interval': '3M',
        'fixed_dcc': 'act/365F',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'GBP.6ML': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'GBP.3ML.IMM': {
        'fixed_interval': '4i',
        'fixed_dcc': 'act/365F',
        'float_interval': '1i',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'GBP.6ML.IMM': {
        'fixed_interval': '2i',
        'fixed_dcc': 'act/365F',
        'float_interval': '2i',
        'float_rate_interval': '6M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP',
        'fixing_offset': 0
    },
    'CAD.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CAD',
        'fixing_offset': 0
    },
    'CAD.3ML': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CAD',
        'fixing_offset': 0
    },
    'CAD.3ML.SD': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CAD',
        'fixing_offset': 0
    },
    'JPY.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'JPY',
        'fixing_offset': -2
    },
    'JPY.1ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1M',
        'float_rate_interval': '1M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+JPY',
        'fixing_offset': -2
    },
    'JPY.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+JPY',
        'fixing_offset': -2
    },
    'JPY.6ML': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+JPY',
        'fixing_offset': -2
    },
    'JPY.6ML.SD': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'GBP+JPY',
        'fixing_offset': -2
    },
    'SEK.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'SEK',
        'fixing_offset': -2
    },
    'SEK.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'SEK',
        'fixing_offset': -2
    },
    'AUD.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'AUD',
        'fixing_offset': -1
    },
    'AUD.3ML': {
        'fixed_interval': '3M',
        'fixed_dcc': 'act/365F',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'AUD',
        'fixing_offset': -1
    },
    'AUD.6ML': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'AUD',
        'fixing_offset': -1
    },
    'NOK.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'NOK',
        'fixing_offset': -2
    },
    'NOK.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'NOK',
        'fixing_offset': -2
    },
    'NOK.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'NOK',
        'fixing_offset': -2
    },
    'DKK.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'DKK',
        'fixing_offset': -2
    },
    'DKK.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'DKK',
        'fixing_offset': -2
    },
    'DKK.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': '30/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'DKK',
        'fixing_offset': -2
    },
    'NZD.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
       'principal_initial_flag': 0,
        'holiday_calendar_name': 'NZD',
        'fixing_offset': -2
    },
    'NZD.3ML': {
        'fixed_interval': '6M',
        'fixed_dcc': 'act/365F',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'NZD',
        'fixing_offset': -2
    },
    'HUF.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'HUF',
        'fixing_offset': -2
    },
    'HUF.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'HUF',
        'fixing_offset': -2
    },
    'HUF.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'HUF',
        'fixing_offset': -2
    },
    'PLN.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365F',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'PLN',
        'fixing_offset': -2
    },
    'PLN.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/act',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'PLN',
        'fixing_offset': -2
    },
    'PLN.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/act',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/365F',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'PLN',
        'fixing_offset': -2
    },
    'CZK.OIS': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '1Y',
        'float_rate_interval': '1D',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'N',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CZK',
        'fixing_offset': -2
    },
    'CZK.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CZK',
        'fixing_offset': -2
    },
    'CZK.6ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/360',
        'float_interval': '6M',
        'float_rate_interval': '6M',
        'float_dcc': 'act/360',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'CZK',
        'fixing_offset': -2
    },
    'ILS.3ML': {
        'fixed_interval': '1Y',
        'fixed_dcc': 'act/365',
        'float_interval': '3M',
        'float_rate_interval': '3M',
        'float_dcc': 'act/365',
        'stub_method': 'f/s',
        'convexity_adjust': 0,
        'vol_model': 'NULL',
        'accrual_bad_day_conv': 'M',
        'pay_bad_day_conv': 'M',
        'reset_bad_day_conv': 'M',
        'principal_final_flag': 0,
        'principal_initial_flag': 0,
        'holiday_calendar_name': 'ILS',
        'fixing_offset': -2
    },
}

CURVE_NAMES_DICT = {
    'USD.OIS': {
        'dcrv': 'USD.OIS.ON',
        'ecrv': 'USD.OIS.ON',
    },
    'USD.1ML': {
        'dcrv': 'USD.OIS.ON',
        'ecrv': 'USD.LIBOR.1M',
    },
    'USD.3ML': {
        'dcrv': 'USD.OIS.ON',
        'ecrv': 'USD.LIBOR.3M',
    },
    'USD.6ML': {
        'dcrv': 'USD.OIS.ON',
        'ecrv': 'USD.LIBOR.6M',
    },
    'USD.3ML.IMM': {
        'dcrv': 'USD.OIS.ON',
        'ecrv': 'USD.LIBOR.3M',
    },
    'CHF.OIS': {
        'dcrv': 'CHF.TOIS.ON',
        'ecrv': 'CHF.TOIS.ON',
    },
    'CHF.1ML': {
        'dcrv': 'CHF.TOIS.ON',
        'ecrv': 'CHF.LIBOR.1M',
    },
    'CHF.3ML': {
        'dcrv': 'CHF.TOIS.ON',
        'ecrv': 'CHF.LIBOR.3M',
   },
    'CHF.6ML': {
        'dcrv': 'CHF.TOIS.ON',
        'ecrv': 'CHF.LIBOR.6M',
    },
    'EUR.OIS': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EONIA.ON',
    },
    'EUR.1ML': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EURIBOR.1M',
    },
    'EUR.3ML': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EURIBOR.3M',
    },
    'EUR.6ML': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EURIBOR.6M',
    },
    'EUR.3ML.IMM': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EURIBOR.3M',
    },
    'EUR.6ML.IMM': {
        'dcrv': 'EUR.EONIA.ON',
        'ecrv': 'EUR.EURIBOR.6M',
    },
    'GBP.OIS': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.SONIA.ON',
    },
    'GBP.1ML': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.LIBOR.1M',
    },
    'GBP.3ML': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.LIBOR.3M',
    },
    'GBP.6ML': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.LIBOR.6M',
    },
    'GBP.3ML.IMM': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.LIBOR.3M',
    },
    'GBP.6ML.IMM': {
        'dcrv': 'GBP.SONIA.ON',
        'ecrv': 'GBP.LIBOR.6M',
    },
    'CAD.OIS': {
        'dcrv': 'CAD.CORRA.ON',
        'ecrv': 'CAD.CORRA.ON',
    },
    'CAD.3ML': {
        'dcrv': 'CAD.CORRA.ON',
        'ecrv': 'CAD.CDOR.3M',
    },
    'CAD.3ML.SD': {
        'dcrv': 'CAD.CDOR.3M',
        'ecrv': 'CAD.CDOR.3M',
    },
    'JPY.OIS': {
        'dcrv': 'JPY.TONAR.ON',
        'ecrv': 'JPY.TONAR.ON',
    },
    'JPY.1ML': {
        'dcrv': 'JPY.TONAR.ON',
        'ecrv': 'JPY.LIBOR.1M',
    },
    'JPY.3ML': {
        'dcrv': 'JPY.TONAR.ON',
        'ecrv': 'JPY.LIBOR.3M',
    },
    'JPY.6ML': {
        'dcrv': 'JPY.TONAR.ON',
        'ecrv': 'JPY.LIBOR.6M',
    },
    'JPY.6ML.SD': {
        'dcrv': 'JPY.LIBOR.6M',
        'ecrv': 'JPY.LIBOR.6M',
    },
    'SEK.OIS': {
        'dcrv': 'SEK.STINA.ON',
        'ecrv': 'SEK.STINA.ON',
    },
    'SEK.3ML': {
        'dcrv': 'SEK.STINA.ON',
        'ecrv': 'SEK.STIBOR.3M',
    },
    'AUD.OIS': {
        'dcrv': 'AUD.AONIA.ON',
        'ecrv': 'AUD.AONIA.ON',
    },
    'AUD.3ML': {
        'dcrv': 'AUD.AONIA.ON',
        'ecrv': 'AUD.BBSW.3M',
    },
    'AUD.6ML': {
        'dcrv': 'AUD.AONIA.ON',
        'ecrv': 'AUD.BBSW.6M',
    },
    'NOK.OIS': {
        'dcrv': 'NOK.NOWA.ON',
        'ecrv': 'NOK.NOWA.ON',
    },
    'NOK.3ML': {
        'dcrv': 'NOK.NOWA.ON',
        'ecrv': 'NOK.NIBOR.3M',
    },
    'NOK.6ML': {
        'dcrv': 'NOK.NOWA.ON',
        'ecrv': 'NOK.NIBOR.6M',
    },
    'DKK.OIS': {
        'dcrv': 'DKK.CITA.ON',
        'ecrv': 'DKK.CITA.ON',
    },
    'DKK.3ML': {
        'dcrv': 'DKK.CITA.ON',
        'ecrv': 'DKK.CIBOR.3M',
    },
    'DKK.6ML': {
        'dcrv': 'DKK.CITA.ON',
       'ecrv': 'DKK.CIBOR.6M',
    },
    'NZD.OIS': {
        'dcrv': 'NZD.NONIA.ON',
        'ecrv': 'NZD.NONIA.ON',
    },
    'NZD.3ML': {
        'dcrv': 'NZD.NONIA.ON',
        'ecrv': 'NZD.BBR.3M',
    },
    'HUF.OIS': {
        'dcrv': 'HUF.HUFONIA.ON',
        'ecrv': 'HUF.HUFONIA.ON',
    },
    'HUF.3ML': {
        'dcrv': 'HUF.HUFONIA.ON',
        'ecrv': 'HUF.BUBOR.3M',
    },
    'HUF.6ML': {
        'dcrv': 'HUF.HUFONIA.ON',
        'ecrv': 'HUF.BUBOR.6M',
    },
    'PLN.OIS': {
        'dcrv': 'PLN.POLONIA.ON',
        'ecrv': 'PLN.POLONIA.ON',
    },
    'PLN.3ML': {
        'dcrv': 'PLN.POLONIA.ON',
        'ecrv': 'PLN.WIBOR.3M',
    },
    'PLN.6ML': {
        'dcrv': 'PLN.POLONIA.ON',
        'ecrv': 'PLN.WIBOR.6M',
    },
    'CZK.OIS': {
        'dcrv': 'CZK.CZEONIA.ON',
        'ecrv': 'CZK.CZEONIA.ON',
    },
    'CZK.3ML': {
        'dcrv': 'CZK.CZEONIA.ON',
        'ecrv': 'CZK.PRIBOR.3M',
    },
    'CZK.6ML': {
        'dcrv': 'CZK.CZEONIA.ON',
        'ecrv': 'CZK.PRIBOR.6M',
    },
    'ILS.3ML': {
        'dcrv': 'ILS.TELBOR.3M',
        'ecrv': 'ILS.TELBOR.3M',
    }
}

VAL_DATE_CONV_DICT = {
    'USD.OIS': [
        ('2d,BM', 'USD')
    ],
    'USD.1ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+USD.FX'),
    ],
    'USD.3ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+USD.FX'),
    ],
    'USD.6ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+USD.FX'),
    ],
    'USD.3ML.IMM': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+USD.FX'),
    ],
    'CHF.OIS': [
        ('2d,BM', 'CHF'),
    ],
    'CHF.1ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+CHF'),
    ],
    'CHF.3ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+CHF'),
    ],
    'CHF.6ML': [
        ('1d,BF', 'GBP'),
       ('1d,BF', 'GBP+CHF'),
    ],
    'EUR.OIS': [
        ('2d,BF', 'EUR'),
    ],
    'EUR.1ML': [
        ('2d,BF', 'EUR'),
    ],
    'EUR.3ML': [
        ('2d,BF', 'EUR'),
    ],
    'EUR.6ML': [
        ('2d,BF', 'EUR'),
    ],
    'EUR.3ML.IMM': [
        ('2d,BF', 'EUR'),
    ],
    'EUR.6ML.IMM': [
        ('2d,BF', 'EUR'),
    ],
    'GBP.OIS': [
        ('0d,BF', 'GBP'),
    ],
    'GBP.1ML': [
        ('0d,BF', 'GBP'),
    ],
    'GBP.3ML': [
        ('0d,BF', 'GBP'),
    ],
    'GBP.6ML': [
       ('0d,BF', 'GBP'),
    ],
    'GBP.3ML.IMM': [
        ('0d,BF', 'GBP'),
    ],
    'GBP.6ML.IMM': [
        ('0d,BF', 'GBP'),
    ],
    'CAD.OIS': [
        ('0d,BF', 'CAD'),
    ],
    'CAD.3ML': [
        ('0d,BF', 'CAD'),
    ],
    'CAD.3ML.SD': [
        ('0d,BF', 'CAD'),
    ],
    'JPY.OIS': [
        ('2d,BF', 'JPY'),
    ],
    'JPY.1ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+JPY'),
    ],
    'JPY.3ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+JPY'),
    ],
    'JPY.6ML': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+JPY'),
    ],
    'JPY.6ML.SD': [
        ('1d,BF', 'GBP'),
        ('1d,BF', 'GBP+JPY'),
    ],
    'SEK.OIS': [
        ('2d,BF', 'SEK'),
    ],
    'SEK.3ML': [
        ('2d,BF', 'SEK'),
    ],
    'AUD.OIS': [
        ('1d,BF', 'AUD'),
    ],
    'AUD.3ML': [
        ('1d,BF', 'AUD'),
    ],
    'AUD.6ML': [
        ('1d,BF', 'AUD'),
    ],
    'NOK.OIS': [
        ('2d,BF', 'NOK'),
    ],
    'NOK.3ML': [
        ('2d,BF', 'NOK'),
    ],
    'NOK.6ML': [
        ('2d,BF', 'NOK'),
    ],
    'DKK.OIS': [
        ('2d,BF', 'DKK'),
    ],
    'DKK.3ML': [
        ('2d,BF', 'DKK'),
    ],
    'DKK.6ML': [
        ('2d,BF', 'DKK'),
    ],
    'NZD.OIS': [
        ('2d,BF', 'NZD'),
    ],
    'NZD.3ML': [
        ('2d,BF', 'NZD'),
    ],
    'HUF.OIS': [
        ('2d,BF', 'HUF'),
    ],
    'HUF.3ML': [
        ('2d,BF', 'HUF'),
    ],
    'HUF.6ML': [
        ('2d,BF', 'HUF'),
    ],
    'PLN.OIS': [
        ('2d,BF', 'PLN'),
    ],
    'PLN.3ML': [
        ('2d,BF', 'PLN'),
    ],
    'PLN.6ML': [
        ('2d,BF', 'PLN'),
    ],
    'CZK.OIS': [
        ('2d,BF', 'CZK'),
    ],
    'CZK.3ML': [
        ('2d,BF', 'CZK'),
    ],
    'CZK.6ML': [
        ('2d,BF', 'CZK'),
    ],
    'ILS.3ML': [
        ('2d,BF', 'ILS')
    ],
    'USD.OV1': [
        ('2d,BF', 'USD')
    ],
    'USD.OV3': [
        ('2d,BF', 'USD')
    ],
    'USD.OV6': [
        ('2d,BF', 'USD')
    ],
    'USD.1V3': [
        ('2d,BF', 'USD')
    ],
    'USD.1V6': [
        ('2d,BF', 'USD')
    ],
    'USD.3V6': [
        ('2d,BF', 'USD')
    ],
    'GBP.OV1': [
        ('0d,BF', 'GBP')
    ],
    'GBP.OV3': [
        ('0d,BF', 'GBP')
    ],
    'GBP.OV6': [
        ('0d,BF', 'GBP')
    ],
    'GBP.1V3': [
        ('0d,BF', 'GBP')
   ],
    'GBP.1V6': [
        ('0d,BF', 'GBP')
    ],
    'GBP.3V6': [
        ('0d,BF', 'GBP')
    ],
}

EXP_DATE_CONV_DICT = {
    'USD.OIS': ('M', 'USD'),
    'USD.1ML': ('M', 'GBP+USD'),
    'USD.3ML': ('M', 'GBP+USD'),
    'USD.6ML': ('M', 'GBP+USD'),
    'USD.3ML.IMM': ('M', 'GBP+USD'),
    'CHF.OIS': ('M', 'USD'),
    'CHF.1ML': ('M', 'GBP+CHF'),
    'CHF.3ML': ('M', 'GBP+CHF'),
    'CHF.6ML': ('M', 'GBP+CHF'),
    'EUR.OIS': ('M', 'EUR'),
    'EUR.1ML': ('M', 'EUR'),
    'EUR.3ML': ('M', 'EUR'),
    'EUR.6ML': ('M', 'EUR'),
    'EUR.3ML.IMM': ('M', 'EUR'),
    'EUR.6ML.IMM': ('M', 'EUR'),
    'GBP.OIS': ('M', 'GBP'),
    'GBP.1ML': ('M', 'GBP'),
    'GBP.3ML': ('M', 'GBP'),
    'GBP.6ML': ('M', 'GBP'),
    'GBP.3ML.IMM': ('M', 'GBP'),
    'GBP.6ML.IMM': ('M', 'GBP'),
    'CAD.3ML': ('M', 'CAD'),
    'CAD.3ML.SD': ('M', 'CAD'),
    'JPY.6ML': ('M', 'GBP+JPY'),
    'JPY.6ML.SD': ('M', 'GBP+JPY'),
}

YCF_CAL_DICT = {
    'USD.OIS': 'USD',
    'USD.1ML': 'USD',
    'USD.3ML': 'USD',
    'USD.6ML': 'USD',
    'USD.3ML.IMM': 'USD',
    'CHF.OIS': 'CHF',
    'CHF.1ML': 'CHF',
    'CHF.3ML': 'CHF',
    'CHF.6ML': 'CHF',
    'EUR.OIS': 'EUR',
    'EUR.1ML': 'EUR',
    'EUR.3ML': 'EUR',
    'EUR.6ML': 'EUR',
    'EUR.6ML.IMM': 'EUR',
    'GBP.OIS': 'GBP',
    'GBP.1ML': 'GBP',
    'GBP.3ML': 'GBP',
    'GBP.6ML': 'GBP',
    'GBP.6ML.IMM': 'GBP',
    'CAD.3ML': 'CAD',
    'CAD.3ML.SD': 'CAD',
    'JPY.6ML': 'JPY',
    'JPY.6ML.SD': 'JPY',
}


BASIS_CONV_DICT = {
    'USD.OV1': {
        'base_curve': 'USD.1ML',
        'base_float_interval': '1M',
        'base_float_rate_interval': '1M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.OIS',
        'spd_float_interval': '1M',
        'spd_float_rate_interval': '1D',
        'spd_float_dcc': 'act/360',
    },
    'USD.OV3': {
        'base_curve': 'USD.3ML',
        'base_float_interval': '3M',
        'base_float_rate_interval': '3M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.OIS',
        'spd_float_interval': '3M',
        'spd_float_rate_interval': '1D',
        'spd_float_dcc': 'act/360',
    },
    'USD.OV6': {
        'base_curve': 'USD.6ML',
        'base_float_interval': '6M',
        'base_float_rate_interval': '6M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.OIS',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '1D',
        'spd_float_dcc': 'act/360',
    },
    'USD.1V3': {
        'base_curve': 'USD.3ML',
        'base_float_interval': '3M',
        'base_float_rate_interval': '3M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.1ML',
        'spd_float_interval': '3M',
        'spd_float_rate_interval': '1M',
        'spd_float_dcc': 'act/360',
    },
    'USD.1V6': {
        'base_curve': 'USD.6ML',
        'base_float_interval': '6M',
        'base_float_rate_interval': '6M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.1ML',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '1M',
        'spd_float_dcc': 'act/360',
    },
    'USD.3V6': {
        'base_curve': 'USD.6ML',
        'base_float_interval': '6M',
        'base_float_rate_interval': '6M',
        'base_float_dcc': 'act/360',
        'spd_curve': 'USD.3ML',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '3M',
        'spd_float_dcc': 'act/360',
    },
    'GBP.OV1': {
        'base_curve': 'GBP.OIS',
        'base_float_interval': '1M',
        'base_float_rate_interval': '1D',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.1ML',
        'spd_float_interval': '1M',
        'spd_float_rate_interval': '1M',
        'spd_float_dcc': 'act/365F',
    },
    'GBP.OV3': {
        'base_curve': 'GBP.OIS',
        'base_float_interval': '1M',
        'base_float_rate_interval': '1D',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.3ML',
        'spd_float_interval': '3M',
        'spd_float_rate_interval': '3M',
        'spd_float_dcc': 'act/365F',
    },
    'GBP.OV6': {
        'base_curve': 'GBP.OIS',
        'base_float_interval': '1M',
        'base_float_rate_interval': '1D',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.6ML',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '6M',
        'spd_float_dcc': 'act/365F',
    },
    'GBP.1V3': {
        'base_curve': 'GBP.3ML',
        'base_float_interval': '3M',
        'base_float_rate_interval': '3M',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.1ML',
        'spd_float_interval': '3M',
        'spd_float_rate_interval': '1M',
        'spd_float_dcc': 'act/365F',
    },
    'GBP.1V6': {
        'base_curve': 'GBP.6ML',
        'base_float_interval': '6M',
        'base_float_rate_interval': '6M',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.1ML',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '1M',
        'spd_float_dcc': 'act/365F',
    },
    'GBP.3V6': {
        'base_curve': 'GBP.6ML',
        'base_float_interval': '6M',
        'base_float_rate_interval': '6M',
        'base_float_dcc': 'act/365F',
        'spd_curve': 'GBP.3ML',
        'spd_float_interval': '6M',
        'spd_float_rate_interval': '3M',
        'spd_float_dcc': 'act/365F',
    },

}

 



swap_pricing_example
import datetime as dt

from panormus.markable.loader import loader
from panormus.markable import irs as irs_mble
from panormus.quant.alib import conventions as quant_conventions
from panormus.quant.alib import utils as quant_utils
from panormus.trade import irs as irs_trade
import pandas as pd

curve_names_dictionary = quant_conventions.CURVE_NAMES_DICT

curve_location = 'ny'  # PvK, please switch to lon

holiday_oracle = quant_utils.Holidays()
print (holiday_oracle)
curve_loader = loader.CurveLoaderAlib(curve_location)
fixings_loader = loader.FixingsLoaderOpenData()

history_start_date = dt.date(2019, 1, 1)
history_end_date = dt.date(2019, 6, 11)

## This generates a list of trading days between two dates
trading_days = quant_utils.trading_date_list(history_start_date, history_end_date, holiday_oracle['GBP+USD.FX'])

## main example
swap_convention = 'USD.3ML'  ## could try USD.OIS or EUR.6ML

dcrv_name = curve_names_dictionary[swap_convention]['dcrv']
ecrv_name = curve_names_dictionary[swap_convention]['ecrv']

## 5Y5Y swap
swap_notional = 10000000

# get the start date of a swap
# get the end date of a swap
start_date, end_date = quant_utils.dates_from_trade_date(trading_days[0], '5y', '5y', swap_convention, holiday_oracle)
print ('start date,end date', start_date,end_date)
## need curves for today to get the par rate from the first trading day
# dcrv: discount curve
dcrv_today = curve_loader.get_curve(dcrv_name, trading_days[0])
print (trading_days[0])
print (dcrv_today)
# ecrv: projection curve
ecrv_today = curve_loader.get_curve(ecrv_name, trading_days[0])
print (ecrv_today)

coupon = quant_utils.swap_rate_with_conv(
    dcrv_today, start_date, end_date, 1, 0, ecrv_today, 0, 0, holiday_oracle, swap_convention)
print (coupon)

swap_trade = irs_trade.IRSTrade(
    1, swap_convention, start_date, end_date, coupon, 'p', holiday_oracle, notional=swap_notional)
print (swap_trade)
mble_list = [
    irs_mble.IRS(swap_trade, d, holiday_oracle, curve_loader, fixings_loader, curve_names_dictionary) for d intrading_days]
print (mble_list)
pv_list = [m.market_value() for m in mble_list]
print (pv_list)


 

panormus-trade-irs.py



from copy import deepcopy

from panormus.quant.alib.conventions import CONV_DICT
import panormus.quant.alib.utils as qau
import panormus.trade.base as aab


class IRSTrade(aab.Trade):
    '''
    Standard fixed-float interest rate swap
    '''

    def __eq__(self, other):
        if isinstance(other, IRSTrade):
            return all([
                self.swap_conv == other.swap_conv,
                self.ccy == other.ccy,
                self.start_date == other.start_date,
                self.end_date == other.end_date,
                self.coupon == other.coupon,
                self.payrec == other.payrec,
                self.notional == other.notional,
                self.float_leg_spread == other.float_leg_spread
            ])
        else:
            return False

    def __init__(
            self, trade_id,
            swap_conv, start_date, end_date,
            coupon, payrec_fixed,
            holiday_oracle, swap_convention_dictionary=None,
            notional=1.0, float_leg_spread=0.0, oRide_fix_ivl=None, oRide_fix_dcc=None
    ):
        '''
        :param trade_id:
        :param swap_conv:
        :param start_date:
        :param end_date:
        :param coupon:
        :param payrec_fixed:
        :param holiday_oracle:
        :param dict|None swap_convention_dictionary: if none, defaults to standard conventions
        :param notional:
        :param float_leg_spread:
        '''
        super().__init__(trade_id)
        if swap_convention_dictionary is None:
            swap_conv_dict = deepcopy(CONV_DICT)
        else:
            swap_conv_dict = deepcopy(swap_convention_dictionary)

        self.swap_conv = swap_conv
        self.ccy = self.swap_conv[:3].lower()
        self.start_date = start_date
        self.end_date = end_date
        self.coupon = coupon
        self.payrec = payrec_fixed[0].lower()
        self.notional = notional
        self.float_leg_spread = float_leg_spread

        if oRide_fix_ivl is not None:
            swap_conv_dict[self.swap_conv]['fixed_interval'] = oRide_fix_ivl
        if oRide_fix_dcc is not None:
            swap_conv_dict[self.swap_conv]['fixed_dcc'] = oRide_fix_dcc

        self.swap_convention_dictionary = swap_conv_dict

        self.holiday_oracle = holiday_oracle
        self.fixed_dcc = self.swap_convention_dictionary[self.swap_conv]['fixed_dcc']
        self.float_dcc = self.swap_convention_dictionary[self.swap_conv]['float_dcc']
        self.fixed_ivl = self.swap_convention_dictionary[self.swap_conv]['fixed_interval']
       self.float_ivl = self.swap_convention_dictionary[self.swap_conv]['float_interval']
        self.float_rate_ivl = self.swap_convention_dictionary[self.swap_conv]['float_rate_interval']
        self.stub_method = self.swap_convention_dictionary[self.swap_conv]['stub_method']
        self.accrual_bdc = self.swap_convention_dictionary[self.swap_conv]['accrual_bad_day_conv']
        self.pay_bdc = self.swap_convention_dictionary[self.swap_conv]['pay_bad_day_conv']
        self.reset_bdc = self.swap_convention_dictionary[self.swap_conv]['reset_bad_day_conv']
        self.principal_init_flag = self.swap_convention_dictionary[self.swap_conv]['principal_initial_flag']
        self.principal_final_flag = self.swap_convention_dictionary[self.swap_conv]['principal_final_flag']
        self.holiday_calendar_name = self.swap_convention_dictionary[self.swap_conv]['holiday_calendar_name']

        self._schedule_done = False
        self._fixed_leg_acc_start_dates = None
        self._fixed_leg_acc_end_dates = None
        self._fixed_leg_pay_dates = None
        self._fixed_leg_dccs = None
        self._fixed_leg_stub_loc = None

        self._float_leg_acc_start_dates = None
        self._float_leg_acc_end_dates = None
        self._float_leg_reset_dates = None
        self._float_leg_pay_dates = None
        self._float_leg_dccs = None
        self._float_rate_end_dates = None
        self._float_rate_stub_loc = None

    def fixed_leg_acc_start_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._fixed_leg_acc_start_dates

    def fixed_leg_acc_end_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._fixed_leg_acc_end_dates

    def fixed_leg_pay_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._fixed_leg_pay_dates

    def fixed_leg_dccs(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._fixed_leg_dccs

    def float_leg_acc_start_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_leg_acc_start_dates

    def float_leg_acc_end_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_leg_acc_end_dates

    def float_leg_reset_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_leg_reset_dates

    def float_leg_pay_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_leg_pay_dates

    def float_leg_dccs(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_leg_dccs

    def float_rate_end_dates(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_rate_end_dates

    def float_rate_stub(self):
        if not self._schedule_done:
            self.generate_schedule()
        return self._float_rate_stub_loc

    def generate_schedule(self):
        self._schedule_done = True
        swap_date_dicts = qau.swap_dates_from_conv(
            self.start_date, self.end_date,
            self.holiday_oracle,
            self.swap_conv,
            self.swap_convention_dictionary)

        self._fixed_leg_acc_start_dates = swap_date_dicts['fix_leg_dates']['acc_start_dates']
        self._fixed_leg_acc_end_dates = swap_date_dicts['fix_leg_dates']['acc_end_dates']
        self._fixed_leg_pay_dates = swap_date_dicts['fix_leg_dates']['payment_dates']
        self._fixed_leg_dccs = swap_date_dicts['fix_leg_dates']['dccs']
        self._fixed_leg_stub_loc = swap_date_dicts['fix_leg_dates']['stub']

        self._float_leg_acc_start_dates = swap_date_dicts['flt_leg_dates']['acc_start_dates']
        self._float_leg_acc_end_dates = swap_date_dicts['flt_leg_dates']['acc_end_dates']
        self._float_leg_reset_dates = swap_date_dicts['flt_leg_dates']['reset_dates']
        self._float_leg_pay_dates = swap_date_dicts['flt_leg_dates']['payment_dates']
        self._float_leg_dccs = swap_date_dicts['flt_leg_dates']['dccs']
        self._float_rate_end_dates = swap_date_dicts['flt_leg_dates']['rate_end_dates']
        self._float_rate_stub_loc = swap_date_dicts['flt_leg_dates']['stub']

    @staticmethod
    def build_roll_details_from_trade_string(trade_string):
        trade_properties = trade_string.lower().split('.')
        return {
            'start_term': trade_properties[4],
            'end_term': trade_properties[5],
            'conv_str': '.'.join([trade_properties[2], trade_properties[3]]).upper(),
            'pay_rec': trade_properties[6]
        }

 

panormus-makable-loader-loader.py



import datetime as dt
import math

import pandas as pd
import numpy as np
from scipy import interpolate as intrp

import panormus.data.open_data as od
import panormus.data.open_data_config as odc
from panormus.markable.loader.config.fixings_config import FIXINGS_OPENDATA_CONFIG as FODC
import panormus.markable.loader.loader_utils as blu
import panormus.quant.alib.utils as qau
import panormus.quant.vol_surface as qvs
import panormus.quant.fx_linear_config as fxlc

from panormus.data import cax
from panormus.config.settings import ALIB_CURVE_REGION
from panormus.utils.cache import (cache, cache_response, cache_put, clear_cache)


class BaseLoader:
    '''
    Base loader class
    '''

    def __init__(self):
        pass


class FixingsLoader(BaseLoader):
    '''
    Base class for interest rate fixings loader
    '''

    def __init__(self):
        super().__init__()


class FixingsLoaderOpenData(FixingsLoader):
    '''
    Loads interest rate fixings from open data
    '''

    cache_fixing_name = 'FixingsLoaderOpenData.get_fixing'
    cache_fixing_region = 'mem_10h'

    def __init__(self):
        super().__init__()

    @cache_response(cache_fixing_name, cache_fixing_region, True)
    def get_fixing(self, conv_name, date):
        ticker = FODC[conv_name]
        return od.df_for_observable_strings([ticker], start_date=date, end_date=date).values[0, 0]

    def get_fixings(self, conv_name, sdate, edate):
        ticker = FODC[conv_name]
        f = od.df_for_observable_strings([ticker], start_date=sdate, end_date=edate)
        f.columns = ['fix']
        f.index = pd.to_datetime(f.index).date
        return f

    def cache_fixing(self, fixing, conv_name, date):
        cache_put(self.cache_fixing_name, self.cache_fixing_region, fixing, conv_name, date)


class FxVolSurfaceLoader(BaseLoader):
    def __init__(self, holiday_oracle=None):
        super().__init__()
        self.holiday_oracle = qau.Holidays() if holiday_oracle is None else holiday_oracle

        self.COLS_TO_DROP = [
            'Date', 'FLY01', 'FLY10', 'FLY20', 'FLY30', 'FLY40', 'Live?', 'Pair', 'RR01', 'RR10', 'RR20', 'RR30',
            'RR40', 'Tenor']

        self.DELTA_NAMES = ['ATM', 'c01', 'c10', 'c20', 'c30', 'c40', 'p01', 'p10', 'p20', 'p30', 'p40']

    def get_fxvolsurface(self, pair, asof_date):
        def build_smile(row):
            strikes = [row[f'{delta_name} stk'] for delta_name in self.DELTA_NAMES]
            vols = [row[f'{delta_name} vol'] / 100.0 for delta_name in self.DELTA_NAMES]

            strikes, vols = (list(t) for t in zip(*sorted(zip(strikes, vols))))

            return qvs.StrikeInterpSmile(strikes, vols)

        holsname = f'{pair[:3].upper()}+{pair[3:].upper()}'
        hol_file = self.holiday_oracle[holsname]

        cax_fn = 'getvolsurface'
        cax_datatype = 'vols'
        cax_ticker = pair.upper() + ' FXV'
        cax_fields = None
        cax_filter = 'source=BAML'

        data_df = cax.cax_df(
            cax_fn, cax_datatype, [cax_ticker], [cax_fields], start_date=asof_date, end_date=asof_date,
            options=cax_filter)

        if 'previous_close' in data_df['Live?'].tolist():
            data_df = data_df[data_df['Live?'] == 'previous_close']
            min_date = min(data_df['Date'].tolist())
            data_df = data_df.set_index('Date').loc[min_date, :].reset_index().set_index('Maturity')
        else:
            data_df = data_df[data_df['Live?'] == 'latest_close'].set_index('Maturity')
        data_df.index = pd.to_datetime(data_df.index)
        data_df = data_df.drop(self.COLS_TO_DROP, axis=1)
        data_df['smiles'] = data_df.apply(build_smile, axis=1)

        smiles = data_df['smiles'].tolist()
        exp_ycfs = [qau.ac_bus_days_diff(asof_date, dte, hol_file) / 251. for dte in data_df.index]
       exp_ycfs, smiles = (list(t) for t in zip(*sorted(zip(exp_ycfs, smiles))))

        return qvs.FxVolSurface(asof_date, exp_ycfs, smiles)


class VolSurfaceLoader(BaseLoader):
    def __init__(self):
        super().__init__()

    def get_vol_surface(self, id, date):
        raise NotImplementedError

    def bulk_load_vol_surfaces(self, id, date_list):
        raise NotImplementedError


class EquityVolSurfaceLoader(VolSurfaceLoader):
    def __init__(self):
        super().__init__()
        self.fwd_money = [0.3 + 0.025 * i for i in range(80)]

    cache_vol_surf_name = 'EquityVolSurfaceLoader.get_vol_surface'
    cache_vol_surf_region_temp = 'mem_10h'
    cache_vol_surf_region_perm = 'disk'

    def get_cax_df(self, id, start_date, end_date, source='SOCGEN'):
        cax_fn = 'getvolsurface'
        cax_datatype = 'vols'
        cax_ticker = id.upper()
        cax_fields = None
        cax_filter = 'source=' + source

        data_df = cax.cax_df(
            cax_fn, cax_datatype, [cax_ticker], [cax_fields], start_date, end_date,
            options=cax_filter)
        return data_df

    def convert_cax_df_to_vol_surface(self, data_df, date):

        if data_df.empty:
            return None

        spot = float(data_df.Spot.unique()[0])

        data_df['Maturity_Date'] = pd.to_datetime(data_df.Maturity)

        data_df['term'] = data_df.apply(
            lambda row: float((row['Maturity_Date'] - row['Date']).days) / 365.0, axis=1)

        terms = sorted(data_df['term'].tolist())

        fwd_factors = data_df['Forward Factor'].tolist()

        moneyness_col = [col for col in data_df.columns if col.replace('.', '').isdigit()]
        strikes = np.asarray([float(mon) * spot for mon in moneyness_col])

        smiles = []
        for item in range(len(terms)):
            vols = data_df[data_df.term == terms[item]][moneyness_col].iloc[0].values
            newvolsfunc = intrp.interp1d(x=strikes, y=vols,
                                         bounds_error=False, fill_value=(
                    vols[0], vols[-1]))
            fwd = float(data_df[data_df.term == terms[item]]['Forward'].unique()[0])
            fwdstrikes = [mon * fwd for mon in self.fwd_money]
            fwdvols = newvolsfunc(fwdstrikes)
            smiles.append(qvs.EquityStrikeInterpSmile(self.fwd_money, fwdvols, fwd))

        return qvs.EquityVolSurface(date, terms, smiles, fwd_factors, spot)

    @cache_response(cache_vol_surf_name, cache_vol_surf_region_temp, True)
    @cache_response(cache_vol_surf_name, cache_vol_surf_region_perm, True)
    def get_vol_surface(self, id, date):

        data_df = self.get_cax_df(id, date, date)
        return self.convert_cax_df_to_vol_surface(data_df, date)

    def bulk_load_vol_surfaces(self, id, date_list):

        start_date = min(date_list)
        end_date = max(date_list)

        data_dfs = self.get_cax_df(id, start_date, end_date)
        trading_date_list = set(data_dfs['Date'])

        for d in trading_date_list:
            data_df = data_dfs[data_dfs['Date'] == d]
            vol_surface = self.convert_cax_df_to_vol_surface(data_df, d)
            cache_put(self.cache_vol_surf_name, self.cache_vol_surf_region_temp, vol_surface, id, d)
            cache_put(self.cache_vol_surf_name, self.cache_vol_surf_region_perm, vol_surface, id, d)


class IrVolCubeLoader(BaseLoader):
    '''
    Base class for loading ir volcubes
    '''

    cache_vol_cube_name = 'IrVolCubeLoader.get_vol_cube'
    cache_vol_cube_region_temp = 'mem_10h'
    cache_vol_cube_region_perm = 'disk'

    def __init__(self):
        super().__init__()

    def get_vol_cube(self, curve_handle, date):
        raise NotImplementedError

    def bulk_load_vol_cubes(self, curve_handle, date_list):
        raise NotImplementedError

    def clear_vol_cube_temp_cache(self):
        clear_cache(self.cache_vol_cube_name, self.cache_vol_cube_region_temp)

    def clear_vol_cube_perm_cache(self):
        clear_cache(self.cache_vol_cube_name, self.cache_vol_cube_region_perm)


class IrVolCubeLoaderStatic(IrVolCubeLoader):
    '''
    Class for loading volcubes from explicit input data
    '''

    def __init__(
            self, holiday_oracle=qau.Holidays()
    ):
        super().__init__()
        self.holiday_oracle = holiday_oracle
        self.cache = {}

    def get_vol_cube(self, curve_handle, date):
        return self.cache.get((curve_handle, date), None)

    def cache_vol_cube(
            self, curve_handle, date, expiry_list, tenor_list,
            vol_grid, beta_grid, rho_grid, volvol_grid, shift_grid
    ):
        res = qau.swaptions_exp_and_tenor_ycf(date, curve_handle, self.holiday_oracle, expiry_list, tenor_list)
        expiry_bus251_ycfs = res['exp_ycfs']
        tenors = res['tenor_ycfs']

        self.cache[(curve_handle, date)] = qvs.SabrIRVolCube(
            date, curve_handle, self.holiday_oracle, expiry_bus251_ycfs, tenors, vol_grid,
            beta_grid, rho_grid, volvol_grid, shift_grid
        )


class IrVolCubeLoaderOpenData(IrVolCubeLoader):
    '''
    Class for loading ir volcubes from opendata
    '''
    cache_vol_cube_name = 'IrVolCubeLoaderOpenData.get_vol_cube'
    cache_vol_cube_region_temp = 'mem_10h'
    cache_vol_cube_region_perm = 'disk'

    def __init__(
            self, holiday_oracle, fit_name='barc_fit'
    ):
        super().__init__()
        self.holiday_oracle = holiday_oracle
        self.fit_name = fit_name

    def reorder_and_merge_data_df(self, merge_df, data_df, item_name, tenor_list, expiry_list):
        data_grid_df = merge_df.set_index(item_name).join(
            data_df.set_index('item')).reset_index().drop(
            [item_name, 'ref_item', 'cut', 'source', 'attribute', 'timestamp'], axis=1
        ).pivot(index='exp', columns='ten', values='value')
        return data_grid_df[tenor_list].reindex(expiry_list)

    def vol_conversion_factor(self, od_swtn_str, asof_date, curve_handle, holidays, convert_to_bus251=True):
        expiry_str = od_swtn_str.split('_')[-4]
        expiry_date = qau.expiry_date_from_trade_date(asof_date, expiry_str, curve_handle, holidays)
        bus_tte = qau.bus_ycf(asof_date, expiry_date, curve_handle, holidays)
        act_tte = qau.ac_day_cnt_frac(asof_date, expiry_date, 'ACT/365F')

        if convert_to_bus251:
            return math.sqrt(act_tte / bus_tte)
        else:
            return math.sqrt(bus_tte / act_tte)

    @cache_response(cache_vol_cube_name, cache_vol_cube_region_temp, True)
    @cache_response(cache_vol_cube_name, cache_vol_cube_region_perm, True)
    def get_vol_cube(self, curve_handle, date, extrap_sabr=False, atm_grid_source='icap'):
        swtn_prefix = odc.OD_SWAPTION_CORE_DICT[curve_handle]['item_prefix']
        cut = odc.OD_SWAPTION_CORE_DICT[curve_handle]['cut']
        holiday_oracle = qau.Holidays()
        if atm_grid_source == 'barcap_rates_grp_clean':
            expiry_list = odc.BARCAP_SWAPTION_FILE_DICT[curve_handle]['expiries']
            tenor_list_raw = odc.BARCAP_SWAPTION_FILE_DICT[curve_handle]['tenors']
            vol_attr = 'normvol_bus251'
       else:
            expiry_list = odc.ICAP_SWAPTION_GRID_DICT[curve_handle]['expiries']
            tenor_list_raw = odc.ICAP_SWAPTION_GRID_DICT[curve_handle]['tenors']
            vol_attr = 'nvol'  # Convert this to bizday vol below

        tenor_list = []
        for t in tenor_list_raw:
            if t[-1].lower() == 'y':
                tenor_list.append(t)

        res = qau.swaptions_exp_and_tenor_ycf(date, curve_handle, holiday_oracle, expiry_list, tenor_list)
        expiry_bus251_ycfs = res['exp_ycfs']
        tenors = res['tenor_ycfs']

        otl_vol = []
        otl_beta = []
        otl_rho = []
        otl_volvol = []
        otl_shift = []

        merge_data_list = []
        for expiry in expiry_list:
            for tenor in tenor_list:
                vol_item = '_'.join([swtn_prefix, expiry, '0d', tenor, 'ao0'])
                ref_item = '_'.join([swtn_prefix, expiry, '0d', tenor, 'ref'])
                merge_data_list.append([expiry, tenor, vol_item, ref_item])
                otl_vol.append((vol_item, vol_attr, cut, atm_grid_source))
                otl_beta.append((ref_item, 'beta_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_rho.append((ref_item, 'rho_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_volvol.append((ref_item, 'volvol_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_shift.append((ref_item, 'shift_%s' % self.fit_name, cut, 'rates_grp_clean'))

        if extrap_sabr:
            extrap_expiry_list = [x for x in ['15y', '20y', '30y'] if x not in expiry_list]
            full_expiry_list = expiry_list + extrap_expiry_list
            for expiry in extrap_expiry_list:

                exp_date = qau.expiry_date_from_trade_date(date, expiry, curve_handle, holiday_oracle)
                expiry_bus251_ycfs.append(qau.bus_ycf(date, exp_date, curve_handle, holiday_oracle))

                for tenor in tenor_list:
                    vol_item = '_'.join([swtn_prefix, expiry, '0d', tenor, 'ao0'])
                    merge_data_list.append([expiry, tenor, vol_item, ''])
                    otl_vol.append((vol_item, 'nvol', cut, 'icap'))

        merge_data_df = pd.DataFrame(merge_data_list[:len(expiry_list) * len(tenor_list)],
                                     columns=['exp', 'ten', 'vol_item', 'ref_item'])

        # TODO: This should be one DB hit
        # Get OD data
        vol_df_raw = od.df_for_observable_tuples(
            otl_vol, start_date=date, end_date=date, return_records=True
        )
        # Convert busday vols
        vol_df_raw.loc[:, 'value'] = vol_df_raw.apply(
            lambda row: self.vol_conversion_factor(
                row['item'], date, curve_handle, holiday_oracle) * row['value']
            if row['source'] == 'icap' else row['value'], axis=1)
        beta_df_raw = od.df_for_observable_tuples(
            otl_beta, start_date=date, end_date=date, return_records=True
        )
        rho_df_raw = od.df_for_observable_tuples(
            otl_rho, start_date=date, end_date=date, return_records=True
        )
        volvol_df_raw = od.df_for_observable_tuples(
            otl_volvol, start_date=date, end_date=date, return_records=True
        )
        shift_df_raw = od.df_for_observable_tuples(
            otl_shift, start_date=date, end_date=date, return_records=True
        )

        # Reformat data
        if extrap_sabr:
            merge_vol_data_df = pd.DataFrame(merge_data_list, columns=['exp', 'ten', 'vol_item', 'ref_item'])
            vol_grid_df = self.reorder_and_merge_data_df(
                merge_df=merge_vol_data_df, data_df=vol_df_raw, item_name='vol_item', tenor_list=tenor_list,
                expiry_list=full_expiry_list
            )
            # interpolating the missing 12yr tenor for long-dated swaptions from icap vols
            for expiry in extrap_expiry_list:
                good_tenors = [x for i, x in enumerate(tenors) if i != tenor_list.index('12y')]
                good_vols = vol_grid_df.loc[expiry, vol_grid_df.columns != '12y'].tolist()
                vol_grid_df.loc[expiry, '12y'] = np.interp(tenors[tenor_list.index('12y')],
                                                           good_tenors, good_vols)

        else:
            vol_grid_df = self.reorder_and_merge_data_df(
                merge_df=merge_data_df, data_df=vol_df_raw, item_name='vol_item', tenor_list=tenor_list,
                expiry_list=expiry_list
            )

        beta_grid_df = self.reorder_and_merge_data_df(
            merge_df=merge_data_df, data_df=beta_df_raw, item_name='ref_item', tenor_list=tenor_list,
            expiry_list=expiry_list
        )
        rho_grid_df = self.reorder_and_merge_data_df(
            merge_df=merge_data_df, data_df=rho_df_raw, item_name='ref_item', tenor_list=tenor_list,
            expiry_list=expiry_list
        )
        volvol_grid_df = self.reorder_and_merge_data_df(
            merge_df=merge_data_df, data_df=volvol_df_raw, item_name='ref_item', tenor_list=tenor_list,
            expiry_list=expiry_list
        )
        shift_grid_df = self.reorder_and_merge_data_df(
            merge_df=merge_data_df, data_df=shift_df_raw, item_name='ref_item', tenor_list=tenor_list,
            expiry_list=expiry_list
        )

        if extrap_sabr:
            expiry_dict = dict(zip(list(range(0, len(full_expiry_list))), full_expiry_list))

            beta_grid_df = beta_grid_df.append([beta_grid_df[-1:]] * len(extrap_expiry_list), ignore_index=True)
            beta_grid_df.index = beta_grid_df.index.to_series().map(expiry_dict)

            rho_grid_df = rho_grid_df.append([rho_grid_df[-1:]] * len(extrap_expiry_list), ignore_index=True)
            rho_grid_df.index = rho_grid_df.index.to_series().map(expiry_dict)

            volvol_grid_df = volvol_grid_df.append([volvol_grid_df[-1:]] * len(extrap_expiry_list), ignore_index=True)
            volvol_grid_df.index = volvol_grid_df.index.to_series().map(expiry_dict)

            shift_grid_df = shift_grid_df.append([shift_grid_df[-1:]] * len(extrap_expiry_list), ignore_index=True)
            shift_grid_df.index = shift_grid_df.index.to_series().map(expiry_dict)

        return qvs.SabrIRVolCube(
            date, curve_handle, holiday_oracle, expiry_bus251_ycfs, tenors, vol_grid_df.values,
            beta_grid_df.values, rho_grid_df.values, volvol_grid_df.values, shift_grid_df.values
        )

    def bulk_load_vol_cubes(self, curve_handle, date_list, write_perm=True):
        '''
        :param date_list: [start_date, end_date]
        :param curve_handle: str like USD.3ML, UR.6ML, GBP.6ML
        :param bool write_perm: cache to disk as well as memory?
        :return: dictionary of _vol cubes
        '''
        holiday_oracle = qau.Holidays()

        start_date = min(date_list)
        end_date = max(date_list)
        trading_date_list = date_list

        swtn_prefix = odc.OD_SWAPTION_CORE_DICT[curve_handle]['item_prefix']
        cut = odc.OD_SWAPTION_CORE_DICT[curve_handle]['cut']

        expiry_list = odc.ICAP_SWAPTION_GRID_DICT[curve_handle]['expiries']
        tenor_list_raw = odc.ICAP_SWAPTION_GRID_DICT[curve_handle]['tenors']

        tenor_list = []
        for t in tenor_list_raw:
            if t[-1].lower() == 'y':
                tenor_list.append(t.lower())

        otl_vol = []
        otl_beta = []
        otl_rho = []
        otl_volvol = []
        otl_shift = []
        merge_data_list = []
        for expiry in expiry_list:
            for tenor in tenor_list:
                vol_item = '_'.join([swtn_prefix, expiry, '0d', tenor, 'ao0'])
                ref_item = '_'.join([swtn_prefix, expiry, '0d', tenor, 'ref'])
                merge_data_list.append([expiry, tenor, vol_item, ref_item])
                # Pull calday vol and adjust farther down.
                otl_vol.append((vol_item, 'nvol', cut, 'icap'))
                otl_beta.append((ref_item, 'beta_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_rho.append((ref_item, 'rho_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_volvol.append((ref_item, 'volvol_%s' % self.fit_name, cut, 'rates_grp_clean'))
                otl_shift.append((ref_item, 'shift_%s' % self.fit_name, cut, 'rates_grp_clean'))

        merge_data_df = pd.DataFrame(merge_data_list, columns=['exp', 'ten', 'vol_item', 'ref_item'])

        ## TODO: this should be one db hit
        vol_df_raw = od.df_for_observable_tuples(otl_vol, start_date, end_date, return_records=True)
        # Adjust calday vols to bizday
        vol_df_raw.loc[:, 'value'] = vol_df_raw.apply(
            lambda row: self.vol_conversion_factor(
                row['item'], row['timestamp'].date(), curve_handle, holiday_oracle) * row['value'],
            axis=1)

        beta_df_raw = od.df_for_observable_tuples(otl_beta, start_date, end_date, return_records=True)
        rho_df_raw = od.df_for_observable_tuples(otl_rho, start_date, end_date, return_records=True)
        volvol_df_raw = od.df_for_observable_tuples(otl_volvol, start_date, end_date, return_records=True)
        shift_df_raw = od.df_for_observable_tuples(otl_shift, start_date, end_date, return_records=True)

        for d in trading_date_list:
            res = qau.swaptions_exp_and_tenor_ycf(d, curve_handle, self.holiday_oracle, expiry_list, tenor_list)
            expiry_ycfs = res['exp_ycfs']
            tenor_ycfs = res['tenor_ycfs']
            vol_grid = self.reorder_and_merge_data_df(
                merge_data_df, vol_df_raw[vol_df_raw['timestamp'] == pd.to_datetime(d, utc=True)], 'vol_item',
                tenor_list, expiry_list
            ).values
            beta_grid = self.reorder_and_merge_data_df(
                merge_data_df, beta_df_raw[beta_df_raw['timestamp'] == pd.to_datetime(d, utc=True)],
                'ref_item', tenor_list, expiry_list
            ).values
            rho_grid = self.reorder_and_merge_data_df(
                merge_data_df, rho_df_raw[rho_df_raw['timestamp'] == pd.to_datetime(d, utc=True)], 'ref_item',
                tenor_list, expiry_list
            ).values
            volvol_grid = self.reorder_and_merge_data_df(
                merge_data_df, volvol_df_raw[volvol_df_raw['timestamp'] == pd.to_datetime(d, utc=True)], 'ref_item',
                tenor_list, expiry_list
            ).values
            shift_grid = self.reorder_and_merge_data_df(
                merge_data_df, shift_df_raw[shift_df_raw['timestamp'] == pd.to_datetime(d, utc=True)], 'ref_item',
                tenor_list, expiry_list
            ).values
            vs = qvs.SabrIRVolCube(
                d, curve_handle, self.holiday_oracle, expiry_ycfs, tenor_ycfs, vol_grid, beta_grid, rho_grid,
                volvol_grid, shift_grid
            )
            # insert into cache such that we mimic the single loader (get_vol_cube).
            cache_put(self.cache_vol_cube_name, self.cache_vol_cube_region_temp, vs, curve_handle, d)
            if write_perm:
                cache_put(self.cache_vol_cube_name, self.cache_vol_cube_region_perm, vs, curve_handle, d)

    def get_vols(self, curve_handle, date_list, fwd_list, strike_list, exp_date_list, tenor_list):
        '''
        All list arguments must have the same length!
        '''
        return [
            self.get_vol_cube(curve_handle, date).get_vol(fwd, stk, exp_date, tenor)
            for date, fwd, stk, exp_date, tenor in zip(date_list, fwd_list, strike_list, exp_date_list, tenor_list)
        ]

    def cache_vol_cube(self, vol_cube, curve_handle, date):
        cache_put(self.cache_vol_cube_name, self.cache_vol_cube_region_temp, vol_cube, curve_handle, date)
        cache_put(self.cache_vol_cube_name, self.cache_vol_cube_region_perm, vol_cube, curve_handle, date)

    def temp_cache_vol_cube(self, vol_cube, curve_handle, date):
        cache_put(self.cache_vol_cube_name, self.cache_vol_cube_region_temp, vol_cube, curve_handle, date)


class CurveLoader(BaseLoader):
    '''
    Base class for loading interest rate curves
    '''

    def __init__(self):
        super().__init__()

    def get_curve(self, curve_name, date):
        raise NotImplementedError

    def get_curves(self, curve_names, date_list):
        df = pd.DataFrame({
            cn: {
                date: self.get_curve(cn, date)
                for date in date_list
            }
            for cn in curve_names
        })
        return df.reindex(index=date_list, columns=curve_names)

    def bulk_load_curves(self, curve_name_list, date_list):
        raise NotImplementedError


class CurveLoaderAlib(CurveLoader):
    '''
    Curve loader for ALIB curves from the file system
    '''
    cache_curve_name = 'CurveLoaderAlib.get_curve'
    cache_curve_region = 'mem_10h'

    def __init__(self, curves_location=ALIB_CURVE_REGION, live_mode=False):
        """
        :param str curves_location: root path with serialized curves in dated subdirectories.
        :param bool live_mode: is curves_location a live-ticking curves directory? \
        When live, curves are in root of curves_location.
        """
        super().__init__()
        self.live_mode = live_mode
        self.curve_base_dir = blu.get_curve_dir_from_loc(curves_location)
        self.cache = cache.get_cache_region(self.cache_curve_name, self.cache_curve_region)

    @cache_response(cache_curve_name, cache_curve_region, True)
    def get_curve(self, curve_name, date):
        return qau.pull_stored_zc(curve_name, date, self.curve_base_dir, self.live_mode)

    # TODO: rework this to performs a faster kind of load and directly put into cache
    def bulk_load_curves(self, curve_name_list, date_list):
        for curve_name in curve_name_list:
            for date in date_list:
                self.get_curve(curve_name, date)

    def cache_curve(self, curve, curve_name, date):
        cache_put(self.cache_curve_name, self.cache_curve_region, curve, curve_name, date)


class FxSpotLoader(BaseLoader):
    '''
    Base loader for FX spots
    '''

    def __init__(self):
        super().__init__()

    def get_spot(self, spot_name, date):
        raise NotImplementedError

    def get_spots(self, spot_name_list, date_list):
        df = pd.DataFrame({
            sn: {
                date: self.get_spot(sn, date)
                for date in date_list
            }
            for sn in spot_name_list
        })
        return df.reindex(index=date_list, columns=spot_name_list)

    def bulk_load_spots(self, spot_name_list, date_list):
        raise NotImplementedError


class FxSpotLoaderOpenData(FxSpotLoader):
    '''
    Loader for FX spots from opendata
    '''
    cache_spot_name = 'FxSpotLoaderOpenData.get_spot'
    cache_spot_region_temp = 'mem_10h'
    cache_spot_region_perm = 'disk'

    cache_spot_1600_name = 'FxSpotLoaderOpenData.get_spot_1600'

    def __init__(self):
        super().__init__()
        self.cache = cache.get_cache_region(self.cache_spot_name, self.cache_spot_region_temp)

    def spot_name_to_od_ticker(self, spot_name):
        return 'fx_linear_%s_sp|outright|nyclose|ph' % spot_name.lower()

    def spot_name_to_od_item(self, spot_name):
        return 'fx_linear_%s_sp' % spot_name.lower()

    def spot_name_to_od_ticker_1600(self, spot_name, is_1600, fxois_config_dict):
        if is_1600:
            return 'fx_linear_%s_sp|outright|lonclose_1600|fo_snap' % spot_name.lower()
        else:
            ticker = 'fx_linear_%s_sp|outright|'  % spot_name.lower()
            cut = fxois_config_dict[spot_name].get('snapcut','nyclose')

            return ticker + cut + '|qa_grp_snap'


    @cache_response(cache_spot_name, cache_spot_region_temp, True)
    @cache_response(cache_spot_name, cache_spot_region_perm, True)
    def get_spot(self, spot_name, date):
        ticker = self.spot_name_to_od_ticker(spot_name)
        data_df = od.df_for_observable_strings(
            [ticker], start_date=date, end_date=date, return_records=False)

        if len(data_df) == 1:
            return data_df.iloc[0, 0]
        else:
            return float('NaN')

    @cache_response(cache_spot_1600_name, cache_spot_region_temp, True)
    @cache_response(cache_spot_1600_name, cache_spot_region_perm, True)
    def get_spot_1600(self, spot_name, date, fxois_config_dict):
        # RL: Not nice.. but we started snapping the ldn close fx on this date
        if date < dt.date(2019, 8, 30):
            ticker = self.spot_name_to_od_ticker_1600(spot_name, False, fxois_config_dict)
        else:
            ticker = self.spot_name_to_od_ticker_1600(spot_name, True, fxois_config_dict)

        data_df = od.df_for_observable_strings(
            [ticker], start_date=date, end_date=date, return_records=False)

        if len(data_df) == 1:
            return data_df.iloc[0, 0]
        else:
            return float('NaN')

    def clear_spot_fx_temp_cache(self):
        clear_cache(self.cache_spot_name, self.cache_spot_region_temp)

    def clear_spot_fx_perm_cache(self):
        clear_cache(self.cache_spot_name, self.cache_spot_region_perm)

    def bulk_load_spots(self, spot_name_list, date_list):
        start_date = min(date_list)
        end_date = max(date_list)
        tickers = [self.spot_name_to_od_ticker(s) for s in spot_name_list]
        ticker_map = {self.spot_name_to_od_item(s): s for s in spot_name_list}
        data_df = od.df_for_observable_strings(
            tickers, start_date=start_date, end_date=end_date, return_records=True)
        data_df.drop(['attribute', 'cut', 'source'], axis=1, inplace=True)
        data_df['item'] = [ticker_map[i] for i in data_df['item']]
        data_df['timestamp'] = [dt.date(t.year, t.month, t.day) for t in data_df['timestamp']]

        for ix, ser in data_df.iterrows():
            cache_put(self.cache_spot_name, self.cache_spot_region_temp,
                      ser.loc['value'], ser.loc['item'], ser.loc['timestamp'])
            cache_put(self.cache_spot_name, self.cache_spot_region_perm,
                      ser.loc['value'], ser.loc['item'], ser.loc['timestamp'])

    def cache_spot(self, spot, spot_name, date):
        cache_put(self.cache_spot_name, self.cache_spot_region_temp, spot, spot_name, date)
        cache_put(self.cache_spot_name, self.cache_spot_region_perm, spot, spot_name, date)

    def temp_cache_spot(self, spot, spot_name, date):
        cache_put(self.cache_spot_name, self.cache_spot_region_temp, spot, spot_name, date)


class AssetDataLoader(BaseLoader):
    '''
    Under construction.
    '''

    def __init__(self):
        super().__init__()
        self.data_dict = {}

    def result_in_store(self, asset_name, property, asof_date):
        try:
            self.data_dict[asset_name][property].loc[asof_date]
            return True
        except KeyError:
            return False

    def get_result(self, asset_name, property, asof_date):
        return self.data_dict[asset_name][property][asof_date]

    def store_result(self, asset_name, property, asof_date, result):
        if asset_name not in self.data_dict:
            self.data_dict[asset_name] = {}
        if property not in self.data_dict[asset_name]:
            self.data_dict[asset_name][property] = pd.Series()
        self.data_dict[asset_name][property].loc[asof_date] = result

 

quant alib utils.py



from ctypes import (byref, c_bool, c_char, c_wchar, c_byte, c_ubyte, c_short, c_ushort,
                    c_int, c_uint, c_long, c_ulong, c_longlong, c_ulonglong, c_float, c_double,
                    c_longdouble, c_char_p, c_wchar_p, c_void_p)

from calendar import monthrange as cal_mr
import datetime as dt
import os
import sys
import warnings as wn

from scipy.optimize import newton

from panormus.quant import option as qo

from panormus.quant.alib import Alib_Class
from panormus.quant.alib.conventions import (
    CONV_DICT, HOLIDAY_PATH, VAL_DATE_CONV_DICT,
    EXP_DATE_CONV_DICT, CURVE_NAMES_DICT, YCF_CAL_DICT)
from panormus.config.settings import ALIB_PATH, ALIB_LOG_PATH, ALIB_CURVE_DIR

# Initialize library and error log file
if sys.platform.startswith('win'):
    from ctypes import windll

    ALib = windll.LoadLibrary(ALIB_PATH)

else:
    from ctypes import cdll

    ALib = cdll.LoadLibrary(ALIB_PATH)

ALib.ALIB_INITIALIZE()
ALib.ALIB_ERR_LOG(c_long(1))
ALib.ALIB_ERR_LOG_FILE_NAME(ALIB_LOG_PATH.encode('ascii', 'ignore'), c_long(1))

# Alib base date
ALIB_TIME_STARTS = dt.date(year=1601, month=1, day=1)
MAX_ARRAY_SIZE = 600


def free_object(alib_obj):
    try:
        ALib.ALIB_OBJECT_FREE(alib_obj.handle)
    except:
        try:
            ALib.ALIB_OBJECT_FREE(alib_obj)
        except:
            pass


def free_objects(*args):
    for alib_obj in args:
        free_object(alib_obj)


def swaptions_exp_and_tenor_ycf(d, curve_handle, holiday_oracle, expiries, tenors):
    """
    :description: Convert lists of expiries and tenors into lists of year count fractions
    :param datetime.date d:
    :param curve_handle:
    :param holiday_oracle:
    :param list[str] expiries:
    :param list[str] tenors:
    :return: dictionary of exp dates, exp ycfs, swap dates, and tenor ycfs.
    """
    exp_dates = [
        expiry_date_from_trade_date(d, e, curve_handle, holiday_oracle) for e in expiries
    ]
    exp_ycfs = [
        bus_ycf(d, ed, curve_handle, holiday_oracle) for ed in exp_dates
    ]
    swap_dates = [
        dates_from_trade_date(d, '0d', t, curve_handle, holiday_oracle) for t in tenors
    ]
    tenor_ycfs = [
        ac_day_cnt_frac(sd[0], sd[1], 'act/365') for sd in swap_dates
    ]
    return {
        'exp_dates': exp_dates, 'exp_ycfs': exp_ycfs,
        'swap_dates': swap_dates, 'tenor_ycfs': tenor_ycfs
    }


# TODO only put raw alib wrappers in here.
# Move higher-level apis to asset-specific modules, irs, date, etc.
def python_date_to_alib_Tdate(python_date):
    '''
    :description: convert any python date with month, day, and year fields into an alib date
    :param python_date: requires month, day, and year fields
    :return: alib date
    '''
    alib_date = ALib.MDY_TO_TDATE(c_int(python_date.month), c_int(python_date.day), c_int(python_date.year))
    if isinstance(alib_date, int):
        alib_date = c_long(alib_date)

    return alib_date


def alib_Tdate_to_python_date(TDate):
    m = c_long(0)
    d = c_long(0)
    y = c_long(0)
    status = ALib.TDATE_TO_MDY(TDate, byref(m), byref(d), byref(y))
    python_date = dt.date(y.value, m.value, d.value)
    return python_date


def python_date_to_alib_str_date(python_date):
    '''
    :description: convert any python date with month, day, and year fields into a string in \
    the format alib expects
    :param python_date: requires month, day, and year fields
    :return: alib string of the date
    '''
    return python_date.strftime('%m/%d/%Y')


def ascii_encode(in_str):
    '''
    :description: convert unicode string to basic string. Basic strings will be returned as-is.
    :param in_str: unicode string
    :return: basic string
    '''
    return in_str.encode('ascii', 'ignore') if isinstance(in_str, str) else in_str


def alib_obj_coerce(params, obj_type, handle):
    """
    :description: wraps alib object_coerce_from_string
    :param str params: symbol to convert for alib
    :param str obj_type: a string indicating object type, such as 'IVL'
    :param handle: pointer to fill with return value
    :return: int error status
    """
    status = ALib.ALIB_OBJECT_COERCE_FROM_STRING(
        ascii_encode(params),
        ascii_encode(obj_type),
        handle
    )
    return status


class Holidays(object):
    '''
    :description: Class instances are dictionary-like objects that store the location of alib holiday \
    files and also provide combination of holiday calendars with plus operator, such as \
    Holidays()['USD+GBP'] to combine USD and GBP holiday calendars.
    '''

    def __init__(self, *args, **kwargs):
        self._holiday_path = kwargs.pop('holiday_path', HOLIDAY_PATH)
        self._hol_dict = {}
        for a in args:
            a_path = os.path.join(self._holiday_path, a.upper() + '.hol')
            self._hol_dict[a] = ascii_encode(a_path)
        for k, v in kwargs.items():
            self._hol_dict[k] = ascii_encode(v)

    def __getitem__(self, key):
        if key in self._hol_dict:
            return self._hol_dict.__getitem__(key)
        elif '+' not in key:
            key_path = os.path.join(self._holiday_path, key.upper() + '.hol')
            self._hol_dict[key] = ascii_encode(key_path)
            return self._hol_dict[key]
        elif '+' in key:
            key_list = key.split('+')
            for k in key_list:
                if k not in self._hol_dict:
                    k_path = os.path.join(self._holiday_path, k.upper() + '.hol')
                    self._hol_dict[k] = ascii_encode(k_path)
            num_keys = len(key_list)
            returned = c_char_p()
            path_array = (c_char_p * num_keys)()
            for i, k in enumerate(key_list):
                path_array[i] = self._hol_dict[k]

            ALib.ALIB_HOLIDAYS_COMBINE(
                c_int(num_keys),
                path_array,
                ascii_encode(key),
                byref(returned)
            )
            self._hol_dict[key] = returned.value
            return self._hol_dict[key]

    def __iter__(self):
        return iter(self._hol_dict)

    def iteritems(self):
        return self._hol_dict.items()

    def add_hols(self, *args, **kwargs):
        for a in args:
            a_path = os.path.join(self._holiday_path, a.upper() + '.hol')
            self._hol_dict[a] = ascii_encode(a_path)
        for k, v in kwargs.items():
            self._hol_dict[k] = ascii_encode(v)


def ac_err_get_log(lines=20):
    """
    :description: Return the alib error log
    :param int lines: number of lines to return, default 20
    :return: string
    """
    raw_res = (c_char_p * lines)()
    failure = ALib.ALIB_ERR_GET_LOG(
        byref(raw_res)
    )
    error_log = b'\n'.join(raw_res[i] for i in range(lines) if raw_res[i])
    return error_log.decode('ascii')


def raise_val_err_w_log(s):
    """
    :description: prepends the user supplied string to the alib error log, and raises a value error
    :param string s: message
    :return: string
    """
    err_log = ac_err_get_log()
    err_str = '\n'.join([s, err_log])
    raise ValueError(err_str)


def ac_interp_lf2(xs, ys, fxys, x, y):
    """
    :description: This function performs 2-dimensional linear interpolation on real number data types. The function assumes that the two arrays are sorted in increasing order.
    :param xs: x values
    :param ys: y values
    :param fxys: matrix of f(x, y) values
    :param x: x value to interp to
    :param y: y value to interp to
    :return:
    """
    nx = len(xs)
    ny = len(ys)

    xs_c = (c_double * nx)(*[])
    for i in range(0, nx):
        xs_c[i] = c_double(xs[i])

    ys_c = (c_double * ny)(*[])
    for i in range(0, ny):
        ys_c[i] = c_double(ys[i])

    FXYS = ((c_double * nx) * ny)
    fxys_c = FXYS()
    for i in range(0, ny):
        for j in range(0, nx):
            fxys_c[i][j] = c_double(fxys[i, j])

    fout = c_double()
    failure = ALib.ALIB_INTERP_LF2(
        c_long(nx), xs_c,
        c_long(ny), ys_c,
        c_long(nx * ny), fxys_c,
        c_double(x), c_double(y),
        byref(fout))
    if failure:
        base_err_msg = '2d interp failed\n'
        base_err_msg = base_err_msg + 'x = %d\n' % x
        base_err_msg = base_err_msg + 'y = %d\n' % y
        raise_val_err_w_log(base_err_msg)
    return fout.value


def ac_interp_rate(crv, rate_end_date):
    """
    :description: Interpolate a zero rate from a zero curve
    :param crv: alib curve object
    :param rate_end_date: end date of the rate (start date will be the base date of the curve)
    :return: double
    """
    alib_end_date = python_date_to_alib_Tdate(rate_end_date)

    rate = c_double()
    failure = ALib.ALIB_INTERP_RATE_O(crv, alib_end_date, byref(rate))
    if failure:
        base_err_msg = 'Error in interping rate'
        raise_val_err_w_log(base_err_msg)
    return rate.value


def ac_ivl_adj_make(ivl_string, bus_day_int, holiday_file, bad_day_conv_str):
    """
    :description: Constructor for an Adjusted Date Interval
    :param ivl_string: alib date inteval string (e.g. 3M)
    :param bus_day_int: Whether to count business days (non-zero) or calendar days (0)
    :param holiday_file: holiday list
    :param bad_day_conv_str: bad day convention (M, F or N)
    :return: ivl adj object
    """
    alib_interval = Alib_Class.IVL()
    alib_obj_coerce(ivl_string, "IVL", byref(alib_interval))
    holiday_file = ascii_encode(holiday_file)
    bad_day_conv_str = ascii_encode(bad_day_conv_str)
    alib_ivl_adj = Alib_Class.IVL_ADJ()
    failure = ALib.ALIB_IVL_ADJ_MAKE(
        alib_interval,
        c_long(bus_day_int),
        holiday_file,
        bad_day_conv_str,
        byref(alib_ivl_adj)
    )
    free_object(alib_interval)
    if failure:
        free_object(alib_ivl_adj)
        base_err_msg = 'Error, cannot construct date adjuster'
        raise_val_err_w_log(base_err_msg)

    return alib_ivl_adj


def ac_ivl_years(ivl_string):
    """
   :description: Returns the year fraction equivalent to a specified DateInterval
    :param ivl_string: Alib interval string (e.g. 3M)
    :return: double
    """
    ycf = c_double()
    alib_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(ivl_string, "IVL", byref(alib_interval))
    failure = ALib.ALIB_IVL_YEARS(alib_interval, byref(ycf))
    free_object(alib_interval)
    if failure:
        base_err_msg = 'Could not compute interval years'
        raise_val_err_w_log(base_err_msg)

    return ycf.value


def pull_stored_zc(curve_name, asof_date, curve_base_dir=None, live_mode=False, lon1600_mode=False):
    """
    :description: Pulls a stored zero curve from the filesystem
    :param string curve_name: e.g USD.LIBOR.3M
    :param asof_date: date to pull the curve on
    :param curve_base_dir: base director for the stored curves, default is the ny quant share
    :param bool live_mode: set True when using a curve_base_dir with live curves directly in root.
    :return: alib curve object
    """
    if curve_base_dir == None:
        crv_dir = ALIB_CURVE_DIR
    else:
        crv_dir = curve_base_dir

    asof_date_str = asof_date.strftime('%Y%m%d')

    if live_mode:
        full_curve_name = curve_name + '.live.xml'
        curve_fn = os.path.join(crv_dir, full_curve_name)
        return load_zc(curve_fn)

    # For non-live mode, path down to dated subfolder.
    asof_year = asof_date.strftime('%Y')
    asof_month = asof_date.strftime('%m')
    asof_day = asof_date.strftime('%d')

    curve_point = 'lon1600' if lon1600_mode else asof_date_str

    full_curve_name = curve_name + '.' + curve_point + '.xml'
    curve_fn = os.path.join(crv_dir, asof_year, asof_month, asof_day, full_curve_name)

    return load_zc(curve_fn)


def load_zc(path):
    """
    :description: Returns a curve object from the xml file
    :param str path: fully qualified path the to curve object
    :return: alib curve object
    """
    path = ascii_encode(path)
    sbuf = c_char_p()
    crv_obj = Alib_Class.ZC()
    failure = ALib.ALIB_OBJECT_LOAD(path, byref(crv_obj), byref(sbuf))
    if failure:
        free_object(crv_obj)
        base_err_msg = 'Error while loading zerocurve from path %s' % path
        raise_val_err_w_log(base_err_msg)

    return crv_obj


def ac_zero_curve_slide(alib_curve, new_base_date):
    """
    :description: Slides a zero curve to a new base date
    :param alib_curve: original curve
    :param new_base_date: new date to slide the curve to
    :return: alib curve
    """
    alib_new_base_date = python_date_to_alib_Tdate(new_base_date)
    new_crv_obj = Alib_Class.ZC()
    failure = ALib.ALIB_ZERO_CURVE_SLIDE(
        alib_curve,
        alib_new_base_date,
        byref(new_crv_obj))
    if failure:
        free_object(new_crv_obj)
        base_err_msg = 'Error while sliding zero curve'
        raise_val_err_w_log(base_err_msg)

    return new_crv_obj


def make_single_date_stub_curve(stub_date):
    alib_stub_zero_curve = Alib_Class.ZC()
    baseDate = python_date_to_alib_Tdate(stub_date)
    cashDCC = Alib_Class.DCC()
    alib_obj_coerce("Act/360", "DCC", byref(cashDCC))
    rt = Alib_Class.RT()
    alib_obj_coerce("Annual", "RT", byref(rt))

    stubDiscDates = (c_long * 1)(*[])
    stubDiscRates = (c_double * 1)(*[])
    stubDiscDates[0] = baseDate
    stubDiscRates[0] = 0
    failure = ALib.ALIB_ZERO_CURVE_MAKE(
        c_long(1),  # numDates
        stubDiscDates,  # Dates
        stubDiscRates,  # Rates
        cashDCC,  # DCC
        ascii_encode("F"),  # interpType
        baseDate,  # baseDate
        rt,  # rateType
        byref(alib_stub_zero_curve)  # output
    )
    free_objects(cashDCC, rt)

    if failure:
        free_object(alib_stub_zero_curve)
        base_err_msg = 'Error making stub curve'
        raise_val_err_w_log(base_err_msg)

    return alib_stub_zero_curve


def ac_zero_curve3(stub_zero_curve, instrument_types, start_dates, end_dates, rates, prices, include_flags,
                   adjustment_rates, money_market_dcc, value_floating_leg, swap_day_count_conventions, swap_coupon_ivls,
                   swap_bad_day_convention, float_rate_fixed, fix_dates, fix_rates, futures_flags, futures_mm_date,
                   ir_vol_model, coupon_interpolation, zero_interpolation, extrapolation_date, holiday_file):
    """
    :description: Wrapping of the raw alib zero curve 3 function.
    :param stub_zero_curve: date or alib zero curve object
    :param instrument_types: array of alib instrument type strings
    :param start_dates: array of start dates
    :param end_dates: array of end dates
    :param rates: array of rates
    :param prices: array of prices
    :param include_flags: array of include flags
    :param adjustment_rates: unsupported, supply 0
    :param money_market_dcc: money market dcc string
    :param value_floating_leg: 0 or 1, 1 is standard
    :param swap_day_count_conventions: array of dcc strings, first element is float leg, second is fixed
    :param swap_coupon_ivls: array of interval strings, first element is float leg, second is fixed
    :param swap_bad_day_convention: swap bad day convention (usually M)
    :param float_rate_fixed: unsupported supply 0
    :param fix_dates: unsupported supply 0
    :param fix_rates: unsupported supply 0
    :param futures_flags: unsupported supply 0
    :param futures_mm_date: unsupported supply 0
    :param ir_vol_model: unsupported supply 0
    :param str coupon_interpolation: interpolation type, 'N'
    :param str zero_interpolation: interpolation type string, F or S
    :param extrapolation_date: date or 0
    :param holiday_file: holiday file
    :return: alib zero curve object
    """

    # raise NotImplementedError('Still not quite working')

    if isinstance(stub_zero_curve, Alib_Class.ZC):
        clear_stub = False
        alib_stub_zero_curve = stub_zero_curve
    else:
        clear_stub = True
        alib_stub_zero_curve = make_single_date_stub_curve(stub_zero_curve)
        # Conversion of the first set of arrays, that use nTypes as their length
        ntypes = 0
    if hasattr(instrument_types, "__len__"):
        ntypes = len(instrument_types)
    alib_ntypes = c_long(ntypes)
    alib_inst_types = (c_void_p * ntypes)(*[])  # Array of size numTypes
    alib_start_dates = (c_void_p * ntypes)(*[])  # Array of size numStartDts */
    alib_end_dates = (c_void_p * ntypes)(*[])  # Array of same size as Types */
    alib_rates = (c_double * ntypes)(*[])  # Array of same size as Types */
    alib_prices = (c_double * ntypes)(*[])  # Array of size numPrices */
    alib_include_flags = (c_long * ntypes)(*[])  # Array of size numInclude */
    for i in range(0, ntypes):
        alib_inst_type = Alib_Class.ZCIT()
        alib_obj_coerce(instrument_types[i], 'ZCIT', byref(alib_inst_type))
        alib_inst_types[i] = alib_inst_type.handle
        # start date
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(start_dates[i]), "DOI", byref(doi))
        alib_start_dates[i] = doi.handle
        # end date
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(end_dates[i]), "DOI", byref(doi))
        alib_end_dates[i] = doi.handle
        #
        alib_rates[i] = c_double(rates[i])
        alib_prices[i] = c_double(prices[i])
        alib_include_flags[i] = c_long(include_flags[i])

    # Conversion of the second set of arrays, that use nadjustments as their length
    nadjustments = 0
    if hasattr(adjustment_rates, "__len__"):
        nadjustments = len(adjustment_rates)
    alib_num_adjustments = c_long(nadjustments)
    alib_adjustment_rates = (c_double * nadjustments)(*[])
    for i in range(0, nadjustments):
        alib_adjustment_rates[i] = c_double(adjustment_rates[i])

    # Conversion of the third set of arrays, that use nswaplegs as their length
    nswaplegs = 0
    if hasattr(swap_day_count_conventions, "__len__"):
        nswaplegs = len(swap_day_count_conventions)
    alib_num_swap_legs = c_long(nswaplegs)
    alib_swap_dccs = (c_void_p * nswaplegs)(*[])  # Array of size numSwapDCCs */
    alib_swap_ivls = (c_void_p * nswaplegs)(*[])
    for i in range(0, nswaplegs):
        alib_swap_dcc = Alib_Class.DCC()
        alib_obj_coerce(swap_day_count_conventions[i], 'DCC', byref(alib_swap_dcc))
        alib_swap_dccs[i] = alib_swap_dcc.handle
        alib_swap_ivl = Alib_Class.IVL()
        alib_obj_coerce(swap_coupon_ivls[i], 'IVL', byref(alib_swap_ivl))
        alib_swap_ivls[i] = alib_swap_ivl.handle

    # Conversion of the fourth set of arrays, that use nfixed_dates as their length
    nfixed_dates = 0
    if hasattr(fix_rates, "__len__"):
        nfixed_dates = len(fix_rates)
    alib_num_fixed_dates = c_long(nfixed_dates)
    alib_fix_dates = (c_void_p * nfixed_dates)(*[])
    alib_fix_rates = (c_double * nfixed_dates)(*[])
    for i in range(0, nfixed_dates):
        # date conversion
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(fix_dates[i]), "DOI", byref(doi))
        alib_fix_dates[i] = doi.handle
        alib_fix_rates[i] = c_double(fix_rates[i])

    # Conversion of the fifth set of arrays, that use nfutures_flags as their length
    nfutures_flags = 0
    if hasattr(futures_flags, "__len__"):
        nfutures_flags = len(futures_flags)
    alib_num_futures_flags = c_long(nfutures_flags)
    alib_futures_flags = (c_long * nfutures_flags)(*[])
   for i in range(0, nfutures_flags):
        alib_futures_flags[i] = c_long(futures_flags[i])

    #
    # Scalar values in order
    alib_mm_dcc = Alib_Class.DCC()
    alib_obj_coerce(money_market_dcc, 'DCC', byref(alib_mm_dcc))
    #
    alib_value_floating_leg = c_long(value_floating_leg)
    alib_swap_bdc = ascii_encode(swap_bad_day_convention)
    alib_float_rate_fixed = c_long(float_rate_fixed)
    #
    alib_futures_mm_date = Alib_Class.DOI()
    alib_obj_coerce(futures_mm_date, "DOI", byref(alib_futures_mm_date))
    #
    alib_ir_vol_model = c_long(ir_vol_model)
    alib_coupon_interpolation = ascii_encode(coupon_interpolation)
    #
    alib_zero_interpolation = ascii_encode(zero_interpolation)
    #
    alib_extrapolation_date = Alib_Class.DOI()
    if isinstance(extrapolation_date, (dt.date, dt.datetime)):
        alib_obj_coerce(python_date_to_alib_str_date(extrapolation_date), "DOI", byref(alib_extrapolation_date))
    else:
        alib_obj_coerce("NONE", "DOI", byref(alib_extrapolation_date))
    #
    alib_holiday_file = ascii_encode(holiday_file)
    fit_curve = Alib_Class.ZC()

    # On Windows, ctypes uses win32 structured exception handling to prevent crashes from general protection faults
    # when functions are called with invalid argument values:

    failure = ALib.ALIB_ZERO_CURVE3(
        alib_stub_zero_curve,  # CLASS_ZC      StubCurve,             /* (I) Scalar */
        alib_ntypes,  # int           numTypes,              /* (I) Size of Types array */
        alib_inst_types,  # string[]      Types,                 /* (I) Array of size numTypes */
        alib_ntypes,  # int           numStartDts,           /* (I) Size of StartDts array */
        alib_start_dates,  # string[]      StartDts,              /* (I) Array of size numStartDts */
        alib_end_dates,  # string[]      EndDts,                /* (I) Array of same size as Types */
        alib_rates,  # double[]      Rates,                 /* (I) Array of same size as Types */
        alib_ntypes,  # int           numPrices,             /* (I) Size of Prices array */
        alib_prices,  # double[]      Prices,                /* (I) Array of size numPrices */
        alib_ntypes,  # int           numInclude,            /* (I) Size of Include array */
        alib_include_flags,  # int[]         Include,               /* (I) Array of size numInclude */
        alib_num_adjustments,  # int           numAdj,                /* (I) Size of Adj array */
        alib_adjustment_rates,  # double[]      Adj,                   /* (I) Array of size numAdj */
        alib_mm_dcc,  # string        MMDCC,                 /* (I) Scalar */
        alib_value_floating_leg,  # int           ValueFlt,              /* (I) Scalar */
        alib_num_swap_legs,  # int           numSwapDCCs,           /* (I) Size of SwapDCCs array */
        alib_swap_dccs,  # string[]      SwapDCCs,              /* (I) Array of size numSwapDCCs */
        alib_num_swap_legs,  # int           numSwapIVLs,           /* (I) Size of SwapIVLs array */
        alib_swap_ivls,  # string[]      SwapIVLs,              /* (I) Array of size numSwapIVLs */
        alib_swap_bdc,  # string        SwapBDC,               /* (I) Scalar */
        alib_float_rate_fixed,  # int           FltFixed,              /* (I) Scalar */
        alib_num_fixed_dates,  # int           numFixDates,           /* (I) Size of FixDates array */
        alib_fix_dates,  # string[]      FixDates,              /* (I) Array of size numFixDates */
        alib_fix_rates,  # double[]      FixRates,              /* (I) Array of same size as FixDates */
        alib_num_futures_flags,  # int           numFutFlags,           /* (I) Size of FutFlags array */
        alib_futures_flags,  # int[]         FutFlags,              /* (I) Array of size numFutFlags */
        alib_futures_mm_date,  # string        FutMMDate,             /* (I) Scalar */
        alib_ir_vol_model,  # CLASS_VMIR    VolModelIR,            /* (I) Scalar */
        alib_coupon_interpolation,  # string        CpnInterp,             /* (I) Scalar */
        alib_zero_interpolation,  # string        ZeroInterp,            /* (I) Scalar */
        alib_extrapolation_date,  # string        ExtrapDate,            /* (I) Scalar */
        alib_holiday_file,  # string        HolidayFile,           /* (I) Scalar */
        byref(fit_curve)  # out CLASS_ZC  ZeroCurve              /* (O) Scalar */
    )
    free_objects(*alib_start_dates)
    free_objects(*alib_end_dates)
    free_objects(*alib_fix_dates)
    free_objects(*alib_inst_types)
    free_objects(*alib_swap_dccs)
    free_objects(*alib_swap_ivls)
    free_objects(alib_mm_dcc, alib_futures_mm_date, alib_extrapolation_date)
    if clear_stub:
        free_object(alib_stub_zero_curve)

    if failure:
        base_err_msg = 'Error in alib zc3 bootstrap'
        raise_val_err_w_log(base_err_msg)

    return fit_curve


def ac_zero_curve4_cb(stub_disc_curve, stub_est_curve, instrument_types, start_dates, end_dates, rates, basis, prices,
                      include_flags, adjustment_rates, money_market_dcc, swap_day_count_conventions, swap_coupon_ivls,
                      swap_bad_day_convention, float_rate_fixed, fix_dates, fix_rates, futures_flags, futures_mm_date,
                      ir_vol_model, coupon_interpolation, zero_interpolation, extrapolation_date, holiday_file,
                      holiday_file_basis):
    """
    :description: Wrapping of the raw alib_zero_curve4_cb  function.
    :param stub_disc_curve: date or alib zero curve object
    :param stub_est_curve: date or alib zero curve object
    :param instrument_types: array of alib instrument type strings
    :param start_dates: array of start dates
    :param end_dates: array of end dates
    :param rates: array of rates
    :param basis: array of basis
    :param prices: array of prices
    :param include_flags: array of include flags
    :param adjustment_rates: unsupported, supply 0
    :param money_market_dcc: money market dcc string
    :param swap_day_count_conventions: array of dcc strings, first element is float leg, second is fixed
    :param swap_coupon_ivls: array of interval strings, first element is float leg, second is fixed
    :param swap_bad_day_convention: swap bad day convention (usually M)
    :param float_rate_fixed: unsupported supply 0
    :param fix_dates: unsupported supply 0
    :param fix_rates: unsupported supply 0
    :param futures_flags: unsupported supply 0
    :param futures_mm_date: unsupported supply 0
    :param ir_vol_model: unsupported supply 0
    :param coupon_interpolation: unsupported supply 0
    :param zero_interpolation: interpolation type string, F or S
    :param extrapolation_date: date or 0
    :param holiday_file: holiday file
    :return: tuple of alib zero curve objects
    """

    if isinstance(stub_disc_curve, Alib_Class.ZC):
        clear_stub = False
        alib_stub_disc_curve = stub_disc_curve
    else:
        clear_stub = True
        alib_stub_disc_curve = make_single_date_stub_curve(stub_disc_curve)

    if isinstance(stub_est_curve, Alib_Class.ZC):
        alib_stub_est_curve = stub_est_curve
    else:
        alib_stub_est_curve = Alib_Class.ZC()
        alib_stub_est_curve = make_single_date_stub_curve(stub_est_curve)

    # Conversion of the first set of arrays, that use nTypes as their length
    ntypes = len(instrument_types)
    alib_ntypes = c_long(ntypes)
    alib_inst_types = (c_void_p * ntypes)(*[])  # Array of size numTypes
    alib_start_dates = (c_void_p * ntypes)(*[])  # Array of size numStartDts */
    alib_end_dates = (c_void_p * ntypes)(*[])  # Array of same size as Types */
    alib_rates = (c_double * ntypes)(*[])  # Array of same size as Types */
    alib_basis = (c_double * ntypes)(*[])  # Array of same size as Types */
    alib_prices = (c_double * ntypes)(*[])  # Array of size numPrices */
    alib_include_flags = (c_long * ntypes)(*[])  # Array of size numInclude */
    for i in range(0, ntypes):
        alib_inst_type = Alib_Class.ZCIT()
        alib_obj_coerce(instrument_types[i], 'ZCIT', byref(alib_inst_type))
        alib_inst_types[i] = alib_inst_type.handle
        # start date
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(start_dates[i]), "DOI", byref(doi))
        alib_start_dates[i] = doi.handle
        # end date
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(end_dates[i]), "DOI", byref(doi))
        alib_end_dates[i] = doi.handle
        #
        alib_rates[i] = c_double(rates[i])
        alib_basis[i] = c_double(basis[i])
        alib_prices[i] = c_double(prices[i])
        alib_include_flags[i] = c_long(include_flags[i])

    # Conversion of the second set of arrays, that use nadjustments as their length
    nadjustments = len(adjustment_rates)
    alib_num_adjustments = c_long(nadjustments)
    alib_adjustment_rates = (c_double * nadjustments)(*[])
    for i in range(0, nadjustments):
        alib_adjustment_rates[i] = c_double(adjustment_rates[i])

    # Conversion of the third set of arrays, that use nswaplegs as their length
    nswaplegs = len(swap_day_count_conventions)
    alib_num_swap_legs = c_long(nswaplegs)
    alib_swap_dccs = (c_void_p * nswaplegs)(*[])  # Array of size numSwapDCCs */
    alib_swap_ivls = (c_void_p * nswaplegs)(*[])
    for i in range(0, nswaplegs):
        alib_swap_dcc = Alib_Class.DCC()
        alib_obj_coerce(swap_day_count_conventions[i], 'DCC', byref(alib_swap_dcc))
        alib_swap_dccs[i] = alib_swap_dcc.handle

        alib_swap_ivl = Alib_Class.IVL()
        alib_obj_coerce(swap_coupon_ivls[i], 'IVL', byref(alib_swap_ivl))
        alib_swap_ivls[i] = alib_swap_ivl.handle

        # Conversion of the fourth set of arrays, that use nfixed_dates as their length
    nfixed_dates = len(fix_rates)
    alib_num_fixed_dates = c_long(nfixed_dates)
    alib_fix_dates = (c_void_p * nfixed_dates)(*[])
    alib_fix_rates = (c_double * nfixed_dates)(*[])
    for i in range(0, nfixed_dates):
        # date conversion
        doi = Alib_Class.DOI()
        alib_obj_coerce(python_date_to_alib_str_date(fix_dates[i]), "DOI", byref(doi))
        alib_fix_dates[i] = doi.handle
        alib_fix_rates[i] = c_double(fix_rates[i])

    # Conversion of the fifth set of arrays, that use nswaplegs as their length
    nfutures_flags = len(futures_flags)
    alib_num_futures_flags = c_long(nfutures_flags)
    alib_futures_flags = (c_long * nfixed_dates)(*[])
    for i in range(0, nfutures_flags):
        alib_futures_flags[i] = c_long(futures_flags[i])

    #
    # Scalar values in order
    alib_mm_dcc = Alib_Class.DCC()
    alib_obj_coerce(money_market_dcc, 'DCC', byref(alib_mm_dcc))
    #
    alib_swap_bdc = swap_bad_day_convention
    alib_float_rate_fixed = c_long(float_rate_fixed)
    #
    alib_futures_mm_date = Alib_Class.DOI()
    alib_obj_coerce(futures_mm_date, "DOI", byref(alib_futures_mm_date))
    #
    alib_ir_vol_model = c_long(ir_vol_model)
    alib_coupon_interpolation = ascii_encode(coupon_interpolation)
    #
    alib_zero_interpolation = ascii_encode(zero_interpolation)
    #
    alib_extrapolation_date = Alib_Class.DOI()
    if isinstance(extrapolation_date, (dt.date, dt.datetime)):
        alib_obj_coerce(python_date_to_alib_str_date(extrapolation_date), "DOI", byref(alib_extrapolation_date))
    else:
        alib_obj_coerce("NONE", "DOI", byref(alib_extrapolation_date))
    #
    alib_holiday_file = ascii_encode(holiday_file)
    alib_holiday_file_basis = ascii_encode(holiday_file_basis)
    disc_zero_curve = Alib_Class.ZC()
    est_zero_curve = Alib_Class.ZC()

    # On Windows, ctypes uses win32 structured exception handling to prevent crashes from general protection faults
    # when functions are called with invalid argument values:
    failure = ALib.ALIB_ZERO_CURVE4_CB(
        alib_stub_disc_curve,  # stubDiscZC
        alib_stub_est_curve,  # stubEstZC
        alib_ntypes,  # numTypes
        alib_inst_types,  # Types
        alib_ntypes,  # numStartDts
        alib_start_dates,  # startDts
        alib_end_dates,  # endDts
        alib_rates,  # rates
        alib_basis,  # basis rates
        alib_ntypes,  # numPrices
        alib_prices,  # prices
        alib_ntypes,  # numIncludes
        alib_include_flags,  # includes
        alib_num_adjustments,  # numAdj
        alib_adjustment_rates,  # adj
        alib_mm_dcc,  # mmDCC
        alib_num_swap_legs,  # numSwapsDCC
        alib_swap_dccs,  # swapDCC
        alib_num_swap_legs,  # numSwapsIVLs
        alib_swap_ivls,  # swapsIVLs
        alib_swap_bdc,  # swapBDC
        alib_float_rate_fixed,  # fltFixed
        alib_num_fixed_dates,  # numFixDates
        alib_fix_dates,  # fixDates
        alib_fix_rates,  # fixRates
        alib_num_futures_flags,  # numFutFlags
        alib_futures_flags,  # futFlags
        alib_futures_mm_date,  # futMMdt
        alib_ir_vol_model,  # volModelIR
        alib_coupon_interpolation,  # cpnInterp
        alib_zero_interpolation,  # zeroInterp
        alib_extrapolation_date,  # extrapDate
        alib_holiday_file,  # holidays
        alib_holiday_file_basis,  # basisHolidays
        byref(disc_zero_curve),
        byref(est_zero_curve)
   )

    free_objects(*alib_start_dates)
    free_objects(*alib_end_dates)
    free_objects(*alib_fix_dates)
    free_objects(*alib_inst_types)
    free_objects(*alib_swap_dccs)
    free_objects(*alib_swap_ivls)
    free_objects(alib_mm_dcc, alib_futures_mm_date, alib_extrapolation_date)
    if clear_stub:
        free_object(alib_stub_disc_curve)

    if failure:
        base_err_msg = 'Error in alib zc4_cb bootstrap'
        raise_val_err_w_log(base_err_msg)

    return disc_zero_curve, est_zero_curve


def ac_zero_curve_make(
        curve_dates,
        curve_zero_rates,
        curve_base_date,
        curve_dcc='Act/365F',
        interp_type='F',
        rate_type='Annual'):
    """
    :description: Zero curve constructor. This function makes a ZeroCurve (ZC) object from its component parts
    :param curve_dates: array of dates (does not include the base date)
    :param curve_zero_rates: array of zero rates
    :param curve_base_date: base date for curve
    :param curve_dcc: day count convention for the curve, default is Act/365
    :param interp_type: interpolation type for the curve [F]lat forward or [S]pline, default is F
    :param rate_type: rate type (default Annual)
    :return: alib curve object
    """
    ndates = len(curve_dates)

    alib_curve_dates = (c_long * ndates)(*[])
    alib_curve_rates = (c_double * ndates)(*[])

    for i in range(0, ndates):
        d = curve_dates[i]
        alib_curve_dates[i] = python_date_to_alib_Tdate(d)
        alib_curve_rates[i] = c_double(curve_zero_rates[i])

    alib_curve_base_date = python_date_to_alib_Tdate(curve_base_date)

    alib_dcc = Alib_Class.DCC()
    alib_obj_coerce(curve_dcc, 'DCC', byref(alib_dcc))

    alib_rate_type = Alib_Class.RT()
    alib_obj_coerce(rate_type, 'RT', byref(alib_rate_type))

    crv_obj = Alib_Class.ZC()
    failure = ALib.ALIB_ZERO_CURVE_MAKE(
        ndates,
        alib_curve_dates,
        alib_curve_rates,
        alib_dcc,
        ascii_encode(interp_type),
        alib_curve_base_date,
        alib_rate_type,
        byref(crv_obj)
    )
    free_objects(alib_dcc, alib_rate_type)

    if failure:
        free_object(crv_obj)
        base_err_msg = 'Error while making zero curve'
        raise_val_err_w_log(base_err_msg)

    return crv_obj


def ac_interp_pv(discount_curve, date):
    """
    :description: interpolate a discount factor from an alib curve
    :param discount_curve: alib curve
    :param date: date for disocunt factor
    :return: float
    """
    try:
        alib_date = python_date_to_alib_Tdate(date)
    except:
        alib_date = date
    df = c_double()

    failure = ALib.ALIB_INTERP_PV(discount_curve, alib_date, byref(df))

    if failure:
        base_err_msg = 'Could not compute discount factor'
        raise_val_err_w_log(base_err_msg)
    return df.value


def ac_rate_to_rate(
        input_rate, start_date, end_date,
        input_dcc_str, output_dcc_str,
        input_rate_type_string='Annual',
        output_rate_type_string='Continuous'
):
    """
    :description: converts a rate of one type (e.g. Act/365 Annual) to another (e.g. Act/365 continuous
    :param float input_rate: rate to convert
    :param dt.date start_date:
    :param dt.date end_date:
    :param input_dcc_str: input rate day count convention
    :param output_dcc_str: output rate day count convention
    :param input_rate_type_string: alib rate type, default 'Annual'
    :param output_rate_type_string: alib rate type, default 'Continuous'
    :return: float
    """
    input_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(input_dcc_str, "DCC", byref(input_dcc))

    output_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(output_dcc_str, "DCC", byref(output_dcc))

    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_end_date = python_date_to_alib_Tdate(end_date)

    input_rate_type = Alib_Class.RT()
    failure = alib_obj_coerce(input_rate_type_string, "RT", byref(input_rate_type))

    output_rate_type = Alib_Class.RT()
    failure = alib_obj_coerce(output_rate_type_string, "RT", byref(output_rate_type))

    output_rate = c_double()

    failure = ALib.ALIB_RATE_TO_RATE(
        c_double(input_rate),  # double                 Rate,                  /* (I) Scalar */
        alib_start_date,  # TDate                  StartDate,             /* (I) Scalar */
        alib_end_date,  # TDate                  MaturityDate,          /* (I) Scalar */
        input_dcc,  # string                 DCC,                   /* (I) Scalar */
        input_rate_type,  # CLASS_RT               RateType,              /* (I) Scalar */
        output_dcc,  # string                 DesiredDCC,            /* (I) Scalar */
        output_rate_type,  # CLASS_RT               DesiredRateType,       /* (I) Scalar */
        byref(output_rate)  # out double             Output1                /* (O) Scalar */
    )

    free_objects(input_dcc, output_dcc, input_rate_type, output_rate_type)
    if failure:
        base_err_msg = 'Failed to convert rate'
        raise_val_err_w_log(base_err_msg)
    return output_rate.value


def ac_mm_rate_o(projection_curve, start_date, end_date, dcc):
    """
    :description: Project a money market rate off a curve
    :param projection_curve: alib curve
    :param start_date:
    :param end_date:
    :param dcc: day count convenction (e.g. 'Act/360')
    :return: float
    """
    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    try:
        alib_end_date = python_date_to_alib_Tdate(end_date)
    except:
        alib_end_date = end_date

    alib_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(dcc, "DCC", byref(alib_dcc))

    rate = c_double()
    failure = ALib.ALIB_MM_RATE_O(
        projection_curve,  # CLASS_ZC      ZeroCurve,             /* (I) Scalar */
        alib_start_date,  # TDate         StartDate,             /* (I) Scalar */
        alib_end_date,  # TDate         MaturityDate,          /* (I) Scalar */
        alib_dcc,  # string        DesiredDCC,            /* (I) Scalar */
        byref(rate))  # out double    DesiredRate            /* (O) Scalar */

    free_object(alib_dcc)

    if failure:
        base_err_msg = 'Could not compute mm rate'
        raise_val_err_w_log(base_err_msg)

    return rate.value

def swap_conventions(conv_str, swap_conv_dict=None):
    if swap_conv_dict is None:
        swap_conv_dict = CONV_DICT

    if conv_str in swap_conv_dict:
        return swap_conv_dict[conv_str]
    else:
        raise ValueError('Conventions do not exist for specified string.  %s was given' % conv_str)

def ac_swap_rate_adj2(
        discount_curve, start_date, end_date, fixed_interval,
        fixed_dcc, value_float_leg, float_pv, projection_curve,
        float_interval, float_dcc, already_fixed, fixing_rate,
        stub_method, convexity_adjust, vol_model, accrual_bad_day_conv,
        pay_bad_day_conv, reset_bad_day_conv, holiday_file
):
    """
    :description: Calculate a par swap rate
    :param discount_curve: alib curve object
    :param start_date:
    :param end_date:
    :param fixed_interval: Alib interval string, e.g. 1Y
    :param fixed_dcc: Day count convention, e.g. '30/360'
    :param value_float_leg: 1 or 0
    :param float_pv: give the floating PV if value_float_leg=0, usually this is 1
    :param projection_curve: alib curve, only required if value_float_leg!=0
    :param float_interval: Alib interval string, e.g. 3M
    :param float_dcc: Day count convention, e.g. 'Act/360'
    :param already_fixed: Has the first floating period fixed 1 or 0
    :param fixing_rate: first period fixing, ingored if already_fixed=0
    :param stub_method: alib stub descriptor, usually 'f/s'
    :param convexity_adjust: apply convexity adjustment, usually 0
    :param vol_model: _vol model object for convexity adjustment, usually 0
    :param accrual_bad_day_conv: one of 'M', 'F' or 'N'
    :param pay_bad_day_conv: one of 'M', 'F' or 'N'
    :param reset_bad_day_conv: one of 'M', 'F' or 'N'
    :param holiday_file: holiday calendar
    :return: float
    """
    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    try:
        alib_end_date = python_date_to_alib_Tdate(end_date)
    except:
        alib_end_date = end_date

    alib_fixed_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(fixed_interval, "IVL", byref(alib_fixed_interval))

    alib_fixed_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(fixed_dcc, "DCC", byref(alib_fixed_dcc))

    alib_value_float_leg = value_float_leg if isinstance(value_float_leg, c_long) else c_long(value_float_leg)
    alib_float_pv = float_pv if isinstance(float_pv, c_double) else c_double(float_pv)

    alib_float_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(float_interval, "IVL", byref(alib_float_interval))

    alib_float_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(float_dcc, "DCC", byref(alib_float_dcc))

    alib_already_fixed = already_fixed if isinstance(already_fixed, c_long) else c_long(already_fixed)
    alib_fixing_rate = fixing_rate if isinstance(fixing_rate, c_double) else c_double(fixing_rate)

    alib_stub_method = Alib_Class.STB()
    failure = alib_obj_coerce(stub_method, "STB", byref(alib_stub_method))

    alib_convexity_adjust = convexity_adjust if isinstance(convexity_adjust, c_long) else c_long(convexity_adjust)

    alib_vol_model = Alib_Class.VMIR()
    failure = alib_obj_coerce(vol_model, "VMIR", byref(alib_vol_model))

    alib_accrual_bad_day_conv = ascii_encode(accrual_bad_day_conv)
    alib_pay_bad_day_conv = ascii_encode(pay_bad_day_conv)
    alib_reset_bad_day_conv = ascii_encode(reset_bad_day_conv)
    alib_holiday_file = ascii_encode(holiday_file)

    rate = c_double()

    failure = ALib.ALIB_SWAP_RATE2(
        discount_curve,
        alib_start_date,
        alib_end_date,
        alib_fixed_interval,
        alib_fixed_dcc,
        alib_value_float_leg,
        alib_float_pv,
        projection_curve,
        alib_float_interval,
        alib_float_dcc,
        alib_already_fixed,
        alib_fixing_rate,
        alib_stub_method,
        alib_convexity_adjust,
        alib_vol_model,
        alib_accrual_bad_day_conv,
        alib_pay_bad_day_conv,
        alib_reset_bad_day_conv,
        alib_holiday_file,
        byref(rate)
    )

    free_objects(alib_fixed_interval, alib_fixed_dcc, alib_float_interval,
                 alib_float_dcc, alib_stub_method, alib_vol_model)
    if failure:
        base_err_msg = 'Could not compute swap rate'
        raise_val_err_w_log(base_err_msg)
    return rate.value


def swap_rate_with_conv(
        discount_curve, start_date, end_date, value_float_leg, float_pv,
        projection_curve, already_fixed, fixing_rate, holiday_oracle, conv_str, swap_conv_dict=None
):
    """
    :description: compute a swap rate using standard market conventions
    :param discount_curve: alib curve object.
    :param start_date:
    :param end_date:
    :param value_float_leg: 1 or 0, usually 1
    :param float_pv: pv of the float leg, ignored unless value_float_leg=0
    :param projection_curve: alib curve object
    :param already_fixed: Has the first floating period fixed 1 or 0
    :param fixing_rate: first period fixing, ingored if already_fixed=0
    :param holiday_oracle: holiday oracle object
    :param conv_str: panormus convention string, e.g. USD.3ML
    :return: float
    """
    conventions = swap_conventions(conv_str, swap_conv_dict)

    needed_conventions = {}
    for s in ['fixed_interval', 'fixed_dcc', 'float_interval', 'float_dcc',
              'stub_method', 'accrual_bad_day_conv', 'convexity_adjust',
              'pay_bad_day_conv', 'reset_bad_day_conv', 'vol_model']:
        needed_conventions[s] = conventions[s]

    holiday_file = holiday_oracle[conventions['holiday_calendar_name']]

    return ac_swap_rate_adj2(
        discount_curve=discount_curve, start_date=start_date,
        end_date=end_date, value_float_leg=value_float_leg,
        float_pv=float_pv, projection_curve=projection_curve,
        already_fixed=already_fixed, fixing_rate=fixing_rate,
        holiday_file=holiday_file, **needed_conventions)


def ac_swap_fixed_pv(
        discount_curve, coupon_rate, start_date, fixed_interval,
        end_date, fixed_dcc, stub_method, accrual_bad_day_conv,
        pay_bad_day_conv, holiday_file, principal_initial_flag,
        principal_final_flag, value_date
):
    """
    :description: compute the fixed pv of a swap
    :param discount_curve: alib curve object
    :param coupon_rate: swap coupon
    :param start_date:
    :param fixed_interval: alib interval, e.g. 6M
    :param end_date:
    :param fixed_dcc: day count convention, e.g. '30/360'
    :param stub_method: alib stub descriptor, usually 'f/s'
    :param accrual_bad_day_conv: one of 'M', 'F' or 'N'
    :param pay_bad_day_conv: one of 'M', 'F' or 'N'
    :param holiday_file:
    :param principal_initial_flag: if 1 inserts a payment of -1 at the start date
    :param principal_final_flag: if 1 inserts a payment of 1 at the maturity date
    :param value_date:
    :return: float
    """
    alib_discount_curve = discount_curve

    alib_coupon_rate = c_double(float(coupon_rate))

    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    alib_fixed_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(fixed_interval, 'IVL', byref(alib_fixed_interval))

    alib_fixed_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(fixed_dcc, 'DCC', byref(alib_fixed_dcc))
    try:
        alib_end_date = python_date_to_alib_Tdate(end_date)
    except:
        alib_end_date = end_date

    alib_stub_method = Alib_Class.STB()
    failure = alib_obj_coerce(stub_method, 'STB', byref(alib_stub_method))

    alib_accrual_bad_day_conv = ascii_encode(accrual_bad_day_conv)

    alib_pay_bad_day_conv = ascii_encode(pay_bad_day_conv)

    alib_holiday_file = ascii_encode(holiday_file)

    alib_principal_initial_flag = c_long(int(principal_initial_flag))

    alib_principal_final_flag = c_long(int(principal_final_flag))

    try:
       alib_value_date = python_date_to_alib_Tdate(value_date)
    except:
        alib_value_date = value_date

    raw_res = c_double()
    failure = ALib.ALIB_SWAP_FIXED_PV(
        alib_discount_curve,
        alib_coupon_rate,
        alib_start_date,
        alib_fixed_interval,
        alib_end_date,
        alib_fixed_dcc,
        alib_stub_method,
        alib_accrual_bad_day_conv,
        alib_pay_bad_day_conv,
        alib_holiday_file,
        alib_principal_initial_flag,
        alib_principal_final_flag,
        alib_value_date,
        byref(raw_res)
    )
    free_objects(alib_fixed_interval, alib_fixed_dcc, alib_stub_method)
    if failure:
        base_err_msg = 'Could not compute swap_fixed_pv'
        raise_val_err_w_log(base_err_msg)
    swap_fixed_pv = raw_res.value
    return swap_fixed_pv


def swap_fixed_pv_with_conv(
        discount_curve, coupon_rate, start_date, end_date,
        holiday_oracle, value_date, conv_str, swap_conv_dict=None
):
    """
    :description: compute a swap fixed PV using standard market conventions
    :param discount_curve: alib curve object
    :param coupon_rate: swap coupon
    :param start_date:
    :param end_date:
    :param holiday_oracle: holiday oracle object
    :param value_date:
    :param conv_str: panormus convention string, e.g. EUR.6ML
    :return: float
    """
    conventions = swap_conventions(conv_str, swap_conv_dict)

    needed_conventions = {}
    for s in ['fixed_interval', 'fixed_dcc', 'stub_method', 'pay_bad_day_conv',
              'accrual_bad_day_conv', 'principal_initial_flag', 'principal_final_flag']:
        needed_conventions[s] = conventions[s]

    holiday_file = holiday_oracle[conventions['holiday_calendar_name']]

    return ac_swap_fixed_pv(discount_curve=discount_curve, coupon_rate=coupon_rate,
                            start_date=start_date, end_date=end_date,
                            holiday_file=holiday_file, value_date=value_date, **needed_conventions)


def swap_annuity_with_conv(
        discount_curve,
        start_date, end_date, holiday_oracle,
        value_date, conv_str,swap_conv_dict=None
):
    """
    :description: compute a swap annuity using standard market conventions
    :param discount_curve: alib curve object
    :param start_date:
    :param end_date:
    :param holiday_oracle: holiday oracle object
    :param value_date:
    :param conv_str: panormus convention string, e.g. EUR.6ML
    :return: float
    """
    return swap_fixed_pv_with_conv(
        discount_curve, 1.0, start_date, end_date,
        holiday_oracle, value_date, conv_str,swap_conv_dict)


def ac_swap_fixed_sens(
        discount_curve, coupon_rate, start_date, fixed_interval, maturity_date,
        fixed_dcc, stub_method, accrual_bad_day_conv,
        pay_bad_day_conv, holiday_file, principal_initial_flag,
        principal_final_flag, value_date, sens_type
):
    """
    :description: compute the PV or PVBP of a fixed swap leg
    :param discount_curve: alib_curve
    :param coupon_rate: swap rate
    :param start_date:
    :param fixed_interval: alib interval string, e.g. 6M
    :param maturity_date:
    :param fixed_dcc: alib day count convention, e.g. Act/360
    :param stub_method: stub descriptor, usually f/s
    :param accrual_bad_day_conv: one of 'M', 'F', or 'N'
    :param pay_bad_day_conv: one of 'M', 'F', or 'N'
    :param holiday_file: holiday file
    :param principal_initial_flag: if 1 inserts a payment of -1 at the start date
    :param principal_final_flag: if 1 inserts a payment of 1 at the maturity date
   :param value_date:
    :param sens_type: one of 'P' for PV, or 'V' for PVBP
    :return:
    """
    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    alib_fixed_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(fixed_interval, 'IVL', byref(alib_fixed_interval))

    try:
        alib_end_date = python_date_to_alib_Tdate(maturity_date)
    except:
        alib_end_date = maturity_date

    alib_fixed_dcc = Alib_Class.DCC()
   failure = alib_obj_coerce(fixed_dcc, 'DCC', byref(alib_fixed_dcc))

    alib_stub_method = Alib_Class.STB()
    failure = alib_obj_coerce(stub_method, 'STB', byref(alib_stub_method))

    alib_accrual_bad_day_conv = ascii_encode(accrual_bad_day_conv)
    alib_pay_bad_day_conv = ascii_encode(pay_bad_day_conv)
    alib_holiday_file = ascii_encode(holiday_file)
    alib_principal_initial_flag = c_long(int(principal_initial_flag))
    alib_principal_final_flag = c_long(int(principal_final_flag))

    try:
        alib_value_date = python_date_to_alib_Tdate(value_date)
    except:
        alib_value_date = value_date

    raw_res = c_double()

    failure = ALib.ALIB_SWAP_FIXED_SENS(
        discount_curve,
        c_double(coupon_rate),
        alib_start_date,
        alib_fixed_interval,
        alib_end_date,
        alib_fixed_dcc,
        alib_stub_method,
        alib_accrual_bad_day_conv,
        alib_pay_bad_day_conv,
        alib_holiday_file,
        alib_principal_initial_flag,
        alib_principal_final_flag,
        alib_value_date,
        ascii_encode(sens_type),
        byref(raw_res))

    free_objects(alib_fixed_interval, alib_fixed_dcc, alib_stub_method)
    if failure:
        base_err_msg = 'Could not compute swap fixed sens'
        raise_val_err_w_log(base_err_msg)
    return raw_res.value


def ac_swap_float_sens(
        discount_curve, notional, spread, projection_curve, start_date,
        float_interval, float_rate_interval, compound_method, end_date,
        float_dcc, stub_method, accrual_bad_day_conv,
        pay_bad_day_conv, reset_bad_day_conv, holiday_file, principal_initial_flag,
        principal_final_flag, first_float_fixed_flag, first_float_fixed_rate,
        value_date, convexity_adjust, vol_model, sens_type
):
    """
    :description: compute the PV or PVBP of a floating swap leg
    :param discount_curve: alib curve object
    :param notional: notional of the swap
    :param spread: spread over the projected rate
    :param projection_curve: alib curve object
    :param start_date:
    :param float_interval: alib interval string, e.g. 6M
    :param float_rate_interval: alib rate interval string, e.g. 6M
    :param compound_method: compound method of spread. 1=compounding, 2=flat compounding
    :param end_date:
    :param float_dcc: day count convention, e.g. 'Act/365F'
    :param stub_method: alib stub descriptor, usually 'f/s'
    :param accrual_bad_day_conv: one of 'M', 'F', or 'N'
    :param pay_bad_day_conv: one of 'M', 'F', or 'N'
    :param reset_bad_day_conv: one of 'M', 'F', or 'N'
    :param holiday_file:
    :param principal_initial_flag: if 1 inserts a payment of -1 at the start date
    :param principal_final_flag: if 1 inserts a payment of 1 at the maturity date
    :param first_float_fixed_flag: Flag to denote that the first floating rate has been fixed.
    :param first_float_fixed_rate: The fixing of the first floating rate. Only required if first float rate fixed = 1.
    :param value_date:
    :param convexity_adjust: unsupported, use 0
    :param vol_model: unsupported use 0
    :param sens_type: one of 'P' for PV, or 'V' for PVBP
    :return:
    """
    alib_notional = c_double(float(notional))
    alib_spread = c_double(float(spread))

    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    alib_float_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(float_interval, 'IVL', byref(alib_float_interval))

    alib_float_rate_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(float_rate_interval, 'IVL', byref(alib_float_rate_interval))

    alib_compound_method = c_long(int(compound_method))

    try:
        alib_end_date = python_date_to_alib_Tdate(end_date)
    except:
        alib_end_date = end_date

    alib_float_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(float_dcc, 'DCC', byref(alib_float_dcc))

    alib_stub_method = Alib_Class.STB()
    failure = alib_obj_coerce(stub_method, 'STB', byref(alib_stub_method))

    alib_accrual_bad_day_conv = ascii_encode(accrual_bad_day_conv)
    alib_pay_bad_day_conv = ascii_encode(pay_bad_day_conv)
    alib_reset_bad_day_conv = ascii_encode(reset_bad_day_conv)
    alib_holiday_file = ascii_encode(holiday_file)
    alib_principal_initial_flag = c_long(int(principal_initial_flag))
    alib_principal_final_flag = c_long(int(principal_final_flag))
    alib_first_float_fixed_flag = c_long(int(first_float_fixed_flag))
    alib_first_float_fixed_rate = c_double(float(first_float_fixed_rate))

    # l_fixed = first_float_fixed_rate
    # if not isinstance(first_float_fixed_rate, list):
    #     l_fixed = [first_float_fixed_rate]
    #
    # nfixed_rates = len(l_fixed)
    # alib_num_fixed_rates = c_long(nfixed_rates)
    # alib_fix_rates = (c_double * nfixed_rates)(*[])
    # for i in range(0, nfixed_rates):
    #     alib_fix_rates[i] = c_double(l_fixed[i])

    try:
        alib_value_date = python_date_to_alib_Tdate(value_date)
    except:
        alib_value_date = value_date

    alib_convexity_adjust = convexity_adjust if isinstance(convexity_adjust, c_long) else c_long(convexity_adjust)

    alib_vol_model = Alib_Class.VMIR()
    failure = alib_obj_coerce(vol_model, 'VMIR', byref(alib_vol_model))

    raw_res = c_double()
    failure = ALib.ALIB_SWAP_FLOAT_SENS(
        discount_curve,
        alib_notional,
        alib_spread,
        projection_curve,
        alib_start_date,
        alib_float_interval,
        alib_end_date,
        alib_float_dcc,
        alib_stub_method,
        alib_accrual_bad_day_conv,
        alib_pay_bad_day_conv,
        alib_reset_bad_day_conv,
        alib_holiday_file,
        alib_principal_initial_flag,
        alib_principal_final_flag,
        alib_first_float_fixed_flag,
        alib_first_float_fixed_rate,
        alib_value_date,
        alib_convexity_adjust,
        alib_vol_model,
        ascii_encode(sens_type),
        byref(raw_res)
    )

    # failure = ALib.ALIB_SWAP_FLOAT_SENS_GENERIC(
    #     discount_curve,
    #     alib_notional,
    #     alib_spread,
    #     projection_curve,
    #     alib_start_date,
    #     alib_float_interval,
    #     alib_float_rate_interval,
    #     alib_compound_method,
    #     alib_end_date,
    #     alib_float_dcc,
    #     alib_stub_method,
    #     alib_accrual_bad_day_conv,
    #     alib_pay_bad_day_conv,
    #     alib_reset_bad_day_conv,
    #     alib_holiday_file,
    #     alib_principal_initial_flag,
    #     alib_principal_final_flag,
    #     alib_first_float_fixed_flag,
    #     alib_num_fixed_rates,
    #     alib_fix_rates,
    #     alib_value_date,
    #     alib_convexity_adjust,
    #     alib_vol_model,
    #     ascii_encode(sens_type),
    #     byref(raw_res)
    # )

    free_objects(alib_float_interval, alib_float_rate_interval, alib_float_dcc, alib_stub_method, alib_vol_model)
    if failure:
        base_err_msg = 'Could not compute swap float sens'
        raise_val_err_w_log(base_err_msg)
    return raw_res.value


def ac_swap_float_pv(
        discount_curve, notional, spread, projection_curve, start_date,
        float_interval, float_rate_interval, compound_method, end_date, float_dcc, stub_method, accrual_bad_day_conv,
        pay_bad_day_conv, reset_bad_day_conv, holiday_file, principal_initial_flag,
        principal_final_flag, first_float_fixed_flag, first_float_fixed_rate,
        value_date, convexity_adjust, vol_model
):
    """
    :description: compute the floating pv of a swap
    :param discount_curve: alib curve object
    :param notional: notional of the swap
    :param spread: spread over the projected rate
    :param projection_curve: alib curve object
    :param start_date:
    :param float_interval: alib interval string, e.g. 6M
    :param float_rate_interval: alib rate interval string, e.g. 6M
    :param compound_method: compound method of spread. 1=compounding, 2=flat compounding
    :param end_date:
    :param float_dcc: day count convention, e.g. 'Act/365F'
    :param stub_method: alib stub descriptor, usually 'f/s'
    :param accrual_bad_day_conv: one of 'M', 'F', or 'N'
    :param pay_bad_day_conv: one of 'M', 'F', or 'N'
    :param reset_bad_day_conv: one of 'M', 'F', or 'N'
    :param holiday_file:
    :param principal_initial_flag: if 1 inserts a payment of -1 at the start date
    :param principal_final_flag: if 1 inserts a payment of 1 at the maturity date
    :param first_float_fixed_flag: Flag to denote that the first floating rate has been fixed.
    :param first_float_fixed_rate: The fixing of the first floating rate. Only required if first float rate fixed = 1.
    :param value_date:
    :param convexity_adjust: unsupported, use 0
    :param vol_model: unsupported use 0
    :return:
    """
    alib_notional = c_double(float(notional))

    alib_spread = c_double(float(spread))

    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    alib_float_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(float_interval, 'IVL', byref(alib_float_interval))

    alib_float_rate_interval = Alib_Class.IVL()
    failure = alib_obj_coerce(float_rate_interval, 'IVL', byref(alib_float_rate_interval))

    alib_compound_method = c_long(int(compound_method))

    try:
        alib_end_date = python_date_to_alib_Tdate(end_date)
    except:
        alib_end_date = end_date

    alib_float_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(float_dcc, 'DCC', byref(alib_float_dcc))

    alib_stub_method = Alib_Class.STB()
    failure = alib_obj_coerce(stub_method, 'STB', byref(alib_stub_method))

    alib_accrual_bad_day_conv = ascii_encode(accrual_bad_day_conv)
    alib_pay_bad_day_conv = ascii_encode(pay_bad_day_conv)
    alib_reset_bad_day_conv = ascii_encode(reset_bad_day_conv)
    alib_holiday_file = ascii_encode(holiday_file)
    alib_principal_initial_flag = c_long(int(principal_initial_flag))
    alib_principal_final_flag = c_long(int(principal_final_flag))
    alib_first_float_fixed_flag = c_long(int(first_float_fixed_flag))
    # alib_first_float_fixed_rate = c_double(float(first_float_fixed_rate))

    l_fixed = first_float_fixed_rate
    if not isinstance(first_float_fixed_rate, list):
        l_fixed = [first_float_fixed_rate]

    nfixed_rates = len(l_fixed)
    alib_num_fixed_rates = c_long(nfixed_rates)
    alib_fix_rates = (c_double * nfixed_rates)(*[])
    for i in range(0, nfixed_rates):
        alib_fix_rates[i] = c_double(l_fixed[i])

    try:
        alib_value_date = python_date_to_alib_Tdate(value_date)
    except:
        alib_value_date = value_date

    alib_convexity_adjust = convexity_adjust if isinstance(convexity_adjust, c_long) else c_long(convexity_adjust)

    alib_vol_model = Alib_Class.VMIR()
    failure = alib_obj_coerce(vol_model, 'VMIR', byref(alib_vol_model))

    raw_res = c_double()
    failure = ALib.ALIB_SWAP_FLOAT_PV_GENERIC(
        discount_curve,
        alib_notional,
        alib_spread,
        projection_curve,
        alib_start_date,
        alib_float_interval,
        alib_float_rate_interval,
        alib_compound_method,
        alib_end_date,
       alib_float_dcc,
        alib_stub_method,
        alib_accrual_bad_day_conv,
        alib_pay_bad_day_conv,
        alib_reset_bad_day_conv,
        alib_holiday_file,
        alib_principal_initial_flag,
        alib_principal_final_flag,
        alib_first_float_fixed_flag,
        alib_num_fixed_rates,
        alib_fix_rates,
        alib_value_date,
        alib_convexity_adjust,
        alib_vol_model,
        byref(raw_res)
    )

    free_objects(alib_float_interval, alib_float_rate_interval, alib_float_dcc, alib_stub_method, alib_vol_model)
    if failure:
        base_err_msg = 'Could not compute swap_float_pv'
        raise_val_err_w_log(base_err_msg)
    swap_float_pv = raw_res.value
    return swap_float_pv


def swap_float_pv_with_conv(
       discount_curve, notional, spread, projection_curve, start_date,
        end_date, holiday_oracle, compound_method, first_float_fixed_flag, first_float_fixed_rate,
        value_date, conv_str, swap_conv_dict=None
):
    """
    :description: pv swap float leg using standard conventions
    :param discount_curve: alib curve object
    :param float notional:
    :param float spread:
    :param projection_curve: alib curve object
    :param datetime.date start_date:
    :param datetime.date end_date:
    :param holiday_oracle: instance of qau.Holidays
    :param compound_method: spraad compounding method
    :param first_float_fixed_flag: Flag to denote that the first floating rate has been fixed.
    :param first_float_fixed_rate: The fixing of the first floating rate. Only required if first float rate fixed = 1.
    :param value_date:
    :param conv_str: panormus convention string, e.g. EUR.6ML
    :return: float pv
    """
    conventions = swap_conventions(conv_str, swap_conv_dict)

    needed_conv_list = ['float_interval', 'float_rate_interval', 'float_dcc', 'stub_method', 'accrual_bad_day_conv',
                        'pay_bad_day_conv', 'reset_bad_day_conv', 'principal_initial_flag',
                        'principal_final_flag', 'convexity_adjust', 'vol_model']
    needed_conventions = {s: conventions[s] for s in needed_conv_list}

    holiday_file = holiday_oracle[conventions['holiday_calendar_name']]

    return ac_swap_float_pv(
        discount_curve=discount_curve, notional=notional, spread=spread,
        projection_curve=projection_curve, start_date=start_date,
        end_date=end_date, holiday_file=holiday_file,
        compound_method=compound_method,
        first_float_fixed_flag=first_float_fixed_flag,
        first_float_fixed_rate=first_float_fixed_rate,
        value_date=value_date, **needed_conventions
    )

def swap_float_sens_with_conv(
        discount_curve, notional, spread, projection_curve, start_date,
        end_date, holiday_oracle, compound_method, first_float_fixed_flag, first_float_fixed_rate,
        value_date, conv_str, sens_type, swap_conv_dict=None
):
    """
    :description: sens swap float leg using standard conventions
    :param discount_curve: alib curve object
    :param float notional:
    :param float spread:
    :param projection_curve: alib curve object
    :param datetime.date start_date:
    :param datetime.date end_date:
    :param holiday_oracle: instance of qau.Holidays
    :param compound_method: spraad compounding method
    :param first_float_fixed_flag: Flag to denote that the first floating rate has been fixed.
    :param first_float_fixed_rate: The fixing of the first floating rate. Only required if first float rate fixed = 1.
    :param value_date:
    :param conv_str: panormus convention string, e.g. EUR.6ML
    :param sens_type: required sens type
    :return: float pv
    """
    conventions = swap_conventions(conv_str, swap_conv_dict)

    needed_conv_list = ['float_interval', 'float_rate_interval', 'float_dcc', 'stub_method', 'accrual_bad_day_conv',
                        'pay_bad_day_conv', 'reset_bad_day_conv', 'principal_initial_flag',
                        'principal_final_flag', 'convexity_adjust', 'vol_model']
    needed_conventions = {s: conventions[s] for s in needed_conv_list}

    holiday_file = holiday_oracle[conventions['holiday_calendar_name']]

    return ac_swap_float_sens(
        discount_curve=discount_curve, notional=notional, spread=spread,
        projection_curve=projection_curve, start_date=start_date,
        end_date=end_date, holiday_file=holiday_file,
        compound_method=compound_method,
        first_float_fixed_flag=first_float_fixed_flag,
        first_float_fixed_rate=first_float_fixed_rate,
        value_date=value_date, sens_type=sens_type, **needed_conventions
    )


def alib_date_to_python(alib_date):
    """
    :description: convert an alib date to a python date
    :param alib_date:
    :return: python datetime
    """
    alib_int = alib_date.value if isinstance(alib_date, c_long) else alib_date
    return ALIB_TIME_STARTS + dt.timedelta(days=alib_int)


def ac_day_cnt_frac(start_date, end_date, dcc_str):
    """
    :description: calculate a day count fraction
    :param start_date:
    :param end_date:
    :param dcc_str: day count convention, e.g. 30/360
    :return: float
    """
    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_end_date = python_date_to_alib_Tdate(end_date)

    alib_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(dcc_str, 'DCC', byref(alib_dcc))
    if failure:
        base_err_msg = 'Could not convert dcc_str to object'
        raise_val_err_w_log(base_err_msg)

    dcc_dbl = c_double()
    failure = ALib.ALIB_DAY_CNT_FRACT_O(
        alib_start_date,
        alib_end_date,
        alib_dcc,
        byref(dcc_dbl))

    free_object(alib_dcc)
    if failure:
        base_err_msg = 'Could not compute dcc'
        raise_val_err_w_log(base_err_msg)
    return dcc_dbl.value


def ac_fx_cross_date(start_date, usd_ivl_adj, ccy1_ivl_adj, ccy2_ivl_adj):
    """
    :description:
    :param start_date:
    :param usd_ivl_adj:
    :param ccy1_ivl_adj:
    :param ccy2_ivl_adj:
    :return:
    """
    alib_start_date = python_date_to_alib_Tdate(start_date)

    raw_res = c_long()
    failure = ALib.ALIB_FX_CROSS_DATE(
        alib_start_date,
        usd_ivl_adj,
        ccy1_ivl_adj,
        ccy2_ivl_adj,
        byref(raw_res))

    if failure:
        base_err_msg = 'Could not compute fx date'
        raise_val_err_w_log(base_err_msg)
    return alib_date_to_python(raw_res.value)


def ac_is_business_day(check_date, holiday_file):
    """
    :description: determines if date is a business date in context of holiday string
    :param check_date:
    :param str holiday_file: holiday file as returned from qau.Holidays()['USD']
    :return: bool
    """
    holiday_file = ascii_encode(holiday_file)
    alib_check_date = python_date_to_alib_Tdate(check_date)
    is_bd = c_bool()
    failure = ALib.ALIB_IS_BUSINESS_DAY(
        alib_check_date,
        holiday_file,
        byref(is_bd))

    if failure:
        base_err_msg = 'Could not check business day'
        raise_val_err_w_log(base_err_msg)
    return is_bd.value


def ac_date_fwd_adj2(start_date, date_offset_str, holiday_file):
    """
    :description: Adjust date forward or backward according to date_offset_str and holiday_file
    :param datetime.date start_date:
    :param str date_offset_str: alib interval string such as '1D,B' for 1 business day forward.
    :param str holiday_file: holiday file as returned from qau.Holidays()['USD']
    :return: adjusted date
    """
    holiday_file = ascii_encode(holiday_file)
    # Handle 0 business day adjustments by looking for a convention string on the end.
    if date_offset_str.upper().startswith('0D,B') or date_offset_str.upper().startswith('-0D,B'):
        for conv in ['M', 'F', 'P']:
            if date_offset_str[4:].upper().find(conv) >= 0:
                return ac_bus_day(start_date=start_date, conv_str=conv, holiday_file=holiday_file)

       wn.warn(
            'Zero day adjustment requested without a bad day convention. Returning start_date.',
            stacklevel=2
        )
        return start_date

    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_doi = Alib_Class.DOI()
    failure = alib_obj_coerce(
        date_offset_str, 'DOI', byref(alib_doi)
    )
    if failure:
        base_err_msg = 'Could not convert date_offset_str to object'
        raise_val_err_w_log(base_err_msg)
    raw_res = c_long()
    failure = ALib.ALIB_DATE_FWD_ADJ2(
        alib_start_date,
        alib_doi,
        holiday_file,
        byref(raw_res)
    )

    free_object(alib_doi)
    if failure:
        base_err_msg = 'Could not compute forward date'
        raise_val_err_w_log(base_err_msg)
    alib_fwd_date = raw_res.value
    return alib_date_to_python(alib_fwd_date)


def ac_bus_day(start_date, conv_str, holiday_file):
    '''
    :description: Adjusts start_date to a valid business date according to given convention and holiday calendar
    :param datetime.date start_date:
    :param conv_str: Business day convention. [text] - (P)revious, (F)ollowing, \
      (M)odified following, (N)one will return the input date with no adjustment.
    :param str holiday_file: Name of holiday list. [text] - file name, 'None', or 'No_Weekends'.
    :return: adjusted date.
    '''
    alib_start_date = python_date_to_alib_Tdate(start_date)
    conv_str = ascii_encode(conv_str)
    holiday_file = ascii_encode(holiday_file)

    raw_res = c_long()
    failure = ALib.ALIB_BUSINESS_DAY(
        alib_start_date,
        conv_str,
        holiday_file,
        byref(raw_res)
    )
    if failure:
        base_err_msg = 'Could not compute business day.'
        raise_val_err_w_log(base_err_msg)
    alib_bus_date = raw_res.value
    return alib_date_to_python(alib_bus_date)


def ac_date_fwd_any_o(start_date, num_periods, interval_str):
    """
    :description: adjust start_date by a num_periods * interval for valid interval_str
    :param datetime.date start_date:
    :param int num_periods:
    :param str interval_str: an interval that alib can parse. See alib documentation.
    :return: adjusted date
    """
    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_ivl = Alib_Class.IVL()
    failure = alib_obj_coerce(
        interval_str, 'IVL', byref(alib_ivl)
    )
    if failure:
        base_err_msg = 'Could not convert interval_str to object'
        raise_val_err_w_log(base_err_msg)
    raw_res = c_long()
    failure = ALib.ALIB_DATE_FWD_ANY_O(
        alib_start_date,
        c_long(int(num_periods)),
        alib_ivl,
        byref(raw_res)
    )
    free_object(alib_ivl)
    if failure:
        base_err_msg = 'Could not compute forward date'
        raise_val_err_w_log(base_err_msg)
    alib_fwd_date = raw_res.value
    return alib_date_to_python(alib_fwd_date)


def ac_bus_days_diff(start_date, end_date, holiday_file):
    """
    :description: count of business days between start date and end date
    :param datetime.date start_date:
    :param datetime.date end_date:
    :param str holiday_file: Name of holiday list. [text] - file name, 'None', or 'No_Weekends'.
    :return: int
    """
    holiday_file = ascii_encode(holiday_file)
    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_end_date = python_date_to_alib_Tdate(end_date)
    raw_res = c_long()
    failure = ALib.ALIB_BUS_DAYS_DIFF(
        alib_start_date,
        alib_end_date,
        holiday_file,
        byref(raw_res)
    )
    if failure:
        base_err_msg = 'Could not compute bus day diff'
        raise_val_err_w_log(base_err_msg)
    return int(raw_res.value)


def ac_bus_days_offset(start_date, offset, holiday_file):
    '''
    :description: simplified function to adjust a number of business days
    :param start_date: start date is either a python date (datetime.date) or a alib date (int)
    :param int offset: business day offset
    :param holiday_file: static holiday files
    :return: python date or alib date depending on start date input
    '''
    holiday_file = ascii_encode(holiday_file)

    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date

    res = c_long()
    failure = ALib.ALIB_BUS_DAYS_OFFSET(
        alib_start_date,
        c_long(offset),
        holiday_file,
        byref(res)
    )
    if failure:
        base_err_msg = 'Could not compute bus day offset'
        raise_val_err_w_log(base_err_msg)

    alib_candidate_date = res.value
    if isinstance(start_date, int):
        return alib_candidate_date
    elif isinstance(start_date, c_long):
        return c_long(alib_candidate_date)
    else:
        return ALIB_TIME_STARTS + dt.timedelta(days=alib_candidate_date)


def bus_ycf(start_date, end_date, conv_str, holidays, bd_per_year=251):
    """
    :description: year count fraction between start and end date
    :param datetime.date start_date:
    :param datetime.date end_date:
    :param conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :param int bd_per_year: business days per year for denominator of ycf
    :return: float
    """
    conv_str = str(conv_str)
    holiday_file = holidays[YCF_CAL_DICT[conv_str]]
    bdc = ac_bus_days_diff(
        start_date=start_date, end_date=end_date, holiday_file=holiday_file)
    return float(bdc) / float(bd_per_year)


def val_date_from_trade_date(trade_date, conv_str, holidays):
    """
    :description: determine swap value date from trade date
    :param datetime.date trade_date:
    :param conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :return: datetime.date
    """
    candidate_date = trade_date
    date_bumps = VAL_DATE_CONV_DICT[conv_str]

    for bump_str, bump_cal_name in date_bumps:
        candidate_date = ac_date_fwd_adj2(candidate_date, bump_str, holidays[bump_cal_name])

    return candidate_date


def start_date_from_trade_date(trade_date, fwd_str, conv_str, holidays):
    """
    :description: determine start date from trade date
    :param datetime.date trade_date:
    :param str fwd_str: forward interval interpretable by alib.
    :param str conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :return: datetime.date
    """
    ivl = Alib_Class.IVL()
    alib_obj_coerce(fwd_str, 'IVL', byref(ivl))

    if 'i' not in fwd_str:
        try:
            alib_trade_date = python_date_to_alib_Tdate(trade_date)
        except:
            alib_trade_date = trade_date

        res = c_long()
        ALib.ALIB_DATE_FWD_ANY_O(
            alib_trade_date,
            c_long(1),
            ivl,
            byref(res)
        )
        free_object(ivl)
        fwd_date = res.value
        if not isinstance(trade_date, int):
            fwd_date = ALIB_TIME_STARTS + dt.timedelta(days=fwd_date)
        return val_date_from_trade_date(trade_date=fwd_date, conv_str=conv_str, holidays=holidays)
    else:
        spot_val_date = val_date_from_trade_date(
            trade_date=trade_date, conv_str=conv_str, holidays=holidays)
        try:
            alib_spot_val_date = python_date_to_alib_Tdate(spot_val_date)
        except:
            alib_spot_val_date = spot_val_date

        res = c_long()
        ALib.ALIB_DATE_FWD_ANY_O(
            alib_spot_val_date,
            c_long(1),
            ivl,
            byref(res)
        )
        free_object(ivl)
        start_date = res.value
        return ALIB_TIME_STARTS + dt.timedelta(days=start_date)


def end_date_from_start_date(start_date, ten_str):
    """
    :description: determine end date from start date
    :param datetime.date start_date:
    :param str ten_str: swap tenor interpretable as alib interval.
    :return: datetime.date
    """
    try:
        alib_start_date = python_date_to_alib_Tdate(start_date)
    except:
        alib_start_date = start_date
    ivl = Alib_Class.IVL()
    alib_obj_coerce(ten_str, 'IVL', byref(ivl))
    res = c_long()
    ALib.ALIB_DATE_FWD_ANY_O(
        alib_start_date,
        c_long(1),
        ivl,
        byref(res)
    )
    free_object(ivl)
    end_date = res.value
    if not isinstance(start_date, int):
        end_date = ALIB_TIME_STARTS + dt.timedelta(days=end_date)
    return end_date


def end_date_from_trade_date(trade_date, fwd_str, ten_str, conv_str, holidays):
    """
    :description: determine end date from trade date
    :param datetime.date trade_date:
    :param str fwd_str: forward interval interpretable by alib.
    :param str ten_str_str: swap tenor interpretable as alib interval.
    :param str conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :return: datetime.date
    """
    start_date = start_date_from_trade_date(
        trade_date=trade_date,
        fwd_str=fwd_str,
        conv_str=conv_str,
        holidays=holidays
    )
    end_date = end_date_from_start_date(
        start_date=start_date,
        ten_str=ten_str
    )
    return end_date


def dates_from_trade_date(trade_date, fwd_str, ten_str, conv_str, holidays):
    """
    :description: determine (start_date, end_date) from trade date
    :param datetime.date trade_date:
    :param str fwd_str: forward interval interpretable by alib.
    :param str ten_str_str: swap tenor interpretable as alib interval.
    :param str conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :return: tuple of datetime.date
    """
    start_date = start_date_from_trade_date(
        trade_date=trade_date,
        fwd_str=fwd_str,
        conv_str=conv_str,
        holidays=holidays
    )
    end_date = end_date_from_start_date(
        start_date=start_date,
        ten_str=ten_str
    )
    return (start_date, end_date)


def expiry_date_from_trade_date(trade_date, expiry_str, conv_str, holidays):
    """
    :description: determine option expiry date from trade date
    :param datetime.date trade_date:
    :param str expiry_str: expiry interval interpretable by alib.
    :param str conv_str: panormus convention string, e.g. USD.3ML
    :param holidays: qau.Holidays instance
    :return: datetime.date
    """
    adjust_str, hol_str = EXP_DATE_CONV_DICT[conv_str]

    bump_str = ','.join([expiry_str, adjust_str])
    try:
        alib_trade_date = python_date_to_alib_Tdate(trade_date)
    except:
        alib_trade_date = trade_date
    doi = Alib_Class.DOI()
    alib_obj_coerce(bump_str, 'DOI', byref(doi))
    res = c_long()
    ALib.ALIB_DATE_FWD_ADJ2(
        alib_trade_date,
        doi,
        ascii_encode(holidays[hol_str]),
        byref(res)
    )
    free_object(doi)
    alib_candidate_date = res.value
    if isinstance(trade_date, int):
        return alib_candidate_date
    else:
        return ALIB_TIME_STARTS + dt.timedelta(days=alib_candidate_date)


def swap_tenor_from_dates(start_date, end_date):
    """
    :description: generate the tenor str as nY or nM based on dates.
    :param dt.date start_date:
    :param dt.date end_date:
    :rtype: str
    """
    year_frac = (end_date - start_date).days / 365.0
    if abs(year_frac % 1) > (1.0 / 13.0):
        return str(int(round(year_frac * 12.0, 0))) + 'M'
    else:
        return str(int(round(year_frac))) + 'Y'


def cash_settled_annuity(fwd_rate, fixed_periods_per_year, tenor_int):
    """
    :description: cash settle annuity for fwd rate
    :param float fwd_rate:
    :param int fixed_periods_per_year:
    :param int tenor_int: number of years
    :return: float
    """
    EPS = 1.0E-8
    n = fixed_periods_per_year * tenor_int
    if abs(fwd_rate) < EPS:
        return tenor_int
    else:
        return 1.0 / fwd_rate * (1. - 1. / (1.0 + fwd_rate / fixed_periods_per_year) ** n)


def cash_annuity_with_conv(
        discount_curve, estimating_curve,
        start_date, end_date, holiday_oracle, conv_str, swap_conv_dict=None):
    """
    :description: cash annuity from conventions
    :param discount_curve: alib curve object
    :param estimating_curve: alib curve object
    :param datetime.date start_date:
    :param datetime.date end_date:
    :param holiday_oracle: instance of qau.Holidays
    :param str conv_str: panormus convention string, e.g. EUR.6ML
    :return: float
    """
    conventions = swap_conventions(conv_str, swap_conv_dict)

    fwd_rate = swap_rate_with_conv(
        discount_curve, start_date, end_date,
        1, 0.0, estimating_curve,
        0, 0., holiday_oracle, conv_str, swap_conv_dict
    )
    fixed_ivl = conventions['fixed_interval']
    fixed_periods_per_year = 1.0 / ac_ivl_years(fixed_ivl)
    tenor_int = round((end_date - start_date).days / 365., 2)
    df = ac_interp_pv(discount_curve, start_date)

    return df * cash_settled_annuity(fwd_rate, fixed_periods_per_year, tenor_int)


def ac_fx_forward_rate(spot_date, spot_rate, forward_date, domestic_curve, foreign_curve, inverted_quote):
    """
    Wraps ALIB_FX_FORWARD_RATE_O
    :param spot_date: python date
    :param spot_rate: double
    :param forward_date: python date
    :param domestic_curve: alib zero curve for the domestic currency (typically USD.OIS)
    :param foreign_curve: alib zero curve for the foreign currency (e.g. EUR.XCCY.USD)
    :param int inverted_quote: 1 if the currency is quoted as CCYUSD, 0 otherwise
    :return: forward level
    """

    alib_spot_date = python_date_to_alib_Tdate(spot_date)
    alib_spot_rate = c_double(spot_rate)
    alib_forward_date = python_date_to_alib_Tdate(forward_date)
    alib_inverted_quote = c_int(inverted_quote)

    alib_forward = c_double()

    failure = ALib.ALIB_FX_FORWARD_RATE_O(
        alib_spot_date, alib_spot_rate, alib_forward_date, domestic_curve, foreign_curve, alib_inverted_quote,
        byref(alib_forward))

    if failure:
        base_err_msg = 'Failed to compute fx forward date'
        raise_val_err_w_log(base_err_msg)
    return alib_forward.value


# TODO: Bond methods haven't been tested for unicode strings (python 3).
def ac_bond_make(bond_type, cpn_freq, cpn_rate, mat_date, dated_date, first_cpn_date, redem_val):
    """
    Creates an alib bond object
    See the ALIB manual for a list of bond types
    """
    bond_obj = Alib_Class.BOND()
    alib_mat_date = python_date_to_alib_Tdate(mat_date)
    alib_dated_date = python_date_to_alib_Tdate(dated_date)
    alib_cpn_freq = c_long(int(cpn_freq))
    if first_cpn_date is None:
        alib_first_cpn_date = c_long(0)
    else:
        alib_first_cpn_date = python_date_to_alib_Tdate(first_cpn_date)
    failure = ALib.ALIB_BOND_MAKE(
        ascii_encode(bond_type),  # string                 BondType,              /* (I) Scalar */
        alib_cpn_freq,  # int                    CpnFreq,               /* (I) Scalar */
        c_double(cpn_rate),  # double                 CpnRate,               /* (I) Scalar */
        alib_mat_date,  # TDate                  MatDate,               /* (I) Scalar */
        alib_dated_date,  # TDate                  DatedDate,             /* (I) Scalar */
        alib_first_cpn_date,  # TDate                  FirstCpnDate,          /* (I) Scalar */
        c_double(redem_val),  # double                 RedemptionVal,         /* (I) Scalar */
        byref(bond_obj))  # out CLASS_BOND         Bond                   /* (O) Scalar */

    if failure:
        free_object(bond_obj)
        base_err_msg = 'Failed to create bond object'
        raise_val_err_w_log(base_err_msg)
    return bond_obj


def ac_bondtrade_make(settlement_date, is_exdiv=0, trade_date=None, value_date=None):
    """
    Creates an alib bondtrade object. This is usually just the settlement date.
    """
    bondtrade_obj = Alib_Class.BONDTRADE()
    alib_settle_date = python_date_to_alib_Tdate(settlement_date)
    if trade_date is None:
        alib_trade_date = alib_settle_date
    else:
        alib_trade_date = python_date_to_alib_Tdate(trade_date)

    if value_date is None:
        alib_value_date = alib_settle_date
    else:
        alib_value_date = python_date_to_alib_Tdate(value_date)

    failure = ALib.ALIB_BONDTRADE_MAKE(
        alib_settle_date,
        c_int(is_exdiv),
        alib_trade_date,
        alib_value_date,
        byref(bondtrade_obj)
    )
    if failure:
        free_object(bondtrade_obj)
        base_err_msg = 'Failed to create bondtrade object'
        raise_val_err_w_log(base_err_msg)
    return bondtrade_obj


def ac_bonds_ai(bond_obj, settle_date, output_flag, holiday_file):
    '''
    This function calculates accrued interest for a bond object. It is the object version of BONDS_AI (page 238). It can return
    one of two quantities: the amount of accrued interest, and the number of days of accrual.

    :param bond_obj:
    :param settle_date:
    :param output_flag: 1 = accrued interest, 2 = number of accrued days
    :param holiday_file:
    :return:
    '''
    holiday_file = ascii_encode(holiday_file)
    alib_bond_trade = ac_bondtrade_make(settle_date)
   acc_int = c_double()
    failure = ALib.ALIB_BONDS_AI_O(
        bond_obj,
        alib_bond_trade,
        c_int(output_flag),
        holiday_file,
        byref(acc_int))

    if failure:
        base_err_msg = 'Could not compute accrued'
        raise_val_err_w_log(base_err_msg)

    return acc_int.value


def ac_bonds_ytm(bond, settle_date, pay_bad_day_conv, holiday_file, bond_price, is_dirty_price, calc_type):
    """
    :descrioption: calculate bond's yield to maturity
    :param bond:
    :param settle_date:
    :param pay_bad_day_conv:
    :param holiday_file:
    :param bond_price:
    :param is_dirty_price:
    :param calc_type:
    :return:
    """
    pay_bad_day_conv = ascii_encode(pay_bad_day_conv)
    holiday_file = ascii_encode(holiday_file)
    alib_bondtrade = ac_bondtrade_make(settle_date)
    ytm = c_double()

    failure = ALib.ALIB_BONDS_YTM_O(
        bond,  # CLASS_BOND             Bond,              /* (I) Scalar *
        alib_bondtrade,  # CLASS_BONDTRADE        Trade,             /* (I) Scalar */
        pay_bad_day_conv,  # string                 PayBadDayConv,     /* (I) Scalar */
        holiday_file,  # string                 HolidayFile,       /* (I) Scalar */
        c_double(bond_price),  # double                 Price,             /* (I) Scalar */
        c_int(is_dirty_price),  # int                    IsDirtyPrice,      /* (I) Scalar */
        ascii_encode(calc_type),  # string                 CalcType,          /* (I) Scalar */
        byref(ytm))  # out double             YTM                /* (O) Scalar */

    if failure:
        base_err_msg = 'Failed to calcuate bond ytm'
        raise_val_err_w_log(base_err_msg)
    if failure:
        base_err_msg = 'Failed to calcuate bond ytm'
        raise_val_err_w_log(base_err_msg)
    return ytm.value


def ac_bonds_repo(
        bond, spot_settle_date, fwd_settle_date,
        repo_dcc, repo_method,
        coup_invest,
        pay_bad_day_conv,
        holiday_file,
        tax_method,
        calc_type,
        repo_rate=None,
        spot_price=None,
        fwd_price=None
):
    """
    Given two of spot price, forward price and repo rate compute the third
    Calc type is one of
    'F' - Calc forward price
    'S' - Calc spot price
    'R' - Calc implied repo
    """
    holiday_file = ascii_encode(holiday_file)
    alib_spottrade = ac_bondtrade_make(spot_settle_date)
    alib_fwdtrade = ac_bondtrade_make(fwd_settle_date)
    alib_dcc = Alib_Class.DCC()
    failure = alib_obj_coerce(repo_dcc, "DCC", byref(alib_dcc))

    alib_rr = c_double(0.0) if repo_rate is None else c_double(repo_rate)
    alib_sp = c_double(0.0) if spot_price is None else c_double(spot_price)
    alib_fp = c_double(0.0) if fwd_price is None else c_double(fwd_price)

    output = c_double()
    failure = ALib.ALIB_BONDS_REPO_O(
        bond,
        alib_spottrade,
        alib_fwdtrade,
        alib_dcc,
        repo_method,
        coup_invest,
        pay_bad_day_conv,
        holiday_file,
        tax_method,
        alib_rr,
        alib_sp,
        alib_fp,
        calc_type,
        byref(output)
    )
    free_object(alib_dcc)
    if failure:
        base_err_msg = 'Alib bonds repo failed'
        raise_val_err_w_log(base_err_msg)

    return output.value


def ac_bonds_sens(bond, settle_date, pay_bad_day_conv, holiday_file, ytm, calc_type, sens_type):
    """
    Calculates bond sensitivites
    sens_type is one of
    1 - clean price
    2 - convexity
    3 - Macaulay durationz
    4 - modified duration
    5 - present value
    6 - PVBP
    7 - theta
    """
    holiday_file = ascii_encode(holiday_file)
    alib_bondtrade = ac_bondtrade_make(settle_date)
    sens = c_double()
    failure = ALib.ALIB_BONDS_SENS_O(
        bond,
        alib_bondtrade,
        pay_bad_day_conv,
        holiday_file,
        c_double(ytm),
        ascii_encode(calc_type),
        c_int(sens_type),
        byref(sens)
    )

    if failure:
        base_err_msg = 'Alib bonds sens failed'
        raise_val_err_w_log(base_err_msg)
    return sens.value


def ac_bonds_zc_spread(
        bond, settle_date,
        holiday_file, pay_bad_day_conv, zero_curve,
        price, is_dirty_price, spread_type
):
    """
    Imply spread to zero curve needed to price a bond back to the market
    spread type is one of
    0 - no spread
    1 - average spread
    2 - forward spread
    """
    alib_bondtrade = ac_bondtrade_make(settle_date)

    spread = c_double()

    failure = ALib.ALIB_BONDS_ZC_SPREAD_O(
        bond,
        alib_bondtrade,
        ascii_encode(holiday_file),
        ascii_encode(pay_bad_day_conv),
        zero_curve,
        c_double(price),
        c_int(is_dirty_price),
        c_int(spread_type),
        byref(spread)
    )
    if failure:
        base_err_msg = 'Alib bonds ZC spread failed'
        raise_val_err_w_log(base_err_msg)
    return spread.value


def ac_bonds_sens_zc(
        bond, settle_date,
        holiday_file,
        pay_bad_day_conv,
        zero_curve,
        spread_type,
        spread,
        sens_type
):
    """
    Price a bond using a zero curve
    spread type is one of
    0 - no spread
    1 - average spread
    2 - forward spread
    sens_type is one of
    1 - clean price
    2 - convexity
    3 - Macaulay durations
    4 - modified duration
    5 - present value
    6 - PVBP
    7 - theta
    """
    alib_bondtrade = ac_bondtrade_make(settle_date)
    sens = c_double()

    failure = ALib.ALIB_BONDS_SENS_ZC_O(
        bond,
        alib_bondtrade,
        ascii_encode(holiday_file),
        ascii_encode(pay_bad_day_conv),
        zero_curve,
        c_int(spread_type),
        c_double(spread),
        c_int(sens_type),
        byref(sens)
    )
    if failure:
        base_err_msg = 'Alib bonds sens ZC failed'
        raise_val_err_w_log(base_err_msg)

    return sens.value


def ac_opt_vol_norm(fwd_undl, strike, yte, put_call, fwd_prem, vol_guess):
    """
    :description:
    :param fwd_undl:
    :param strike:
    :param yte:
    :param put_call:
    :param fwd_prem:
    :param vol_guess:
    :return:
    """

    def target_func(proposed_vol):
        price_guess = qo.norm_opt_sens(strike, yte, put_call, proposed_vol, fwd_undl, 'p')
        return price_guess - fwd_prem

    return newton(target_func, vol_guess)


def ac_opt_sens(fwd_undl, strike, yte, ytp, put_call, vol, disc_r, sens_type):
    """
    :description:
    :param fwd_undl:
    :param strike:
    :param yte:
    :param ytp:
    :param put_call:
    :param vol:
    :param disc_r:
    :param sens_type:
    :return:
    """
    fwd_undl = c_double(fwd_undl)
    strike = c_double(strike)
    yte = c_double(yte)
    ytp = c_double(ytp)
    vol = c_double(vol)
    disc_r = c_double(disc_r)
    put_call = ascii_encode(put_call)
    sens_type = ascii_encode(sens_type)
    sens_result = c_double()

    failure = ALib.ALIB_OPT_SENS(
        put_call,
        fwd_undl,
        strike,
        yte,
        ytp,
        vol,
        disc_r,
        sens_type,
        byref(sens_result))

    if failure:
        base_err_msg = 'Could not compute option norm _vol'
        raise_val_err_w_log(base_err_msg)

    return sens_result.value


def ac_opt_sens2(spot_undl, strike, yte, ytp, put_call, vol, disc_r, div_r, grw_r, sens_type):
    '''
    :description:This function calculates the price and sensitivities of a generic European option, \
    using Black's option model.
    :param float spot_undl: Spot underlying variable of the option
    :param float strike: strike
    :param float yte: Time to option expiration in years (>=0)
    :param float ytp: Time to option payment in years (>=0)
    :param str put_call: (C)all or (P)ut
    :param float vol: volatility
    :param float disc_r: Annually compounded (Act/365) risk free interest rate (>-1).
    :param float div_r: Annually compounded rate underlier is expected to lose (>-1).
    :param float grw_r:Annually compounded. Rate underlier is expected to gain (>-1).
    :param str sens_type: Determines whether option premium, or which sensitivity is calculated. \
    Use the following: (P)remium, (D)elta, (G)amma, (V)ega, (T)heta, (R)ho (discount), \
    (Y) rho (dividend), (E) rho (growth earnings), (T)heta = derivative of price w.r.t. years to expiry., \
    (C)arry = derivative of price w.r.t. years to payment., (F)orward theta = much like theta,
    forward of underlier is held constant instead of spot.
    :return: Option premium or sensitivity
    '''
    spot_undl = c_double(spot_undl)
    strike = c_double(strike)
    yte = c_double(yte)
    ytp = c_double(ytp)
    vol = c_double(vol)
    disc_r = c_double(disc_r)
    div_r = c_double(div_r)
    grw_r = c_double(grw_r)
    put_call = ascii_encode(put_call)
    sens_type = ascii_encode(sens_type)
    sens_result = c_double()

    failure = ALib.ALIB_OPT_SENS2(
        put_call,
        spot_undl,
        strike,
        yte,
        ytp,
        vol,
        disc_r,
        div_r,
        grw_r,
        sens_type,
        byref(sens_result))

    if failure:
        base_err_msg = 'Could not compute option sensitivities'
        raise_val_err_w_log(base_err_msg)

    return sens_result.value


def ac_opt_sens2_american(spot_undl, strike, yte, ytp, put_call, vol, disc_r, div_r, grw_r, sens_type):
    '''
    This function calculates the price and sensitivities of a vanilla American option, \
    using a trinomial tree model.

    :param float spot_undl: Spot underlying variable of the option
    :param float strike: strike
    :param float yte: Time to option expiration in years (>=0)
    :param float ytp: Time to option payment in years (>=0)
    :param str put_call: (C)all or (P)ut
    :param float vol: volatility
    :param float disc_r: Annually compounded (Act/365) risk free interest rate (>-1).
    :param float div_r: Annually compounded rate underlier is expected to lose (>-1).
    :param float grw_r: Annually compounded. Rate underlier is expected to gain (>-1).
    :param str sens_type: Determines whether option premium, or other sensitivity is calculated. \
    One of the following: (P)remium, (D)elta, (G)amma.

    :return: Option premium or sensitivity
    '''
    spot_undl = c_double(spot_undl)
    strike = c_double(strike)
    yte = c_double(yte)
    ytp = c_double(ytp)
    vol = c_double(vol)
    disc_r = c_double(disc_r)
    div_r = c_double(div_r)
    grw_r = c_double(grw_r)
    put_call = ascii_encode(put_call)
    sens_type = ascii_encode(sens_type)
    sens_result = c_double()

    failure = ALib.ALIB_OPT_SENS2_AMERICAN(
        put_call,
        spot_undl,
        strike,
        yte,
        ytp,
        vol,
        disc_r,
        div_r,
       grw_r,
        sens_type,
        byref(sens_result))

    if failure:
        base_err_msg = 'Could not compute option sensitivities'
        raise_val_err_w_log(base_err_msg)

    return sens_result.value


def bus_day_of_month(cal_date, hol_file=None):
    '''
    :description: Calculate the number of business days from the 1st to cal_date.day
    :param cal_date: any date object with .year, .month, and .day fields
    :param hol_file: str path to alib holiday file (use holiday oracle)
    :return: int
    '''
    hol_file = ascii_encode(hol_file)
    return ac_bus_days_diff(
        start_date=dt.date(cal_date.year, cal_date.month, 1) - dt.timedelta(days=1),
        end_date=cal_date,
        holiday_file=hol_file
    )


def bus_day_frac_of_month(cal_date, hol_file=None):
    '''
    :description: Calculate the fraction of business days that have passed in cal_date.month
    :param cal_date: any date object with .year, .month, and .day fields
    :param hol_file: str path to alib holiday file (use holiday oracle)
    :return: float
    '''
    return bus_day_of_month(cal_date=cal_date, hol_file=hol_file) \
           / float(total_bus_days_in_month(cal_date=cal_date, hol_file=hol_file))


def total_bus_days_in_month(cal_date, hol_file=None):
    '''
    :description: calculate the total number of business days in the month of cal_date
    :param cal_date: any date object with .year, .month, and .day fields
    :param hol_file: str path to alib holiday file (use holiday oracle)
    :return: int
    '''
    yr = cal_date.year
    mo = cal_date.month
    last_day = cal_mr(yr, mo)[1]
    return bus_day_of_month(dt.date(yr, mo, last_day), hol_file)


def swap_roll_date(value_date, maturity_date, pay_freq, holiday_file, bad_day_conv):
    """
    :description:
    :param value_date:
    :param maturity_date:
    :param pay_freq:
    :param holiday_file:
    :param bad_day_conv:
    :return:
    """
    mult_mapping_dict = {
        '1m': 12,
        '3m': 4,
        '6m': 2,
        '12m': 1,
        '1y': 1
    }
    mult_by = mult_mapping_dict[pay_freq.lower()]
    num_periods = round((maturity_date - value_date).days * mult_by / 365.0, 0)
    test_roll_date = ac_date_fwd_any_o(maturity_date, -num_periods, ascii_encode(pay_freq))
    if test_roll_date > value_date:
        test_roll_date = ac_date_fwd_any_o(maturity_date, -(num_periods + 1), ascii_encode(pay_freq))
    return ac_bus_day(test_roll_date, bad_day_conv, holiday_file)


def swap_fixing_date(roll_date, fixing_offset, holiday_file):
    """
    :description:
    :param roll_date:
    :param fixing_offset:
    :param holiday_file:
    :return:
    """
    fixing_term = ''.join((fixing_offset, 'D,B'))
    return ac_date_fwd_adj2(roll_date, fixing_term, holiday_file)


def swap_dates_from_conv(start_date, maturity_date, holiday_oracle, swap_conv, swap_conv_dict=None):
    """
    :description:
    :param start_date:
    :param maturity_date:
    :param holiday_oracle:
    :param swap_conv:
    :param dict swap_conv_dict:
    :return:
    """
    conventions = swap_conventions(swap_conv, swap_conv_dict)

    fixed_ivl = conventions['fixed_interval']
    fixed_period_type = fixed_ivl[-1]
    fixed_num_periods = int(fixed_ivl.replace(fixed_period_type, ''))
    fix_dcc = conventions['fixed_dcc']

   float_ivl = conventions['float_interval']
    float_period_type = float_ivl[-1]
    float_num_periods = int(float_ivl.replace(float_period_type, ''))
    float_reset_offset = abs(conventions['fixing_offset'])
    flt_dcc = conventions['float_dcc']
    float_rate_ivl = conventions['float_rate_interval']

    stub_method = conventions['stub_method']
    stub_loc = stub_method[0].upper()
    stub_type = 0 if stub_method[-1].lower() == 's' else 1
    acc_bdc = conventions['accrual_bad_day_conv']
    pay_bdc = conventions['pay_bad_day_conv']
    res_bdc = conventions['reset_bad_day_conv']

    holiday_file = holiday_oracle[conventions['holiday_calendar_name']]

    fix_coupon_dates_dict = ac_coupon_dates_swap(
        start_date, maturity_date,
        fixed_num_periods, fixed_period_type,
        1,
        0,
        0,
        0,
        stub_loc, stub_type,
        0,
        0,
        0,
        acc_bdc, pay_bdc, res_bdc,
        holiday_file
    )
    fix_coupon_dates_dict['dccs'] = [
        ac_day_cnt_frac(sd, ed, fix_dcc)
        for sd, ed in
        zip(fix_coupon_dates_dict['acc_start_dates'], fix_coupon_dates_dict['acc_end_dates'])
    ]

    flt_coupon_dates_dict = ac_coupon_dates_swap(
        start_date, maturity_date,
        float_num_periods, float_period_type,
        1,
        0,
        0,
        float_reset_offset,
        stub_loc, stub_type,
        0,
        0,
        0,
        acc_bdc, pay_bdc, res_bdc,
        holiday_file
    )

    flt_coupon_dates_dict['dccs'] = [
        ac_day_cnt_frac(sd, ed, flt_dcc)
        for sd, ed in zip(flt_coupon_dates_dict['acc_start_dates'], flt_coupon_dates_dict['acc_end_dates'])
    ]
    flt_ivl_adj = float_rate_ivl + ',' + acc_bdc
    flt_rate_end_dates = [
        ac_date_fwd_adj2(asd, flt_ivl_adj, holiday_file)
        for asd in flt_coupon_dates_dict['acc_start_dates']
    ]
    flt_coupon_dates_dict['rate_end_dates'] = flt_rate_end_dates

    return {
        'fix_leg_dates': fix_coupon_dates_dict,
        'flt_leg_dates': flt_coupon_dates_dict
    }


def ac_coupon_dates_swap(
        start_date, maturity_date, num_periods, period_type,
        adjust_last_accrual, arrears_setting_indic, pay_offset_days,
        reset_offset_days, stub_location, stub_type,
        first_roll_date, last_roll_date, full_first_coupon_date,
        acc_day_bdc, pay_day_bdc, res_day_bdc, holiday_file
):
    """
    :description:
    :param start_date: valid date
    :param maturity_date:  valid date > start_date
    :param int num_periods: integer
    :param period_type: alib period type (e.g. 'Y' for annual, 'I' for IMM
    :param int adjust_last_accrual: 0 or 1 (1 is standard)
    :param int arrears_setting_indic: 0 or 1 (0 is standard)
    :param int pay_offset_days: integer
    :param int reset_offset_days: integer
    :param str stub_location: 'B' or 'F' ('F'ront is standard)
    :param int stub_type: 0 or 1 (0 short stub, is standard)
    :param first_roll_date: 0 or valid date, see alib docs if further details needed
    :param last_roll_date: 0 or valid date, see alib docs if further details needed
    :param full_first_coupon_date: 0 or valid date, see alib docs if further details needed
    :param acc_day_bdc: alib bad day convention, 'M', 'F', 'P' or 'N'
    :param pay_day_bdc: alib bad day convention, 'M', 'F', 'P' or 'N'
    :param res_day_bdc: alib bad day convention, 'M', 'F', 'P' or 'N'
    :param holiday_file: holiday_file, from a holiday oracle
    :return:
    """
    alib_start_date = python_date_to_alib_Tdate(start_date)
    alib_maturity_date = python_date_to_alib_Tdate(maturity_date)

    alib_first_roll_date = c_long(0)
    if first_roll_date != 0:
        alib_first_roll_date = python_date_to_alib_Tdate(first_roll_date)

    alib_last_roll_date = c_long(0)
    if last_roll_date != 0:
        alib_last_roll_date = python_date_to_alib_Tdate(last_roll_date)

    alib_full_first_coupon_date = c_long(0)
    if full_first_coupon_date != 0:
        alib_full_first_coupon_date = python_date_to_alib_Tdate(full_first_coupon_date)

    maxSize = 1000
    accrual_start_dates = (c_int * maxSize)(*[])
    accrual_end_dates = (c_int * maxSize)(*[])
    reset_dates = (c_int * maxSize)(*[])
    payment_dates = (c_int * maxSize)(*[])
    stub_loc = c_int()

    failure = ALib.ALIB_COUPON_DATES_SWAP(
        alib_start_date,
        alib_maturity_date,
        c_long(num_periods),
        ascii_encode(period_type),
        c_int(adjust_last_accrual),
        c_int(arrears_setting_indic),
        c_long(pay_offset_days),
        c_long(reset_offset_days),
        ascii_encode(stub_location),
        c_int(stub_type),
        alib_first_roll_date,
        alib_last_roll_date,
        alib_full_first_coupon_date,
        ascii_encode(acc_day_bdc),
        ascii_encode(pay_day_bdc),
        ascii_encode(res_day_bdc),
        ascii_encode(holiday_file),
        byref(accrual_start_dates),
        byref(accrual_end_dates),
        byref(payment_dates),
        byref(reset_dates),
        byref(stub_loc)
    )
    if failure:
        base_err_msg = 'Could not compute swap schedule'
        raise_val_err_w_log(base_err_msg)

    return {
        'acc_start_dates': [alib_date_to_python(asd) for asd in accrual_start_dates if asd != 0],
        'acc_end_dates': [alib_date_to_python(aed) for aed in accrual_end_dates if aed != 0],
        'reset_dates': [alib_date_to_python(rd) for rd in reset_dates if rd != 0],
        'payment_dates': [alib_date_to_python(pd) for pd in payment_dates if pd != 0],
        'stub': stub_loc.value
    }


def ac_object_save(alib_obj, file_name, encoding_format='xml'):
    """
    :description: Writes an object to an ASCII file in a very specific format which resembles the format for \
    initializing a C data structure. The function calls the object¡¯s class encode method to convert the object \
    into an ASCII representation
    :param alib_obj:
    :param string file_name:
    :param string encoding_format:
    :return: int error status. 0 if success.
    """
    file_name = ascii_encode(file_name)
    encoding_format = ascii_encode(encoding_format)
    failure = ALib.ALIB_OBJECT_SAVE(alib_obj, file_name, encoding_format)
    if failure:
        base_err_msg = 'Failed to serialize alib object'
        raise_val_err_w_log(base_err_msg)

    return failure


def ac_disc_to_rate(discount_factor, start_date, end_date, dcc='Act/365', rate_type='Annual'):
    """
    :description:
    :param discount_factor:
    :param start_date:
    :param end_date:
    :param dcc:
    :param rate_type:
    :return:
    """
    alib_dcc = Alib_Class.DCC()
    alib_obj_coerce(dcc, 'DCC', byref(alib_dcc))

    alib_rate_type = Alib_Class.RT()
    alib_obj_coerce(rate_type, 'RT', byref(alib_rate_type))

    rate = c_double()

    alib_start_date = ALib.MDY_TO_TDATE(start_date.month, start_date.day, start_date.year)
    alib_end_date = ALib.MDY_TO_TDATE(end_date.month, end_date.day, end_date.year)

    failure = ALib.ALIB_DISC_TO_RATE_O(
        c_double(discount_factor),
        alib_start_date,
        alib_end_date,
        alib_dcc,
        alib_rate_type,
        byref(rate))

    free_objects(alib_dcc, alib_rate_type)
    if failure:
        base_err_msg = 'Error while converity discount factor to rate'
        raise_val_err_w_log(base_err_msg)
    return rate.value


def ac_bonds_cfl_make(bond_obj, settle_date, pay_bdc, holiday_file, include_ai):
    """
    :description:
    :param bond_obj:
    :param settle_date:
    :param pay_bdc:
    :param holiday_file:
    :param include_ai:
    :return:
    """
    bond_trade = ac_bondtrade_make(settle_date)
    cfl = Alib_Class.CFL()

    failure = ALib.ALIB_BONDS_CFL_MAKE(
        bond_obj,
        bond_trade,
        ascii_encode(pay_bdc),
        ascii_encode(holiday_file),
        include_ai,
        byref(cfl)
    )
    free_object(cfl)
    if failure:
        base_err_msg = 'Error while computing bond cashflows'
        raise_val_err_w_log(base_err_msg)

    return cfl


def ac_frn_type_make(dcc='ACT/360', bda='M', creeping=0, adjust_dm=0, ex_div_rule="None", ex_div_days=0):
    """

    :param dcc: Day count convention.
    :param str bda: Bad day convention.
    :param int creeping: Creeping coupons.
    :param int adjust_dm: Adjust discount margin frequency.
    :param str ex_div_rule: Ex-dividend rule.
    :param int ex_div_days: Ex-dividend days.
    :return: a FRN_TYPE object
    """
    alib_mm_dcc = Alib_Class.DCC()
    alib_obj_coerce(dcc, "DCC", byref(alib_mm_dcc))
    frn_type = Alib_Class.FRN_TYPE()

    failure = ALib.ALIB_FRN_TYPE_MAKE(
        alib_mm_dcc,  # Day count convention. [DCC]
        ascii_encode(bda),  # Bad day convention. [text]
        c_int(creeping),  # Creeping coupons. [integer]
        c_int(adjust_dm),  # Adjust discount margin frequency. [integer]
        ascii_encode(ex_div_rule),  # Ex-dividend rule. [text]
        c_int(ex_div_days),  # Ex-dividend days. [integer]
        byref(frn_type)  # Base object name. [text]
    )
    free_objects(alib_mm_dcc)

    if failure:
        free_object(frn_type)
        base_err_msg = 'Error while construct an FRN_TYPE object'
        raise_val_err_w_log(base_err_msg)

    return frn_type


def ac_frn_make(frn_type, cpn_freq, quote_margin, quote_margin_dt, is_perpetual, redemption, current_cpn,
                current_cpn_dt, next_cpn_dt, hols, calls, call_dates, puts, put_dates):
    """

   :param frn_type: Floating rate note type
    :param cpn_freq: Coupon frequency.
    :param quote_margin: Quoted margins.
    :param quote_margin_dt: Quoted margin dates.
    :param is_perpetual: Is perpetual flag.
    :param redemption: Redemption value.
    :param current_cpn: Current coupon.
    :param current_cpn_dt: Current coupon date.
    :param next_cpn_dt: Next coupon date.
    :param hols: Name of holiday list.
    :param calls: Call values.
    :param call_dates: Call dates.
    :param puts: Put values.
    :param put_dates: Put dates.
    :return: A floating rate note object
    """

    # Convert - Quoted margins. [real number array]
    n_quote_margin = len(quote_margin)
    alib_quote_margin = (c_double * n_quote_margin)(*[])
    for i in range(0, n_quote_margin):
        alib_quote_margin[i] = c_double(quote_margin[i])

    # Convert - Quoted margin dates. [date array]
    n_quote_margin_dt = len(quote_margin_dt)
    alib_quote_margin_dt = (c_long * n_quote_margin_dt)(*[])
    for i in range(0, n_quote_margin_dt):
        alib_quote_margin_dt[i] = python_date_to_alib_Tdate(quote_margin_dt[i])

    # Convert - Current coupon. [real number]
    alib_current_cpn = c_double(current_cpn)

    # Convert - Current coupon date. [date]
    alib_current_cpn_dt = python_date_to_alib_Tdate(current_cpn_dt)

    # Convert - Next coupon date. [date]
    alib_next_cpn_dt = python_date_to_alib_Tdate(next_cpn_dt)

    # Convert - Call values. [real number array]
    n_calls = len(calls)
    alib_calls = (c_double * n_calls)(*[])
    for i in range(0, n_calls):
        alib_calls[i] = c_double(calls[i])

    # Convert - Quoted margin dates. [date array]
    n_call_dates = len(call_dates)
    alib_call_dates = (c_long * n_call_dates)(*[])
    for i in range(0, n_call_dates):
        alib_call_dates[i] = python_date_to_alib_Tdate(call_dates[i])

    # Convert - Put values. [real number array]
    n_puts = len(puts)
    alib_puts = (c_double * n_puts)(*[])
    for i in range(0, n_puts):
        alib_puts[i] = c_double(puts[i])

    # Convert - Put dates. [date array]
    n_put_dates = len(put_dates)
    alib_put_dates = (c_long * n_put_dates)(*[])
    for i in range(0, n_put_dates):
        alib_put_dates[i] = python_date_to_alib_Tdate(put_dates[i])

    frn = Alib_Class.FRN()
    failure = ALib.ALIB_FRN_MAKE(
        frn_type,  # Floating rate note type. [FRN_TYPE]
        c_long(cpn_freq),  # Coupon frequency. [integer]
        c_long(n_quote_margin_dt),
        alib_quote_margin,  # Quoted margins. [real number array]
        alib_quote_margin_dt,  # Quoted margin dates. [date array
        c_long(is_perpetual),  # Is perpetual flag. [integer]
        c_double(redemption),  # Redemption value. [real number]
        alib_current_cpn,  # Current coupon. [real number]
        alib_current_cpn_dt,  # Current coupon date. [date]
        alib_next_cpn_dt,  # Next coupon date. [date]
        ascii_encode(hols),  # Name of holiday list. [text]
        c_long(n_calls),
        alib_calls,  # Call values. [real number array]
        alib_call_dates,  # Call dates. [date array]
        c_long(n_puts),
        alib_puts,  # Put values. [real number array]
        alib_put_dates,  # Put dates. [date array]
        byref(frn)  # Base object name. [text]
    )

    if failure:
        free_object(frn)
        base_err_msg = 'Error while computing frn make'
        raise_val_err_w_log(base_err_msg)

    return frn


def ac_trade_make(settlement_date, is_ex_div, trade_date, value_date):
    """

    :param settlement_date: settlement date of the trade
    :param is_ex_div: flag on whether the bond is currently trading ex-dividend
    :param trade_date: trade date of the bond
    :param value_date: value date of the bond
    :return: a TRADE object
    """

    alib_settlement_date = python_date_to_alib_Tdate(settlement_date)
    alib_is_ex_div = c_long(is_ex_div)
    if isinstance(trade_date, dt.date):
        alib_trade_date = python_date_to_alib_Tdate(trade_date)
    else:
        alib_trade_date = (c_long * 0)(*[])

    if isinstance(value_date, dt.date):
        alib_value_date = python_date_to_alib_Tdate(value_date)
    else:
        alib_value_date = (c_long * 0)(*[])

    trade = Alib_Class.TRADE()

    failure = ALib.ALIB_TRADE_MAKE(
        alib_settlement_date,
        alib_is_ex_div,
        alib_trade_date,
        alib_value_date,
        byref(trade)
    )

    if failure:
        free_object(trade)
        base_err_msg = 'Error while construct an TRADE object'
        raise_val_err_w_log(base_err_msg)

    return trade


def ac_frn_dm_o(frn, settle_dt, current_index_rate, assumed_index_rate, price, is_dirty):
    '''

    :param frn: a FRN object
    :param settle_dt: settle date
    :param current_index_rate: simple rate to next coupon date. This is assumed to be \
    in the FRN¡¯s day count convention.
    :param assumed_index_rate: assumed indicator rate for all coupons after the next \
    coupon date. This is assumed to be in the FRN¡¯s day count convention.
    :param price: price of the FRN
    :param is_dirty: 0 = clean price, 1 = dirty price
    :return: discount margin
    '''
    alib_current_index_rate = c_double(current_index_rate)
    alib_assumed_index_rate = c_double(assumed_index_rate)
    alib_price = c_double(price)
    alib_is_dirty = c_long(is_dirty)

    alib_trade = ac_trade_make(settle_dt, 0, settle_dt, settle_dt)
    my_dm = c_double()
    failure = ALib.ALIB_FRN_DM_O(frn, alib_trade, alib_current_index_rate, alib_assumed_index_rate,
                                 alib_price, alib_is_dirty, byref(my_dm))
    free_object(alib_trade)
    if failure:
        base_err_msg = 'Error in frn dm'
        raise_val_err_w_log(base_err_msg)
    return my_dm.value


def trading_date_list(start_date, end_date, holiday_file):
    '''
    :description: generate a list of datetime objects based on the holiday convention
    :param dt.date start_date: start date for the trading date list
    :param dt.date end_date: end date for the trading date list
    :param holiday_file: a holiday object
    :return: a list of datetime objects
    '''
    wn.warn(
        "This function will be going away soon. Use same function name from panormus.quant.market_date instead.",
        PendingDeprecationWarning)
    use_start_date = ac_bus_day(start_date, 'F', holiday_file)
    use_end_date = ac_bus_day(end_date, 'P', holiday_file)

    curr_date = use_start_date
    dates_out = [curr_date]
    while curr_date < use_end_date:
        curr_date = ac_date_fwd_adj2(curr_date, '1D,B', holiday_file)
        dates_out.append(curr_date)

    return dates_out


def ac_object_get_num_items(curve):
    '''
    :description: get the number of items
    :param curve:
    :return:
    '''
    num_items = c_long()
    num_items2 = c_long()
    failure = ALib.ALIB_OBJECT_GET(curve, ascii_encode('numItems'), byref(num_items), byref(num_items2))

    if failure:
        base_err_msg = 'Error getting number of items from curve'
        raise_val_err_w_log(base_err_msg)

    return num_items.value


def ac_object_get_rates(curve):
    num_items = ac_object_get_num_items(curve)
    rates = (c_double * MAX_ARRAY_SIZE)(*[])
    alib_dates = (c_long * MAX_ARRAY_SIZE)(*[])

    intZero = (c_long * 1)(*[])
    intZero[0] = 0

    rate_type = Alib_Class.RT()
    failure = ALib.ALIB_RT_MAKE_ID(
        c_long(1),
        byref(rate_type)
    )
    if failure:
        base_err_msg = 'Error getting rates from curve (RT_MAKE_ID step)'
        raise_val_err_w_log(base_err_msg)

    dcc_act_365Fstr = Alib_Class.DCC()
    failure = alib_obj_coerce("Act/365F", "DCC", byref(dcc_act_365Fstr))
    if failure:
        base_err_msg = 'Error getting rates from curve (RT_MAKE_ID step)'
        raise_val_err_w_log(base_err_msg)

    failure = ALib.ALIB_ZERO_CURVE_RATES(
        curve,
        c_long(len(intZero)),
        intZero,
        0,
        None,
        None,
        rate_type,
        dcc_act_365Fstr,
        byref(alib_dates),
        byref(rates)
    )

    free_objects(rate_type)
    free_objects(dcc_act_365Fstr)

    if failure:
        base_err_msg = 'Error getting rates from curve'
        raise_val_err_w_log(base_err_msg)

    python_dates = [alib_Tdate_to_python_date(d) for d in alib_dates]

    dates_lst = python_dates[:num_items]
    rates_lst = list(rates[:num_items])
    return dates_lst, rates_lst

 



